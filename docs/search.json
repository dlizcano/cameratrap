[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Camera Trap Blog",
    "section": "",
    "text": "Esfuerzo de muestreo y RAI en Fototrampeo\n\n\n\nR\n\n\nfototrampeo\n\n\ncurso\n\n\nIAR\n\n\n\nParte del Curso Introducción al Fototrampeo\n\n\n\nDiego J. Lizcano, Lain E. Pardo, Angélica Diaz-Pulido\n\n\nDec 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Season Occupancy Model\n\n\n\nR\n\n\noccupancy\n\n\nubms\n\n\nunmarked\n\n\n\nFits the single season occupancy model of MacKenzie et al (2002) using camera trap data, unmarked and ubms\n\n\n\nDiego J. Lizcano\n\n\nJul 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Stacked” Models\n\n\n\nR\n\n\noccupancy\n\n\nubms\n\n\nunmarked\n\n\n\nSuppose you have a dataset of repeated detections/non detections or counts that are collected over several years, but do not want to fit a dynamic model.\n\n\n\nDiego J. Lizcano, José F. González-Maya\n\n\nJul 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA multi-species (species interactions) occupancy model\n\n\n\nR\n\n\noccupancy\n\n\ntapir\n\n\n\nA mountain tapir, puma and andean bear interacting model\n\n\n\nDiego J. Lizcano\n\n\nJul 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies diversity\n\n\n\nR\n\n\ndiversity\n\n\naccumulation\n\n\neffort\n\n\n\nusing packages vegan and iNext to analyze diversity on camera trap data\n\n\n\nDiego J. Lizcano\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRiqueza de especies\n\n\n\nR\n\n\ndiversity\n\n\naccumulation\n\n\neffort\n\n\n\nUso de los paquetes vegan y iNext para analizar la diversidad con datos de fototrampeo\n\n\n\nDiego J. Lizcano\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultispecies occupancy model\n\n\n\nR\n\n\noccupancy\n\n\nJAGS\n\n\ncamtrapR\n\n\n\nMultispecies occupancy models combines information from multiple species to estimate both individual and community-level responses to environmental variables\n\n\n\nDiego J. Lizcano\n\n\nJun 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA calendar to visualize camera trap data\n\n\n\nR\n\n\ncalendar\n\n\nmap\n\n\n\nUsing several camera trap data campaingns from Galictis Biodiversidad\n\n\n\nDiego J. Lizcano\n\n\nJun 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html",
    "href": "posts/2024-12-10-RAI/index.html",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "",
    "text": "Este post es parte de el curso Introducción al fototrampeo realizado en el Instituto de ciencias naturales (ICN), de la Universidad Nacional de Colombia en Bogota en Diciembre 2024."
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#introducción",
    "href": "posts/2024-12-10-RAI/index.html#introducción",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "INTRODUCCIÓN",
    "text": "INTRODUCCIÓN\nEl estudio de mamíferos medianos y grandes en bosques tropicales suele ser difícil ya que muchas de estas especies son crípticas, nocturnas y esquivas, dificultando su detección. Los métodos tradicionales requieren de la captura de los animales y suelen ser costosos, de difícil manejo y poco efectivos a gran escala. Por esta razón, estas técnicas tradicionales han sido reemplazadas rápidamente por la técnica del fototrampeo, la cual usa cámaras fijas que se activan para captar imágenes de animales en el momento que pasan frente a la cámara, mediante sensores de movimiento y calor que activan la cámara.\nLas cámaras trampa son utilizadas en investigaciones biológicas como una herramienta importante para determinar distintos parámetros ecológicos como la densidad, ocurrencia, ocupación y riqueza, entre otros. Las ventajas de esta técnica son amplias, por lo que su uso es cada vez más frecuente. Por esta razón es importante conocer las características de los equipos, los alcances y limitaciones del empleo de cámaras, así como tener claridad sobre el tipo de datos proporcionados por esta técnica y su correcta interpretación. El éxito de esta técnica, como de cualquier otra, depende a su vez de un adecuado planteamiento de las preguntas de investigación y del diseño de muestreo, así como el empleo de una base conceptual sólida que permita alcanzar los objetivos planteados.\nEn este curso queremos hacer una introducción general al fototrampeo donde los participantes entiendan como funcionan las cámaras trampa, que consideraciones se deben tener en cuenta para el diseño de un estudio con fototrampeo, así como una introducción a tres análisis básicos (abundancia relativa, estimación de riqueza y patrones de actividad). En este post haremos énfasis en la abundancia relativa. La estimación de riqueza se puede ver en éste enlace. Los patrones de actividad seran objeto de un próximo post donde trataré el tema con bastante profundidad."
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#objetivos",
    "href": "posts/2024-12-10-RAI/index.html#objetivos",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "OBJETIVOS",
    "text": "OBJETIVOS\n\nCapacitar estudiantes de pregrado y personas interesadas en entender el uso y los alcances de la técnica del fototrampeo.\nEntender de manera general como se diseña un estudio con fototrampeo y como se estructuran e interpretan los datos derivados, haciendo con énfasis en lo basico: riqueza, abundancia relativa y patrones de actividad."
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#metodología",
    "href": "posts/2024-12-10-RAI/index.html#metodología",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "METODOLOGÍA",
    "text": "METODOLOGÍA\nEl curso está estructurado en dos grandes temas. El primero concerniente a la técnica propiamente dicha y el segundo relacionado con las preguntas de investigación y su análisis. Todas las sesiones son teórico-prácticas y para los ejercicios de montaje de cámaras se hará una práctica corta al interior de la Universidad Nacional. Al finalizar el curso se entregará una carpeta digital con material relacionado como libros y artículos y las presentaciones en pdf, así como los códigos en R usados en las demostraciones."
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#libro",
    "href": "posts/2024-12-10-RAI/index.html#libro",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "Libro",
    "text": "Libro\nEl curso incluye una copia impresa del Libro Fototrampeo en R de Salvador Mandujano:\n\n\nFototrampeo en R"
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#indice-de-abundancia-relativa-rai",
    "href": "posts/2024-12-10-RAI/index.html#indice-de-abundancia-relativa-rai",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "Indice de Abundancia Relativa (RAI)",
    "text": "Indice de Abundancia Relativa (RAI)\n\nRecuerda que el RAI es el número de fotos por unidad de esfuerzo de muestreo y puede ser expresado con la siguiente formula\n\n\\[\nRAI = \\dfrac{Número de fotos independientes}{dias de camara * 100}\n\\]\ny ten muy encuenta que:\n\n\n\n\n\n\nRAI no es exactamente un buen estimado de abundancia\n\n\n\nPor eso es mucho mejor llamarla Frecuencia de Captura (Capture Rate).\n\n\nPueden ver una revision muy interesante del concepto de abundancia relativa en el articulo:\n\n\n link to artículo.\n\n\nFigura 1"
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#veamos-un-ejemplo",
    "href": "posts/2024-12-10-RAI/index.html#veamos-un-ejemplo",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "Veamos un ejemplo",
    "text": "Veamos un ejemplo\nCargar Paquetes\nPrimero cargamos algunos paquetes de R\n\nCódigo\nlibrary(grateful) # Facilitate Citation of R Packages\nlibrary(patchwork) # The Composer of Plots\nlibrary(readxl) # Read Excel Files\nlibrary(sf) # Simple Features for R\nlibrary(mapview) # Interactive Viewing of Spatial Data in R\nlibrary(tmap) #nice mapr in R\nlibrary(tmaptools) #expands tmap\n# library(terra) # Spatial Data Analysis\nlibrary(readr) # Read Rectangular Text Data\nlibrary(camtrapR) # Camera Trap Data Management and Preparation of Occupancy and Spatial Capture-Recapture Analyses \nlibrary(RColorBrewer) # ColorBrewer Palettes\nlibrary(DT) # A Wrapper of the JavaScript Library 'DataTables'\nlibrary(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax\nlibrary(tidyverse) # Easily Install and Load the 'Tidyverse'\n\n# source(\"C:/CodigoR/CameraTrapCesar/R/organiza_datos.R\")\n\n\nCarguemos los datos\nSon dos archivos uno de las cámaras y otro de las especies.\n\nCódigo\n\n# library(hrbrthemes)\nlibrary(viridis)\n\ncameras &lt;- read.csv(\"C:/CodigoR/CameraTrapCesar/posts/2024-12-10-RAI/data/survey_metadata_sp_rich.csv\")\n# sp recs\nsp_rec &lt;- read.csv2(\"C:/CodigoR/CameraTrapCesar/posts/2024-12-10-RAI/data/FAZ_sp_rec_only_mammals_indep_with_generics.csv\", encoding = \"LATIN1\")\n\n# Delete cams that were not active\ncameras1 &lt;- cameras %&gt;% \n              filter(!No_spp == 0) #\n  \n# check cams vs sp records\n\nnot_in_cams &lt;- left_join(sp_rec, cameras1, by =\"deployment_id\") \nunique(not_in_cams$deployment_id) #OK\n#&gt;  [1] \"FAZ001\" \"FAZ002\" \"FAZ003\" \"FAZ004\" \"FAZ005\" \"FAZ007\" \"FAZ008\" \"FAZ009\"\n#&gt;  [9] \"FAZ010\" \"FAZ011\" \"FAZ012\" \"FAZ013\" \"FAZ014\" \"FAZ015\" \"FAZ017\" \"FAZ018\"\n#&gt; [17] \"FAZ019\" \"FAZ020\" \"FAZ021\" \"FAZ022\" \"FAZ023\" \"FAZ024\" \"FAZ025\" \"FAZ026\"\n#&gt; [25] \"FAZ027\" \"FAZ028\" \"FAZ029\"\n\n\nCamaras\nVeamos las cámaras en una tabla.\n\nCódigodatatable(head(cameras))\n\n\n\n\n\nEspecies\nVeamos las especies.\n\nCódigodatatable(sp_rec)\n\n\n\n\n\nUsemos las funciones de camtrapR\n\nEstas funciones nos permiten organizar y manipular las tablas para obtener 5 tablas derivadas. En este caso usaremos la funcion survey_rep, pero tenga en cuenta que esta función pronto será reemplazada en una proxima version de camtrapR.\nResultando en una lista de R que consta de 5 partes:\n\nsurvey_rep[[1]] esfuerzo_muestreo. camera trap operation times and image date ranges\nsurvey_rep[[2]] number of species by station\nsurvey_rep[[3]] number of events and number of stations by species\nsurvey_rep[[4]] registros_especies. number of species events by station\nsurvey_rep[[5]] number of species events by station including 0s (non-observed species)\n\n\nCódigo# first fix dates\nsp_rec$start_time &lt;- as.POSIXct(sp_rec$start_time, format = \"%Y-%m-%d %H:%M\") #     \n# make the survey report\nsurvey_rep &lt;- surveyReport(recordTable = sp_rec,\n                               CTtable = cameras1,\n                               speciesCol = \"spanish_common_name\", \n                               stationCol = \"deployment_id\",\n                               setupCol = \"start_date\", \n                               retrievalCol = \"end_date\",\n                               recordDateTimeCol = \"start_time\",\n                               makezip = F # prepara un archivo .zip, False here \n                               #sinkpath = \"data_out\",\n                            # camOp = cam_op) # directorio donde guardara .zip\n                           )\n#&gt; \n#&gt; -------------------------------------------------------\n#&gt; [1] \"Total number of stations:  27\"\n#&gt; \n#&gt; -------------------------------------------------------\n#&gt; [1] \"Number of operational stations:  27\"\n#&gt; \n#&gt; -------------------------------------------------------\n#&gt; [1] \"n nights with cameras set up (operational or not. NOTE: only correct if 1 camera per station): 4531\"\n#&gt; \n#&gt; -------------------------------------------------------\n#&gt; [1] \"n nights with cameras set up and active (trap nights. NOTE: only correct if 1 camera per station): 4531\"\n#&gt; \n#&gt; -------------------------------------------------------\n#&gt; [1] \"total trapping period:  2024-01-03 - 2024-08-28\"\n\n\nCalculemos el RAI\n\n\n\n\n\n\nTip\n\n\n\nRecuerda que el RAI es en realidad más una tasa de captura que una abundancia.\n\n\nPrimero unimos las dos tablas\nUnimos las el esfuerzo de muestreo y los registros de las especies.\n\nCódigoesfuerzo_muestreo &lt;- survey_rep[[1]] |&gt; left_join(cameras)\nn_activas &lt;- esfuerzo_muestreo[c(\"deployment_id\", \"n_nights_active\",\n                                 \"longitude\", \"latitude\")]\nwildlife.data &lt;- merge(survey_rep[[4]], n_activas, all.y = T)\ndatatable(head(wildlife.data))# \n\n\n\n\n\nRenombramos algunas columnas\n\nCódigonames(wildlife.data)[names(wildlife.data) == \"deployment_id\"] &lt;- \"Camera\"\nnames(wildlife.data)[names(wildlife.data) == \"n_events\"] &lt;- \"Events\"\nnames(wildlife.data)[names(wildlife.data) == \"n_nights_active\"] &lt;- \"Effort\"\nnames(wildlife.data)[names(wildlife.data) == \"spanish_common_name\"] &lt;- \"common_name\"\n\n\nVeamos en un mapa el esfuerzo de cada camara\n\nCódigo# primero convertimos la tabla a un objeto sf\nEfort_map &lt;- wildlife.data |&gt; \n  select(c(\"Camera\", \"longitude\", \"latitude\", \"Effort\")) |&gt; \n  st_as_sf(coords = c('longitude', 'latitude'), crs = 4326)\n# mapview\nmapview(Efort_map, \n        alpha = 0,\n        map.types = \"Esri.WorldImagery\",\n        cex = \"Effort\")\n\n\n\n\n\nRAI general por especie\nEl RAI general que se calcula agrupando toda la información de las cámaras por especie y es un valor que tiene en cuenta los registros de cada especie dividido por el esfuerzo.\n\nCódigoRAI &lt;- wildlife.data |&gt; group_by(common_name) |&gt; mutate (RAI_general=round ( (sum(Events) / sum(Effort) ) * 100, 2)) |&gt; ungroup()\ndatatable(head(RAI))\n\n\n\n\n\nRAI alternativo\nEl RAI alternativo se calcula por especie por camara y es un valor para cada especie en cada camara.\n\nCódigoRAI2 &lt;- RAI |&gt; group_by(common_name) |&gt; mutate (RAI_camara=round ( Events / Effort * 100, 2)) |&gt; ungroup()\n# make common_name factor\nRAI2$common_name &lt;- as.factor(RAI2$common_name)\ndatatable(head(RAI2))\n\n\n\n\n\nVeamolo como graficas\n\nCódigo\n# Barplot\nggplot(RAI, aes(x=reorder(common_name, RAI_general), y=RAI_general)) + \n  geom_bar(stat = \"identity\") +  ggtitle(\"RAI general\") +\n  coord_flip()\n\n\n\n\n\n\nCódigo\n# Barplot\n# Plot\nRAI2 %&gt;%\n  ggplot( aes(x=common_name, y=RAI_camara, fill=common_name)) +\n    geom_boxplot() + scale_x_discrete(guide = guide_axis(angle = 90)) +\n    scale_fill_viridis(discrete = TRUE, alpha=0.6) +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    # theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"RAI alternativo\") +\n    xlab(\"\")\n\n\n\n\n\n\n\nVeamos el RAI alternativo como un mapa\nPara esto usaremos las facilidades que ofrece el paquete tmap. Los puntos negros son las cámaras y los rojos el RAI alternativo para cada especie.\n\nCódigo######| layout-nrow: 1\n#### column: screen-inset-shaded\n\n# primero convertimos el RAI2 a un sf\nRAI2 &lt;- RAI2 |&gt; st_as_sf(coords = c('longitude', 'latitude'), crs = 4326)\n\n# veamos un mapa por especie\ntm_shape(RAI2, bbox = tmaptools::bb(RAI2, ext = 1.5))  + \n  tm_basemap(\"Esri.WorldImagery\") + # usa basemap\n    tm_symbols(shape = 1, col = \"black\", fill = \"black\",size =0.2) + #punto negro\n  tm_shape(RAI2, bbox = tmaptools::bb(RAI2, ext = 1.5))  + \n    tm_bubbles(fill = \"red\", col = \"red\", size = \"RAI_camara\", scale = 1.5) +\n    tm_facets(by = \"common_name\") + #, ncol = 5) +\n  tm_tiles(\"Esri_WorldImagery\") +\n  tm_legend_hide() \n\n\n\n\n\n\n\n\nFacil no?\n\nNormalmente el trabajo con datos de fototrampeo involucra 80% del tiempo ajustando los datos y las tablas y tan solo en 20% del tiempo corriendo el analisis."
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#package-citation",
    "href": "posts/2024-12-10-RAI/index.html#package-citation",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "Package Citation",
    "text": "Package Citation\n\nCódigopkgs &lt;- cite_packages(output = \"paragraph\", out.dir = \".\") #knitr::kable(pkgs)\npkgs\n\nWe used R version 4.4.2 (R Core Team 2024) and the following R packages: camtrapR v. 2.3.0 (Niedballa et al. 2016), devtools v. 2.4.5 (Wickham et al. 2022), DT v. 0.33 (Xie, Cheng, y Tan 2024), kableExtra v. 1.4.0 (Zhu 2024), mapview v. 2.11.2 (Appelhans et al. 2023), patchwork v. 1.3.0 (Pedersen 2024), quarto v. 1.4.4 (Allaire y Dervieux 2024), RColorBrewer v. 1.1.3 (Neuwirth 2022), rmarkdown v. 2.29 (Xie, Allaire, y Grolemund 2018; Xie, Dervieux, y Riederer 2020; Allaire et al. 2024), sf v. 1.0.19 (Pebesma 2018; Pebesma y Bivand 2023), styler v. 1.10.3 (Müller y Walthert 2024), tidyverse v. 2.0.0 (Wickham et al. 2019), tmap v. 4.1 (Tennekes 2018), tmaptools v. 3.2 (Tennekes 2025), viridis v. 0.6.5 (Garnier et al. 2024)."
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#sesion-info",
    "href": "posts/2024-12-10-RAI/index.html#sesion-info",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "Sesion info",
    "text": "Sesion info\n\nSession info\n\n#&gt; ─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.4.2 (2024-10-31 ucrt)\n#&gt;  os       Windows 10 x64 (build 19045)\n#&gt;  system   x86_64, mingw32\n#&gt;  ui       RTerm\n#&gt;  language (EN)\n#&gt;  collate  Spanish_Colombia.utf8\n#&gt;  ctype    Spanish_Colombia.utf8\n#&gt;  tz       America/Bogota\n#&gt;  date     2025-07-11\n#&gt;  pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n#&gt;  ! package           * version   date (UTC) lib source\n#&gt;    abind               1.4-8     2024-09-12 [1] CRAN (R 4.4.1)\n#&gt;    base64enc           0.1-3     2015-07-28 [1] CRAN (R 4.4.0)\n#&gt;    brew                1.0-10    2023-12-16 [1] CRAN (R 4.4.2)\n#&gt;    bslib               0.8.0     2024-07-29 [1] CRAN (R 4.4.2)\n#&gt;    cachem              1.1.0     2024-05-16 [1] CRAN (R 4.4.2)\n#&gt;    camtrapR          * 2.3.0     2024-02-26 [1] CRAN (R 4.4.2)\n#&gt;    cellranger          1.1.0     2016-07-27 [1] CRAN (R 4.4.2)\n#&gt;    class               7.3-22    2023-05-03 [2] CRAN (R 4.4.2)\n#&gt;    classInt            0.4-10    2023-09-05 [1] CRAN (R 4.4.2)\n#&gt;    cli                 3.6.3     2024-06-21 [1] CRAN (R 4.4.2)\n#&gt;    codetools           0.2-20    2024-03-31 [2] CRAN (R 4.4.2)\n#&gt;    colorspace          2.1-1     2024-07-26 [1] CRAN (R 4.4.2)\n#&gt;    cols4all            0.8       2024-10-16 [1] CRAN (R 4.4.2)\n#&gt;    crosstalk           1.2.1     2023-11-23 [1] CRAN (R 4.4.2)\n#&gt;    curl                6.0.0     2024-11-05 [1] CRAN (R 4.4.2)\n#&gt;    data.table          1.16.4    2024-12-06 [1] CRAN (R 4.4.2)\n#&gt;    DBI                 1.2.3     2024-06-02 [1] CRAN (R 4.4.2)\n#&gt;    devtools            2.4.5     2022-10-11 [1] CRAN (R 4.4.2)\n#&gt;    dichromat           2.0-0.1   2022-05-02 [1] CRAN (R 4.4.0)\n#&gt;    digest              0.6.37    2024-08-19 [1] CRAN (R 4.4.2)\n#&gt;    dplyr             * 1.1.4     2023-11-17 [1] CRAN (R 4.4.2)\n#&gt;    DT                * 0.33      2024-04-04 [1] CRAN (R 4.4.2)\n#&gt;    e1071               1.7-16    2024-09-16 [1] CRAN (R 4.4.2)\n#&gt;    ellipsis            0.3.2     2021-04-29 [1] CRAN (R 4.4.2)\n#&gt;    evaluate            1.0.1     2024-10-10 [1] CRAN (R 4.4.2)\n#&gt;    farver              2.1.2     2024-05-13 [1] CRAN (R 4.4.2)\n#&gt;    fastmap             1.2.0     2024-05-15 [1] CRAN (R 4.4.2)\n#&gt;    forcats           * 1.0.0     2023-01-29 [1] CRAN (R 4.4.2)\n#&gt;    fs                  1.6.5     2024-10-30 [1] CRAN (R 4.4.2)\n#&gt;    generics            0.1.3     2022-07-05 [1] CRAN (R 4.4.2)\n#&gt;    ggplot2           * 3.5.2     2025-04-09 [1] CRAN (R 4.4.3)\n#&gt;    glue                1.8.0     2024-09-30 [1] CRAN (R 4.4.2)\n#&gt;    grateful          * 0.2.10    2024-09-04 [1] CRAN (R 4.4.2)\n#&gt;    gridExtra           2.3       2017-09-09 [1] CRAN (R 4.4.2)\n#&gt;    gtable              0.3.6     2024-10-25 [1] CRAN (R 4.4.2)\n#&gt;    hms                 1.1.3     2023-03-21 [1] CRAN (R 4.4.2)\n#&gt;    htmltools           0.5.8.1   2024-04-04 [1] CRAN (R 4.4.2)\n#&gt;    htmlwidgets         1.6.4     2023-12-06 [1] CRAN (R 4.4.2)\n#&gt;    httpuv              1.6.15    2024-03-26 [1] CRAN (R 4.4.2)\n#&gt;    jquerylib           0.1.4     2021-04-26 [1] CRAN (R 4.4.2)\n#&gt;    jsonlite            1.8.9     2024-09-20 [1] CRAN (R 4.4.2)\n#&gt;    kableExtra        * 1.4.0     2024-01-24 [1] CRAN (R 4.4.2)\n#&gt;    KernSmooth          2.23-24   2024-05-17 [2] CRAN (R 4.4.2)\n#&gt;    knitr               1.49      2024-11-08 [1] CRAN (R 4.4.2)\n#&gt;    labeling            0.4.3     2023-08-29 [1] CRAN (R 4.4.0)\n#&gt;    later               1.3.2     2023-12-06 [1] CRAN (R 4.4.2)\n#&gt;    lattice             0.22-6    2024-03-20 [2] CRAN (R 4.4.2)\n#&gt;    leafem              0.2.4     2025-05-01 [1] CRAN (R 4.4.3)\n#&gt;    leaflegend          1.2.1     2024-05-09 [1] CRAN (R 4.4.2)\n#&gt;    leaflet             2.2.2     2024-03-26 [1] CRAN (R 4.4.2)\n#&gt;    leaflet.providers   2.0.0     2023-10-17 [1] CRAN (R 4.4.2)\n#&gt;    leafpop             0.1.0     2021-05-22 [1] CRAN (R 4.4.2)\n#&gt;    leafsync            0.1.0     2019-03-05 [1] CRAN (R 4.4.2)\n#&gt;    lifecycle           1.0.4     2023-11-07 [1] CRAN (R 4.4.2)\n#&gt;    lubridate         * 1.9.4     2024-12-08 [1] CRAN (R 4.4.2)\n#&gt;    lwgeom              0.2-14    2024-02-21 [1] CRAN (R 4.4.2)\n#&gt;    magrittr            2.0.3     2022-03-30 [1] CRAN (R 4.4.2)\n#&gt;    maptiles            0.8.0     2024-10-22 [1] CRAN (R 4.4.2)\n#&gt;    mapview           * 2.11.2    2023-10-13 [1] CRAN (R 4.4.2)\n#&gt;    MASS                7.3-61    2024-06-13 [2] CRAN (R 4.4.2)\n#&gt;    Matrix              1.7-1     2024-10-18 [2] CRAN (R 4.4.2)\n#&gt;    memoise             2.0.1     2021-11-26 [1] CRAN (R 4.4.2)\n#&gt;    mgcv                1.9-1     2023-12-21 [2] CRAN (R 4.4.2)\n#&gt;    microbenchmark      1.5.0     2024-09-04 [1] CRAN (R 4.4.2)\n#&gt;    mime                0.12      2021-09-28 [1] CRAN (R 4.4.0)\n#&gt;    miniUI              0.1.1.1   2018-05-18 [1] CRAN (R 4.4.2)\n#&gt;    munsell             0.5.1     2024-04-01 [1] CRAN (R 4.4.2)\n#&gt;    mvtnorm             1.3-2     2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    nlme                3.1-166   2024-08-14 [2] CRAN (R 4.4.2)\n#&gt;    patchwork         * 1.3.0     2024-09-16 [1] CRAN (R 4.4.2)\n#&gt;    pillar              1.10.1    2025-01-07 [1] CRAN (R 4.4.2)\n#&gt;    pkgbuild            1.4.5     2024-10-28 [1] CRAN (R 4.4.2)\n#&gt;    pkgconfig           2.0.3     2019-09-22 [1] CRAN (R 4.4.2)\n#&gt;    pkgload             1.4.0     2024-06-28 [1] CRAN (R 4.4.2)\n#&gt;    png                 0.1-8     2022-11-29 [1] CRAN (R 4.4.0)\n#&gt;    processx            3.8.4     2024-03-16 [1] CRAN (R 4.4.2)\n#&gt;    profvis             0.4.0     2024-09-20 [1] CRAN (R 4.4.2)\n#&gt;    promises            1.3.0     2024-04-05 [1] CRAN (R 4.4.2)\n#&gt;    proxy               0.4-27    2022-06-09 [1] CRAN (R 4.4.2)\n#&gt;    ps                  1.8.1     2024-10-28 [1] CRAN (R 4.4.2)\n#&gt;    purrr             * 1.0.2     2023-08-10 [1] CRAN (R 4.4.2)\n#&gt;    quarto            * 1.4.4     2024-07-20 [1] CRAN (R 4.4.2)\n#&gt;    R.cache             0.16.0    2022-07-21 [1] CRAN (R 4.4.2)\n#&gt;    R.methodsS3         1.8.2     2022-06-13 [1] CRAN (R 4.4.0)\n#&gt;    R.oo                1.27.0    2024-11-01 [1] CRAN (R 4.4.1)\n#&gt;    R.utils             2.12.3    2023-11-18 [1] CRAN (R 4.4.2)\n#&gt;    R6                  2.6.1     2025-02-15 [1] CRAN (R 4.4.2)\n#&gt;    raster              3.6-30    2024-10-02 [1] CRAN (R 4.4.2)\n#&gt;    RColorBrewer      * 1.1-3     2022-04-03 [1] CRAN (R 4.4.0)\n#&gt;    Rcpp                1.0.13-1  2024-11-02 [1] CRAN (R 4.4.2)\n#&gt;    RcppNumerical       0.6-0     2023-09-06 [1] CRAN (R 4.4.2)\n#&gt;  D RcppParallel        5.1.9     2024-08-19 [1] CRAN (R 4.4.2)\n#&gt;    readr             * 2.1.5     2024-01-10 [1] CRAN (R 4.4.2)\n#&gt;    readxl            * 1.4.3     2023-07-06 [1] CRAN (R 4.4.2)\n#&gt;    remotes             2.5.0     2024-03-17 [1] CRAN (R 4.4.2)\n#&gt;    renv                1.0.11    2024-10-12 [1] CRAN (R 4.4.2)\n#&gt;    rlang               1.1.4     2024-06-04 [1] CRAN (R 4.4.2)\n#&gt;    rmarkdown           2.29      2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    rstudioapi          0.17.1    2024-10-22 [1] CRAN (R 4.4.2)\n#&gt;    s2                  1.1.7     2024-07-17 [1] CRAN (R 4.4.2)\n#&gt;    sass                0.4.9     2024-03-15 [1] CRAN (R 4.4.2)\n#&gt;    satellite           1.0.5     2024-02-10 [1] CRAN (R 4.4.2)\n#&gt;    scales              1.3.0     2023-11-28 [1] CRAN (R 4.4.2)\n#&gt;    secr                5.1.0     2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    sessioninfo         1.2.2     2021-12-06 [1] CRAN (R 4.4.2)\n#&gt;    sf                * 1.0-19    2024-11-05 [1] CRAN (R 4.4.2)\n#&gt;    shiny               1.9.1     2024-08-01 [1] CRAN (R 4.4.2)\n#&gt;    slippymath          0.3.1     2019-06-28 [1] CRAN (R 4.4.2)\n#&gt;    sp                  2.1-4     2024-04-30 [1] CRAN (R 4.4.2)\n#&gt;    spacesXYZ           1.3-0     2024-01-23 [1] CRAN (R 4.4.2)\n#&gt;    stars               0.6-8     2025-02-01 [1] CRAN (R 4.4.2)\n#&gt;    stringi             1.8.4     2024-05-06 [1] CRAN (R 4.4.0)\n#&gt;    stringr           * 1.5.1     2023-11-14 [1] CRAN (R 4.4.2)\n#&gt;    styler            * 1.10.3    2024-04-07 [1] CRAN (R 4.4.2)\n#&gt;    svglite             2.1.3     2023-12-08 [1] CRAN (R 4.4.2)\n#&gt;    systemfonts         1.1.0     2024-05-15 [1] CRAN (R 4.4.2)\n#&gt;    terra               1.8-21    2025-02-10 [1] CRAN (R 4.4.2)\n#&gt;    tibble            * 3.2.1     2023-03-20 [1] CRAN (R 4.4.2)\n#&gt;    tidyr             * 1.3.1     2024-01-24 [1] CRAN (R 4.4.2)\n#&gt;    tidyselect          1.2.1     2024-03-11 [1] CRAN (R 4.4.2)\n#&gt;    tidyverse         * 2.0.0     2023-02-22 [1] CRAN (R 4.4.2)\n#&gt;    timechange          0.3.0     2024-01-18 [1] CRAN (R 4.4.2)\n#&gt;    tmap              * 4.1       2025-05-12 [1] CRAN (R 4.4.3)\n#&gt;    tmaptools         * 3.2       2025-01-13 [1] CRAN (R 4.4.3)\n#&gt;    tzdb                0.4.0     2023-05-12 [1] CRAN (R 4.4.2)\n#&gt;    units               0.8-5     2023-11-28 [1] CRAN (R 4.4.2)\n#&gt;    urlchecker          1.0.1     2021-11-30 [1] CRAN (R 4.4.2)\n#&gt;    usethis             3.0.0     2024-07-29 [1] CRAN (R 4.4.1)\n#&gt;    uuid                1.2-1     2024-07-29 [1] CRAN (R 4.4.1)\n#&gt;    vctrs               0.6.5     2023-12-01 [1] CRAN (R 4.4.2)\n#&gt;    viridis           * 0.6.5     2024-01-29 [1] CRAN (R 4.4.2)\n#&gt;    viridisLite       * 0.4.2     2023-05-02 [1] CRAN (R 4.4.2)\n#&gt;    withr               3.0.2     2024-10-28 [1] CRAN (R 4.4.2)\n#&gt;    wk                  0.9.4     2024-10-11 [1] CRAN (R 4.4.2)\n#&gt;    xfun                0.49      2024-10-31 [1] CRAN (R 4.4.2)\n#&gt;    XML                 3.99-0.17 2024-06-25 [1] CRAN (R 4.4.1)\n#&gt;    xml2                1.3.6     2023-12-04 [1] CRAN (R 4.4.2)\n#&gt;    xtable              1.8-4     2019-04-21 [1] CRAN (R 4.4.2)\n#&gt;    yaml                2.3.10    2024-07-26 [1] CRAN (R 4.4.1)\n#&gt; \n#&gt;  [1] C:/Users/usuario/AppData/Local/R/win-library/4.4\n#&gt;  [2] C:/Program Files/R/R-4.4.2/library\n#&gt; \n#&gt;  D ── DLL MD5 mismatch, broken installation.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html",
    "href": "posts/2024-07-17-stackmodel/index.html",
    "title": "“Stacked” Models",
    "section": "",
    "text": "Multi-season (or dynamic) models are commonly used to estimate colonization and/or extinction probabilities, and to test hypotheses on these parameters (using covariates on the parameters gamma and epsilon). This approach needs good amounts of data (many sites, and specially many seasons or years). If you don’t need to estimate dynamic parameters (Colonization or extinction, gamma and epsilon) but you’d like to test for temporal variation in occupancy (Psi) between two or three years taking in to account detection probability (p) you could apply a single-season model with random effects (being random effects the camera trap, sampling unit, or site), by stacking years (i.e., your sampling units would be combination camera-years)."
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#using-random-effects-with-ubms",
    "href": "posts/2024-07-17-stackmodel/index.html#using-random-effects-with-ubms",
    "title": "“Stacked” Models",
    "section": "Using random effects with ubms\n",
    "text": "Using random effects with ubms\n\nOne of the advantages of the package ubms is that it is possible to include random effects easily in your models, using the same syntax as lme4 (Bates et al. 2015). For example, if you have a group or site covariate, you can fit a model with random intercepts by group-site by including + (1|site) in your parameter formula. Random slopes, or a combination of random slopes and intercepts, are also possible.\nTo illustrate the use of random effects using the package ubms, in this post, we fit a model using a “stacked” model approach. Additionally in ubms you can instead include, for example, random site intercepts to account for possible pseudoreplication.\nRecently (February 2025) version 1.5.0 of unmarked also incorporated random effects and community models."
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#load-packages",
    "href": "posts/2024-07-17-stackmodel/index.html#load-packages",
    "title": "“Stacked” Models",
    "section": "Load packages",
    "text": "Load packages\nFirst we load some packages\n\nCode\nlibrary(grateful) # Facilitate Citation of R Packages\nlibrary(patchwork) # The Composer of Plots\nlibrary(readxl) # Read Excel Files\nlibrary(sf) # Simple Features for R\nlibrary(mapview) # Interactive Viewing of Spatial Data in R\nlibrary(terra) # Spatial Data Analysis\nlibrary(elevatr) # Access Elevation Data from Various APIs\nlibrary(readr) # read csv files \n\nlibrary(camtrapR) # Camera Trap Data Management and Preparation of Occupancy and Spatial Capture-Recapture Analyses \nlibrary(ubms) # bayesian occupancy modeling\nlibrary(lme4) # \nlibrary(DT) # nice tables\n\nlibrary(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax\nlibrary(tidyverse) # Load the 'Tidyverse'"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#load-data",
    "href": "posts/2024-07-17-stackmodel/index.html#load-data",
    "title": "“Stacked” Models",
    "section": "Load data",
    "text": "Load data\nThe data set is downloaded from Initiative Monitoreo Katios in Wildlife insights were we sampled with an array of 30 cameras on two consecutive years in Katios National Park in Colombia.\n\n\nInitiative Monitoreo Katios\n\n\nCode\npath &lt;- \"C:/CodigoR/CameraTrapCesar/data/katios/\"\ncameras &lt;- read_csv(paste(path, \"cameras.csv\", sep=\"\"))\ndeployment &lt;- read_csv(paste(path, \"deployments.csv\", sep=\"\"))\nimages &lt;- read_csv(paste(path, \"images.csv\", sep=\"\"))\nproject &lt;- read_csv(paste(path, \"projects.csv\", sep=\"\"))\n\n# join_by(project_id, camera_id, camera_name)`\ncam_deploy &lt;- cameras |&gt; left_join(deployment) |&gt; \n  dplyr::mutate(year=lubridate::year(start_date)) #|&gt; filter(year== 2023)\ncam_deploy_image &lt;- images  |&gt; \n  left_join(cam_deploy) |&gt; \n  mutate(scientificName= paste(genus, species, sep = \" \")) |&gt; \n   mutate(deployment_id_cam=paste(deployment_id, camera_id, sep = \"-\")) #|&gt; \n  # filter(year==2022)"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#convert-to-sf-and-view-the-map",
    "href": "posts/2024-07-17-stackmodel/index.html#convert-to-sf-and-view-the-map",
    "title": "“Stacked” Models",
    "section": "Convert to sf and view the map",
    "text": "Convert to sf and view the map\n\nCode\ndatos_distinct &lt;- cam_deploy_image |&gt; distinct(longitude, latitude, deployment_id, samp_year) |&gt; as.data.frame()\n\n# Fix NA camera 16\ndatos_distinct[16,] &lt;- c( -77.2787, 7.73855, \n                      \"CT-K1-31-124\", 2021)\n\nprojlatlon &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\ndatos_sf &lt;-  st_as_sf(x = datos_distinct,\n                         coords = c(\"longitude\", \n                                    \"latitude\"),\n                         crs = projlatlon)\n\nmapview(st_jitter(datos_sf, 0.00075) , zcol=\"samp_year\")\n\n\n\n\n\nNotice we used the function st_jitter() because the points are on top of the previous year."
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#extract-site-covariates",
    "href": "posts/2024-07-17-stackmodel/index.html#extract-site-covariates",
    "title": "“Stacked” Models",
    "section": "Extract site covariates",
    "text": "Extract site covariates\nUsing the coordinates of the sf object (datos_sf) we put the cameras on top of the covaraies and with the function terra::extract() we get the covariate value.\nIn this case we used as covariates:\n\nCattle distribution as number of cows per 10 square kilometer (Gilbert et al. 2018).\nPercent of tree cover from MODIS product 44B.\nRoad density from (Meijer et al. 2018).\nLand cover types from MODIS.\n\n\nCode#load rasters\nper_tree_cov &lt;- rast(\"C:/CodigoR/WCS-CameraTrap/raster/latlon/Veg_Cont_Fields_Yearly_250m_v61/Perc_TreeCov/MOD44B_Perc_TreeCov_2021_065.tif\")\nroad_den &lt;- rast(\"C:/CodigoR/WCS-CameraTrap/raster/latlon/RoadDensity/grip4_total_dens_m_km2.asc\")\n# elev &lt;- rast(\"D:/CORREGIDAS/elevation_z7.tif\")\nlandcov &lt;- rast(\"C:/CodigoR/WCS-CameraTrap/raster/latlon/LandCover_Type_Yearly_500m_v61/LC1/MCD12Q1_LC1_2021_001.tif\") \ncattle &lt;- rast(\"C:/CodigoR/WCS-CameraTrap/raster/latlon/Global cattle distribution/5_Ct_2010_Da.tif\")\n#river &lt;- st_read(\"F:/WCS-CameraTrap/shp/DensidadRios/MCD12Q1_LC1_2001_001_RECLASS_MASK_GRID_3600m_DensDrenSouthAmer.shp\")\n\n# get elevation map\n# elevation_detailed &lt;- rast(get_elev_raster(sites, z = 10, clip=\"bbox\", neg_to_na=TRUE))\n# elevation_detailed &lt;- get_elev_point (datos_sf, src=\"aws\", overwrite=TRUE)\n\n\n# extract covs using points and add to sites\n# covs &lt;- cbind(sites, terra::extract(SiteCovsRast, sites))\nper_tre &lt;- terra::extract(per_tree_cov, datos_sf)\nroads &lt;- terra::extract(road_den, datos_sf)\n# eleva &lt;- terra::extract(elevation_detailed, sites)\nland_cov &lt;- terra::extract(landcov, datos_sf)\ncattle_den &lt;-  terra::extract(cattle, datos_sf)\n\n#### drop geometry \nsites &lt;- datos_sf %&gt;%\n  mutate(lat = st_coordinates(.)[,1],\n         lon = st_coordinates(.)[,2]) %&gt;%\n  st_drop_geometry() |&gt; as.data.frame()\n\n# remove decimals convert to factor\nsites$land_cover &lt;-  factor(land_cov$MCD12Q1_LC1_2021_001)\n# sites$elevation &lt;-  eleva$file3be898018c3\nsites$per_tree_cov &lt;- per_tre$MOD44B_Perc_TreeCov_2021_065 \n#  fix 200 isue\nind &lt;- which(sites$per_tree_cov== 200)\nsites$per_tree_cov[ind] &lt;- 0\n\n# sites$elevation &lt;- elevation_detailed$elevation\nsites$roads &lt;- roads$grip4_total_dens_m_km2\nsites$cattle &lt;- cattle_den[,2]\n\n\nwrite.csv(sites, \"C:/CodigoR/CameraTrapCesar/data/katios/stacked/site_covs.csv\")"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#select-by-years-and-convert-to-stacked-format",
    "href": "posts/2024-07-17-stackmodel/index.html#select-by-years-and-convert-to-stacked-format",
    "title": "“Stacked” Models",
    "section": "Select by years and convert to stacked format",
    "text": "Select by years and convert to stacked format\nTo get the detection history we use the function detectionHistory of the camtrapR package.\n\n\n\n\n\n\nTake in to account, at the end we need to stack the data in this format:\n\n\n\n\n\nobs1\nobs2\nobs3\nsite\nyear\n\n\n\n0\n0\n0\n1\n1\n\n\n0\n0\n0\n2\n1\n\n\n1\nNA\nNA\n3\n1\n\n\n0\n0\n0\n4\n1\n\n\n0\n0\n0\n1\n2\n\n\n1\n0\n1\n2\n2\n\n\n0\n1\nNA\n3\n2\n\n\n\n\n\nSo we need to go by years and then stack de two tables.\nFirst year 2021\nHere we use the function detectionHistory() from the package camtrapR to generate species detection histories that can be used later in occupancy analyses, with package unmarked and ubms. detectionHistory() generates detection histories in different formats, with adjustable occasion length and occasion start time and effort covariates. Notice we first need to get the camera operation dates using the function cameraOperation().\n\nCode\n# filter first year and make uniques\n\nCToperation_2021  &lt;- cam_deploy_image |&gt; #multi-season data\n  filter(samp_year==2021) |&gt; \n  group_by(deployment_id) |&gt; \n  mutate(minStart=min(start_date), maxEnd=max(end_date)) |&gt; \n  distinct(longitude, latitude, minStart, maxEnd, samp_year) |&gt; \n  ungroup() |&gt; as.data.frame()\n\n\n# Fix NA camera 16\nCToperation_2021[16,] &lt;- c(\"CT-K1-31-124\", -77.2787,    7.73855, \n                      \"2021-10-10\", \"2021-12-31\", 2021)\n\n# make numeric sampling year\nCToperation_2021$samp_year &lt;- as.numeric(CToperation_2021$samp_year)\n\n# camera operation matrix for _2021\n# multi-season data. Season1\ncamop_2021 &lt;- cameraOperation(CTtable= CToperation_2021, # Tabla de operación\n                         stationCol= \"deployment_id\", # Columna que define la estación\n                         setupCol= \"minStart\", #Columna fecha de colocación\n                         retrievalCol= \"maxEnd\", #Columna fecha de retiro\n                         sessionCol = \"samp_year\", # multi-season column\n                         #hasProblems= T, # Hubo fallos de cámaras\n                         dateFormat= \"%Y-%m-%d\")#, #, # Formato de las fechas\n                         #cameraCol=\"CT\")\n                         #sessionCol= \"samp_year\")\n\n# Generar las historias de detección ---------------------------------------\n## remove plroblem species\n# ind &lt;- which(datos_PCF$Species==\"Marmosa sp.\")\n# datos_PCF &lt;- datos_PCF[-ind,]\n\n# filter y1\ndatay_2021 &lt;- cam_deploy_image |&gt; filter(samp_year ==2021) # |&gt; \n  # filter(samp_year==2022) \n\nDetHist_list_2021 &lt;- lapply(unique(datay_2021$scientificName), FUN = function(x) {\n  detectionHistory(\n    recordTable         = datay_2021, # Tabla de registros\n    camOp                = camop_2021, # Matriz de operación de cámaras\n    stationCol           = \"deployment_id\",\n    speciesCol           = \"scientificName\",\n    recordDateTimeCol    = \"timestamp\",\n    recordDateTimeFormat  = \"%Y-%m-%d %H:%M:%S\",\n    species              = x,     # la función reemplaza x por cada una de las especies\n    occasionLength       = 15, # Colapso de las historias a días\n    day1                 = \"station\", #inicie en la fecha de cada survey\n    datesAsOccasionNames = FALSE,\n    includeEffort        = TRUE,\n    scaleEffort          = FALSE,\n    unmarkedMultFrameInput=TRUE,\n    timeZone             = \"America/Bogota\" \n    )\n  }\n)\n\n# names\nnames(DetHist_list_2021) &lt;- unique(datay_2021$scientificName)\n\n# Finalmente creamos una lista nueva donde estén solo las historias de detección\nylist_2021 &lt;- lapply(DetHist_list_2021, FUN = function(x) x$detection_history)\n# y el esfuerzo\neffortlist_2021 &lt;- lapply(DetHist_list_2021, FUN = function(x) x$effort)\n\n### Danta, Jaguar\nwhich(names(ylist_2021) ==\"Tapirus bairdii\")\n#&gt; integer(0)\nwhich(names(ylist_2021) ==\"Panthera onca\") \n#&gt; [1] 5\n\n\nNext, the year 2022\n\nCode\n# filter firs year and make uniques\n\nCToperation_2022  &lt;- cam_deploy_image |&gt; #multi-season data\n  filter(samp_year==2022) |&gt; \n  group_by(deployment_id) |&gt; \n  mutate(minStart=min(start_date), maxEnd=max(end_date)) |&gt; \n  distinct(longitude, latitude, minStart, maxEnd, samp_year) |&gt; \n  ungroup() |&gt; as.data.frame()\n\n\n# Fix NA camera 16\n# CToperation_2022[16,] &lt;- c(\"CT-K1-31-124\", -77.2787,  7.73855, \n#                       \"2022-10-10\", \"2022-12-31\", 2022)\n\n# make numeric sampling year\nCToperation_2022$samp_year &lt;- as.numeric(CToperation_2022$samp_year)\n\n# camera operation matrix for _2022\n# multi-season data. Season1\ncamop_2022 &lt;- cameraOperation(CTtable= CToperation_2022, # Tabla de operación\n                         stationCol= \"deployment_id\", # Columna que define la estación\n                         setupCol= \"minStart\", #Columna fecha de colocación\n                         retrievalCol= \"maxEnd\", #Columna fecha de retiro\n                         sessionCol = \"samp_year\", # multi-season column\n                         #hasProblems= T, # Hubo fallos de cámaras\n                         dateFormat= \"%Y-%m-%d\")#, #, # Formato de las fechas\n                         #cameraCol=\"CT\")\n                         #sessionCol= \"samp_year\")\n\n# Generar las historias de detección ---------------------------------------\n## remove plroblem species\n# ind &lt;- which(datos_PCF$Species==\"Marmosa sp.\")\n# datos_PCF &lt;- datos_PCF[-ind,]\n\n# filter y1\ndatay_2022 &lt;- cam_deploy_image |&gt; filter(samp_year ==2022) # |&gt; \n  # filter(samp_year==2022) \n\nDetHist_list_2022 &lt;- lapply(unique(datay_2022$scientificName), FUN = function(x) {\n  detectionHistory(\n    recordTable         = datay_2022, # Tabla de registros\n    camOp                = camop_2022, # Matriz de operación de cámaras\n    stationCol           = \"deployment_id\",\n    speciesCol           = \"scientificName\",\n    recordDateTimeCol    = \"timestamp\",\n    recordDateTimeFormat  = \"%Y-%m-%d %H:%M:%S\",\n    species              = x,     # la función reemplaza x por cada una de las especies\n    occasionLength       = 25, # Colapso de las historias a días\n    day1                 = \"station\", #inicie en la fecha de cada survey\n    datesAsOccasionNames = FALSE,\n    includeEffort        = TRUE,\n    scaleEffort          = FALSE,\n    unmarkedMultFrameInput=TRUE,\n    timeZone             = \"America/Bogota\" \n    )\n  }\n)\n\n# names\nnames(DetHist_list_2022) &lt;- unique(datay_2022$scientificName)\n\n# Finalmente creamos una lista nueva donde estén solo las historias de detección\nylist_2022 &lt;- lapply(DetHist_list_2022, FUN = function(x) x$detection_history)\neffortlist_2022 &lt;- lapply(DetHist_list_2022, FUN = function(x) x$effort)\n\n### Danta, Jaguar\n### Danta, Jaguar\nwhich(names(ylist_2022) ==\"Tapirus bairdii\")\n#&gt; [1] 21\nwhich(names(ylist_2022) ==\"Panthera onca\") \n#&gt; [1] 19"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#save-and-fix-in-excel",
    "href": "posts/2024-07-17-stackmodel/index.html#save-and-fix-in-excel",
    "title": "“Stacked” Models",
    "section": "Save and fix in excel",
    "text": "Save and fix in excel\nWe are using the data for the Jaguar\n\nCode# Jaguar\n# datatable (ylist_2021[[5]], caption = 'Jaguar 2021')\n# datatable (ylist_2022[[19]], caption = 'Jaguar 2022')\n\n# y obs\nwrite.csv(ylist_2021[[5]], \"C:/CodigoR/CameraTrapCesar/data/katios/stacked/y_jaguar2021.csv\")\n# effort\nwrite.csv(effortlist_2021[[5]], \"C:/CodigoR/CameraTrapCesar/data/katios/stacked/effort_jaguar2021.csv\")\n# y obs\nwrite.csv(ylist_2022[[19]], \"C:/CodigoR/CameraTrapCesar/data/katios/stacked/y_jaguar2022.csv\")\n# effort\nwrite.csv(effortlist_2022[[5]], \"C:/CodigoR/CameraTrapCesar/data/katios/stacked/effort_jaguar2022.csv\")"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#fitting-a-stacked-model-for-jaguar",
    "href": "posts/2024-07-17-stackmodel/index.html#fitting-a-stacked-model-for-jaguar",
    "title": "“Stacked” Models",
    "section": "Fitting a stacked model for jaguar",
    "text": "Fitting a stacked model for jaguar\nLets use the ubms package to make a stacked occupancy model pooling 2021 and 2022 data together and use the percent tree cover, the road density and the cattle density as covariates for the occupancy and the effort as the number of sampling days as covariate for detection.\nLoad the data\n\nCodejaguar &lt;- read.csv(\"C:/CodigoR/CameraTrapCesar/data/katios/stacked/y_jaguar_stacked.csv\")\n\n\nLook at the data\n\nCode\ndatatable(head(jaguar))\n\n\n\n\n\nNotice we collapsed the events to 15 days in the 2021 sampling season, and to 25 days in the 2022 sampling season, to end with 6 repeated observations in de matrix. In the matrix o1 to o6 are observations and e1 to e6 are sampling effort(observation covariates). Land_cover, per_tree_cov and roads are site covariates.\nCreate an unmarked frame\nWith our stacked dataset constructed, we build the unmarkedFrame() object.\n\nCode\n# fix NA spread\n# yj &lt;- rbind(ylist[[62]][1:30,1:8], # 62 is Jaguar\n#             ylist[[62]][31:50,12:19])\n\n# ej &lt;- rbind(effortlist[[4]][1:30,1:8],\n#             effortlist[[4]][31:50,12:19])\n    \n  \numf &lt;- unmarkedFrameOccu(y=jaguar[,2:7], \n                         siteCovs=jaguar[,c(8,9,16:19)],\n                         obsCovs=list(effort=jaguar[10:15])\n                      )\n\nplot(umf)\n\n\n\n\n\n\n\nFit models\nFit the Stacked Model\nWe’ll now we fit a model with fixed effects of percent tree cover road density and cattle density (per_tree_cov, roads and cattle) on occupancy and a effort affecting the detection. In addition, we will include random intercepts by site, since in stacking the data we have pseudoreplication by site. To review, random effects are specified using the approach used in with the lme4 package. For example, a random intercept for each level of the covariate site is specified with the formula component (1|site). Including random effects in a model in ubms usually significantly increases the run time, but at the end is worth the waiting time.\nNext we perform model selection.\n\nCode# fit_0 &lt;- occu(~1~1, data=umf) # unmarked\n\nfit_j0 &lt;- stan_occu(~1~1 + (1|site),\n                       data=umf, chains=3, iter=10000, cores=3)\nfit_j1 &lt;- stan_occu(~scale(effort) ~1 + (1|site), \n                       data=umf, chains=3, iter=10000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 0.000155 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.55 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 9.556 seconds (Warm-up)\n#&gt; Chain 1:                12.041 seconds (Sampling)\n#&gt; Chain 1:                21.597 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 9.5e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.95 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 12.506 seconds (Warm-up)\n#&gt; Chain 2:                13.451 seconds (Sampling)\n#&gt; Chain 2:                25.957 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 9.3e-05 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.93 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 14.915 seconds (Warm-up)\n#&gt; Chain 3:                13.331 seconds (Sampling)\n#&gt; Chain 3:                28.246 seconds (Total)\n#&gt; Chain 3:\nfit_j2 &lt;- stan_occu(~scale(effort) ~scale(per_tree_cov) + (1|site), \n                       data=umf, chains=3, iter=10000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 0.000105 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.05 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 9.74 seconds (Warm-up)\n#&gt; Chain 1:                116.09 seconds (Sampling)\n#&gt; Chain 1:                125.83 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 9.9e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.99 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 14.308 seconds (Warm-up)\n#&gt; Chain 2:                23.488 seconds (Sampling)\n#&gt; Chain 2:                37.796 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 9.7e-05 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.97 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 49.852 seconds (Warm-up)\n#&gt; Chain 3:                227.862 seconds (Sampling)\n#&gt; Chain 3:                277.714 seconds (Total)\n#&gt; Chain 3:\nfit_j3 &lt;- stan_occu(~scale(effort) ~scale(roads) + (1|site), \n                       data=umf, chains=3, iter=10000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 9.8e-05 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.98 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 12.603 seconds (Warm-up)\n#&gt; Chain 1:                18.231 seconds (Sampling)\n#&gt; Chain 1:                30.834 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 9.7e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.97 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 22.958 seconds (Warm-up)\n#&gt; Chain 2:                37.816 seconds (Sampling)\n#&gt; Chain 2:                60.774 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 9.8e-05 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.98 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 12.868 seconds (Warm-up)\n#&gt; Chain 3:                10.975 seconds (Sampling)\n#&gt; Chain 3:                23.843 seconds (Total)\n#&gt; Chain 3:\nfit_j4 &lt;- stan_occu(~scale(effort) ~scale(cattle) + (1|site), \n                       data=umf, chains=3, iter=10000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 9.5e-05 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.95 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 10.14 seconds (Warm-up)\n#&gt; Chain 1:                11.181 seconds (Sampling)\n#&gt; Chain 1:                21.321 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 0.00014 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.4 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 14.082 seconds (Warm-up)\n#&gt; Chain 2:                13.832 seconds (Sampling)\n#&gt; Chain 2:                27.914 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 0.000101 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.01 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 11.47 seconds (Warm-up)\n#&gt; Chain 3:                18.408 seconds (Sampling)\n#&gt; Chain 3:                29.878 seconds (Total)\n#&gt; Chain 3:\n# compare\nmodels &lt;- list(Null = fit_j0,\n                effort = fit_j1,\n                effort_treecov = fit_j2,\n                effort_road = fit_j3,\n                effort_cattle = fit_j4)\n\nmods &lt;- fitList(fits = models)\n\n\n## see model selection as a table\ndatatable( \n  round(modSel(mods), 3)\n  )\n\n\n\n\n\nInstead of AIC, models are compared using leave-one-out cross-validation (LOO) (Vehtari, Gelman, and Gabry 2017) via the loo package. Based on this cross-validation, the expected predictive accuracy (elpd) for each model is calculated. The model with the largest elpd (effort_cattle) performed best. The looic value is analogous to AIC.\n\nCodeloo(fit_j4)\n#&gt; \n#&gt; Computed from 15000 by 53 log-likelihood matrix.\n#&gt; \n#&gt;          Estimate   SE\n#&gt; elpd_loo    -54.7 13.5\n#&gt; p_loo         4.4  1.0\n#&gt; looic       109.5 27.0\n#&gt; ------\n#&gt; MCSE of elpd_loo is 0.1.\n#&gt; MCSE and ESS estimates assume MCMC draws (r_eff in [0.1, 0.8]).\n#&gt; \n#&gt; All Pareto k estimates are good (k &lt; 0.7).\n#&gt; See help('pareto-k-diagnostic') for details.\n\n\n\nBest model is effort_cattle (fit_j4) which has effort on detection and percent tree cover on occupancy.\n\n\nCodefit_j4\n#&gt; \n#&gt; Call:\n#&gt; stan_occu(formula = ~scale(effort) ~ scale(cattle) + (1 | site), \n#&gt;     data = umf, chains = 3, iter = 10000)\n#&gt; \n#&gt; Occupancy (logit-scale):\n#&gt;                Estimate    SD    2.5% 97.5% n_eff Rhat\n#&gt; (Intercept)      -1.306 0.755 -2.6588 0.336  3145 1.00\n#&gt; scale(cattle)    -1.244 1.038 -3.7209 0.327  3676 1.00\n#&gt; sigma [1|site]    0.717 0.710  0.0775 2.686   192 1.01\n#&gt; \n#&gt; Detection (logit-scale):\n#&gt;               Estimate    SD  2.5%  97.5% n_eff Rhat\n#&gt; (Intercept)     -1.623 0.486 -2.67 -0.763  1872    1\n#&gt; scale(effort)    0.338 0.365 -0.35  1.084  1873    1\n#&gt; \n#&gt; LOOIC: 109.464\n#&gt; Runtime: 79.113 sec\n\n\nLooking at the summary of fit_j4, we conclude MCMC chains have converged if all R^&gt;1.05 To visualize convergence, look at the traceplots:\n\nCodetraceplot(fit_j4, pars=c(\"beta_state\", \"beta_det\"))\n\n\n\n\n\n\n\nEvaluate model fit\nStatistic should be near 0.5 if the model fits well.\n\nCode# eval\nfit_top_gof &lt;- gof(fit_j4, draws=300, quiet=TRUE)\nfit_top_gof\n#&gt; MacKenzie-Bailey Chi-square \n#&gt; Point estimate = 51.366\n#&gt; Posterior predictive p = 0.503\n\nplot(fit_top_gof)\n\n\n\n\n\n\n\nModel inference\nEffort in detection and cattle density in occupancy\n\nCodeubms::plot_effects(fit_j4, \"det\")\n\n\n\n\n\n\nCodeubms::plot_effects(fit_j4, \"state\")"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#comparing-occupancy-between-years",
    "href": "posts/2024-07-17-stackmodel/index.html#comparing-occupancy-between-years",
    "title": "“Stacked” Models",
    "section": "Comparing occupancy between years",
    "text": "Comparing occupancy between years\nUsing the posterior_predict function in ubms, you can generate an equivalent posterior distribution of z, and latter to do a post-hoc analyses to test for a difference in mean occupancy probability between sites 2021 and sites 2022.\n\nCodezpost &lt;- posterior_predict(fit_j4, \"z\", draws=1000)\ndim(zpost)\n#&gt; [1] 1000   55\n\nyear_2021 &lt;- rowMeans(zpost[,1:32], na.rm=TRUE) \nyear_2022 &lt;- rowMeans(zpost[,33:55], na.rm=TRUE)\n\nplot_dat &lt;- rbind(data.frame(group=\"year_2021\", occ=mean(year_2021),\n                             lower=quantile(year_2021, 0.025),\n                             upper=quantile(year_2021, 0.975)),\n                  data.frame(group=\"year_2022\", occ=mean(year_2022),\n                             lower=quantile(year_2022, 0.025),\n                             upper=quantile(year_2022, 0.975)))\n\n# Now plot the posterior distributions of the two means:\n\n\na &lt;- ggplot(plot_dat, aes(x=group, y=occ)) +\n  geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2) +\n  geom_point(size=3) +\n  ylim(0.1, 0.85) +\n  labs(x=\"Year\", y=\"Occupancy + 95% UI\") +\n  theme_bw() +\n  theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank(),\n        axis.text=element_text(size=12), axis.title=element_text(size=14))\n\n# print graph\na\n\n\n\n\n\n\n\nIt seems there is a reduction in mean occupancy probability, but the difference in mean occupancy probability between years is not significant.\nComparing against year in occupancy\n\nCodefit_j5 &lt;- stan_occu(~scale(effort) ~scale(cattle) + year + (1|site), \n                       data=umf, chains=3, iter=50000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 0.000154 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.54 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 55.515 seconds (Warm-up)\n#&gt; Chain 1:                76.408 seconds (Sampling)\n#&gt; Chain 1:                131.923 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 9.7e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.97 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 71.85 seconds (Warm-up)\n#&gt; Chain 2:                70.896 seconds (Sampling)\n#&gt; Chain 2:                142.746 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 9.9e-05 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.99 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 77.625 seconds (Warm-up)\n#&gt; Chain 3:                162.743 seconds (Sampling)\n#&gt; Chain 3:                240.368 seconds (Total)\n#&gt; Chain 3:\n\nfit_j6 &lt;- stan_occu(~scale(effort) ~ year + (1|site), \n                       data=umf, chains=3, iter=50000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 9.3e-05 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.93 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 73.353 seconds (Warm-up)\n#&gt; Chain 1:                93.581 seconds (Sampling)\n#&gt; Chain 1:                166.934 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 9.7e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.97 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 60.629 seconds (Warm-up)\n#&gt; Chain 2:                89.247 seconds (Sampling)\n#&gt; Chain 2:                149.876 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 0.0001 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 55.853 seconds (Warm-up)\n#&gt; Chain 3:                78.768 seconds (Sampling)\n#&gt; Chain 3:                134.621 seconds (Total)\n#&gt; Chain 3:\n\n# compare\nmodels2 &lt;- list(Null = fit_j0,\n                effort_cattle = fit_j4,\n                effort_cattle_yr = fit_j5,\n                effort_yr = fit_j6)\n\nmods2 &lt;- fitList(fits = models2)\n\n## see model selection as a table\ndatatable( \n  round(modSel(mods2), 3)\n  )\n\n\n\n\n\nPlot year in occupancy model\nFirst the plot extracting posteriors, second using year as covariate\n\nCodeb &lt;- ubms::plot_effects(fit_j6, \"state\", level=0.95)\n\nlibrary(patchwork)\na + b\n\n\n\n\n\n\n\nIt seems to be better to extract the years from the posteriors.\nLets make a bayes T-Test to check diferences in occupancy between years.\n\nCodeposterior_totest &lt;- data.frame(posterior_occu=\n                                 as.vector(c(year_2021, \n                                             year_2022)))\nposterior_totest$year &lt;- as.factor(c(rep(\"2021\",1000), rep(\"2022\",1000)))\n\n\n# anov &lt;- glm(posterior_occu ~ year, data = posterior_totest)\n\nlibrary(bayesAB)\nAB1 &lt;- bayesTest(as.vector(year_2021), \n                 as.vector(year_2022), \n                  priors = c('mu' = 0.5, \n                             'lambda' = 1, \n                             'alpha' = 3, \n                             'beta' = 1), \n                 distribution = 'normal')\n\nsummary(AB1)\n#&gt; Quantiles of posteriors for A and B:\n#&gt; \n#&gt; $Mu\n#&gt; $Mu$A\n#&gt;        0%       25%       50%       75%      100% \n#&gt; 0.3023394 0.3155605 0.3180952 0.3206259 0.3344544 \n#&gt; \n#&gt; $Mu$B\n#&gt;        0%       25%       50%       75%      100% \n#&gt; 0.2203704 0.2342196 0.2368812 0.2395492 0.2532107 \n#&gt; \n#&gt; \n#&gt; $Sig_Sq\n#&gt; $Sig_Sq$A\n#&gt;         0%        25%        50%        75%       100% \n#&gt; 0.01176654 0.01369237 0.01410487 0.01453943 0.01734703 \n#&gt; \n#&gt; $Sig_Sq$B\n#&gt;         0%        25%        50%        75%       100% \n#&gt; 0.01284015 0.01504891 0.01551010 0.01598601 0.01902671 \n#&gt; \n#&gt; \n#&gt; --------------------------------------------\n#&gt; \n#&gt; P(A &gt; B) by (0, 0)%: \n#&gt; \n#&gt; $Mu\n#&gt; [1] 1\n#&gt; \n#&gt; $Sig_Sq\n#&gt; [1] 0.06553\n#&gt; \n#&gt; --------------------------------------------\n#&gt; \n#&gt; Credible Interval on (A - B) / B for interval length(s) (0.9, 0.9) : \n#&gt; \n#&gt; $Mu\n#&gt;        5%       95% \n#&gt; 0.2986277 0.3888942 \n#&gt; \n#&gt; $Sig_Sq\n#&gt;           5%          95% \n#&gt; -0.180292983  0.008517019 \n#&gt; \n#&gt; --------------------------------------------\n#&gt; \n#&gt; Posterior Expected Loss for choosing A over B:\n#&gt; \n#&gt; $Mu\n#&gt; [1] 0\n#&gt; \n#&gt; $Sig_Sq\n#&gt; [1] 0.1033198\nplot(AB1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLets make a similar test using Bayes Factors\nHowever this time we test if occupancy in 2021 is &lt; than occupancy in 2022.\n\nCode\nlibrary(rstanarm)\n\nmodel &lt;- stan_glm(\n  formula = posterior_occu ~ year,\n  data = posterior_totest,\n  prior = student_t(1, 0.3, autoscale = TRUE),\n  chains = 3, iter = 50000, warmup = 1000\n)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 7.3e-05 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  1001 / 50000 [  2%]  (Sampling)\n#&gt; Chain 1: Iteration:  6000 / 50000 [ 12%]  (Sampling)\n#&gt; Chain 1: Iteration: 11000 / 50000 [ 22%]  (Sampling)\n#&gt; Chain 1: Iteration: 16000 / 50000 [ 32%]  (Sampling)\n#&gt; Chain 1: Iteration: 21000 / 50000 [ 42%]  (Sampling)\n#&gt; Chain 1: Iteration: 26000 / 50000 [ 52%]  (Sampling)\n#&gt; Chain 1: Iteration: 31000 / 50000 [ 62%]  (Sampling)\n#&gt; Chain 1: Iteration: 36000 / 50000 [ 72%]  (Sampling)\n#&gt; Chain 1: Iteration: 41000 / 50000 [ 82%]  (Sampling)\n#&gt; Chain 1: Iteration: 46000 / 50000 [ 92%]  (Sampling)\n#&gt; Chain 1: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 0.068 seconds (Warm-up)\n#&gt; Chain 1:                8.856 seconds (Sampling)\n#&gt; Chain 1:                8.924 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 1.5e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  1001 / 50000 [  2%]  (Sampling)\n#&gt; Chain 2: Iteration:  6000 / 50000 [ 12%]  (Sampling)\n#&gt; Chain 2: Iteration: 11000 / 50000 [ 22%]  (Sampling)\n#&gt; Chain 2: Iteration: 16000 / 50000 [ 32%]  (Sampling)\n#&gt; Chain 2: Iteration: 21000 / 50000 [ 42%]  (Sampling)\n#&gt; Chain 2: Iteration: 26000 / 50000 [ 52%]  (Sampling)\n#&gt; Chain 2: Iteration: 31000 / 50000 [ 62%]  (Sampling)\n#&gt; Chain 2: Iteration: 36000 / 50000 [ 72%]  (Sampling)\n#&gt; Chain 2: Iteration: 41000 / 50000 [ 82%]  (Sampling)\n#&gt; Chain 2: Iteration: 46000 / 50000 [ 92%]  (Sampling)\n#&gt; Chain 2: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 0.068 seconds (Warm-up)\n#&gt; Chain 2:                9.038 seconds (Sampling)\n#&gt; Chain 2:                9.106 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 4.2e-05 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  1001 / 50000 [  2%]  (Sampling)\n#&gt; Chain 3: Iteration:  6000 / 50000 [ 12%]  (Sampling)\n#&gt; Chain 3: Iteration: 11000 / 50000 [ 22%]  (Sampling)\n#&gt; Chain 3: Iteration: 16000 / 50000 [ 32%]  (Sampling)\n#&gt; Chain 3: Iteration: 21000 / 50000 [ 42%]  (Sampling)\n#&gt; Chain 3: Iteration: 26000 / 50000 [ 52%]  (Sampling)\n#&gt; Chain 3: Iteration: 31000 / 50000 [ 62%]  (Sampling)\n#&gt; Chain 3: Iteration: 36000 / 50000 [ 72%]  (Sampling)\n#&gt; Chain 3: Iteration: 41000 / 50000 [ 82%]  (Sampling)\n#&gt; Chain 3: Iteration: 46000 / 50000 [ 92%]  (Sampling)\n#&gt; Chain 3: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 0.07 seconds (Warm-up)\n#&gt; Chain 3:                8.917 seconds (Sampling)\n#&gt; Chain 3:                8.987 seconds (Total)\n#&gt; Chain 3:\n\n\nlibrary(bayestestR)\n\nMy_first_BF &lt;- bayesfactor_parameters(model, direction = \"&lt;\")\nMy_first_BF\n#&gt; Bayes Factor (Savage-Dickey density ratio)\n#&gt; \n#&gt; Parameter   |       BF\n#&gt; ----------------------\n#&gt; (Intercept) |    0.002\n#&gt; year2022    | 2.11e+17\n#&gt; \n#&gt; * Evidence Against The Null: 0\n#&gt; *                 Direction: Left-Sided test\n\neffectsize::interpret_bf(exp(My_first_BF$log_BF[2]), include_value = TRUE)\n#&gt; [1] \"extreme evidence (BF = 2.11e+17) in favour of\"\n#&gt; (Rules: jeffreys1961)\n\n\n#library(see)\n#plot(My_first_BF)\n\n\nIn fact occupancy from posteriors are different, and 2021 is &lt; than 2022."
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#package-citation",
    "href": "posts/2024-07-17-stackmodel/index.html#package-citation",
    "title": "“Stacked” Models",
    "section": "Package Citation",
    "text": "Package Citation\n\nCodepkgs &lt;- cite_packages(output = \"paragraph\", out.dir = \".\") #knitr::kable(pkgs)\npkgs\n\nWe used R version 4.4.2 (R Core Team 2024) and the following R packages: bayesAB v. 1.1.3 (Portman 2021), bayestestR v. 0.16.1 (Makowski, Ben-Shachar, and Lüdecke 2019), camtrapR v. 2.3.0 (Niedballa et al. 2016), devtools v. 2.4.5 (Wickham et al. 2022), DT v. 0.33 (Xie, Cheng, and Tan 2024), effectsize v. 0.8.9 (Ben-Shachar, Lüdecke, and Makowski 2020), elevatr v. 0.99.0 (Hollister et al. 2023), kableExtra v. 1.4.0 (Zhu 2024), lme4 v. 1.1.35.5 (Bates et al. 2015), mapview v. 2.11.2 (Appelhans et al. 2023), patchwork v. 1.3.0 (Pedersen 2024), quarto v. 1.4.4 (Allaire and Dervieux 2024), rmarkdown v. 2.29 (Xie, Allaire, and Grolemund 2018; Xie, Dervieux, and Riederer 2020; Allaire et al. 2024), rstanarm v. 2.32.1 (Brilleman et al. 2018; Goodrich et al. 2024), sf v. 1.0.21 (Pebesma 2018; Pebesma and Bivand 2023), styler v. 1.10.3 (Müller and Walthert 2024), terra v. 1.8.21 (Hijmans 2025), tidyverse v. 2.0.0 (Wickham et al. 2019), ubms v. 1.2.7 (Kellner et al. 2021)."
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#sesion-info",
    "href": "posts/2024-07-17-stackmodel/index.html#sesion-info",
    "title": "“Stacked” Models",
    "section": "Sesion info",
    "text": "Sesion info\n\nSession info\n\n#&gt; ─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.4.2 (2024-10-31 ucrt)\n#&gt;  os       Windows 10 x64 (build 19045)\n#&gt;  system   x86_64, mingw32\n#&gt;  ui       RTerm\n#&gt;  language (EN)\n#&gt;  collate  Spanish_Colombia.utf8\n#&gt;  ctype    Spanish_Colombia.utf8\n#&gt;  tz       America/Bogota\n#&gt;  date     2025-07-17\n#&gt;  pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n#&gt;  ! package           * version  date (UTC) lib source\n#&gt;    abind               1.4-8    2024-09-12 [1] CRAN (R 4.4.1)\n#&gt;    backports           1.5.0    2024-05-23 [1] CRAN (R 4.4.0)\n#&gt;    base64enc           0.1-3    2015-07-28 [1] CRAN (R 4.4.0)\n#&gt;    bayesAB           * 1.1.3    2021-06-25 [1] CRAN (R 4.4.3)\n#&gt;    bayesplot           1.11.1   2024-02-15 [1] CRAN (R 4.4.3)\n#&gt;    bayestestR        * 0.16.1   2025-07-01 [1] CRAN (R 4.4.3)\n#&gt;    bit                 4.5.0.1  2024-12-03 [1] CRAN (R 4.4.2)\n#&gt;    bit64               4.5.2    2024-09-22 [1] CRAN (R 4.4.2)\n#&gt;    boot                1.3-31   2024-08-28 [2] CRAN (R 4.4.2)\n#&gt;    brew                1.0-10   2023-12-16 [1] CRAN (R 4.4.2)\n#&gt;    bslib               0.8.0    2024-07-29 [1] CRAN (R 4.4.2)\n#&gt;    cachem              1.1.0    2024-05-16 [1] CRAN (R 4.4.2)\n#&gt;    camtrapR          * 2.3.0    2024-02-26 [1] CRAN (R 4.4.2)\n#&gt;    cellranger          1.1.0    2016-07-27 [1] CRAN (R 4.4.2)\n#&gt;    checkmate           2.3.2    2024-07-29 [1] CRAN (R 4.4.2)\n#&gt;    class               7.3-22   2023-05-03 [2] CRAN (R 4.4.2)\n#&gt;    classInt            0.4-10   2023-09-05 [1] CRAN (R 4.4.2)\n#&gt;    cli                 3.6.3    2024-06-21 [1] CRAN (R 4.4.2)\n#&gt;    coda                0.19-4.1 2024-01-31 [1] CRAN (R 4.4.2)\n#&gt;    codetools           0.2-20   2024-03-31 [2] CRAN (R 4.4.2)\n#&gt;    colorspace          2.1-1    2024-07-26 [1] CRAN (R 4.4.2)\n#&gt;    colourpicker        1.3.0    2023-08-21 [1] CRAN (R 4.4.3)\n#&gt;    crayon              1.5.3    2024-06-20 [1] CRAN (R 4.4.2)\n#&gt;    crosstalk           1.2.1    2023-11-23 [1] CRAN (R 4.4.2)\n#&gt;    curl                6.0.0    2024-11-05 [1] CRAN (R 4.4.2)\n#&gt;    data.table          1.16.4   2024-12-06 [1] CRAN (R 4.4.2)\n#&gt;    datawizard          1.2.0    2025-07-17 [1] CRAN (R 4.4.2)\n#&gt;    DBI                 1.2.3    2024-06-02 [1] CRAN (R 4.4.2)\n#&gt;    devtools            2.4.5    2022-10-11 [1] CRAN (R 4.4.2)\n#&gt;    digest              0.6.37   2024-08-19 [1] CRAN (R 4.4.2)\n#&gt;    distributional      0.5.0    2024-09-17 [1] CRAN (R 4.4.2)\n#&gt;    dplyr             * 1.1.4    2023-11-17 [1] CRAN (R 4.4.2)\n#&gt;    DT                * 0.33     2024-04-04 [1] CRAN (R 4.4.2)\n#&gt;    dygraphs            1.1.1.6  2018-07-11 [1] CRAN (R 4.4.3)\n#&gt;    e1071               1.7-16   2024-09-16 [1] CRAN (R 4.4.2)\n#&gt;    effectsize          0.8.9    2024-07-03 [1] CRAN (R 4.4.2)\n#&gt;    elevatr           * 0.99.0   2023-09-12 [1] CRAN (R 4.4.2)\n#&gt;    ellipsis            0.3.2    2021-04-29 [1] CRAN (R 4.4.2)\n#&gt;    emmeans             1.11.1   2025-05-04 [1] CRAN (R 4.4.3)\n#&gt;    estimability        1.5.1    2024-05-12 [1] CRAN (R 4.4.3)\n#&gt;    evaluate            1.0.1    2024-10-10 [1] CRAN (R 4.4.2)\n#&gt;    farver              2.1.2    2024-05-13 [1] CRAN (R 4.4.2)\n#&gt;    fastmap             1.2.0    2024-05-15 [1] CRAN (R 4.4.2)\n#&gt;    forcats           * 1.0.0    2023-01-29 [1] CRAN (R 4.4.2)\n#&gt;    fs                  1.6.5    2024-10-30 [1] CRAN (R 4.4.2)\n#&gt;    generics            0.1.3    2022-07-05 [1] CRAN (R 4.4.2)\n#&gt;    ggplot2           * 3.5.2    2025-04-09 [1] CRAN (R 4.4.3)\n#&gt;    glue                1.8.0    2024-09-30 [1] CRAN (R 4.4.2)\n#&gt;    grateful          * 0.2.10   2024-09-04 [1] CRAN (R 4.4.2)\n#&gt;    gridExtra           2.3      2017-09-09 [1] CRAN (R 4.4.2)\n#&gt;    gtable              0.3.6    2024-10-25 [1] CRAN (R 4.4.2)\n#&gt;    gtools              3.9.5    2023-11-20 [1] CRAN (R 4.4.2)\n#&gt;    hms                 1.1.3    2023-03-21 [1] CRAN (R 4.4.2)\n#&gt;    htmltools           0.5.8.1  2024-04-04 [1] CRAN (R 4.4.2)\n#&gt;    htmlwidgets         1.6.4    2023-12-06 [1] CRAN (R 4.4.2)\n#&gt;    httpuv              1.6.15   2024-03-26 [1] CRAN (R 4.4.2)\n#&gt;    igraph              2.1.4    2025-01-23 [1] CRAN (R 4.4.2)\n#&gt;    inline              0.3.20   2024-11-10 [1] CRAN (R 4.4.2)\n#&gt;    insight             1.3.1    2025-06-30 [1] CRAN (R 4.4.3)\n#&gt;    isoband             0.2.7    2022-12-20 [1] CRAN (R 4.4.2)\n#&gt;    jquerylib           0.1.4    2021-04-26 [1] CRAN (R 4.4.2)\n#&gt;    jsonlite            1.8.9    2024-09-20 [1] CRAN (R 4.4.2)\n#&gt;    kableExtra        * 1.4.0    2024-01-24 [1] CRAN (R 4.4.2)\n#&gt;    KernSmooth          2.23-24  2024-05-17 [2] CRAN (R 4.4.2)\n#&gt;    knitr               1.49     2024-11-08 [1] CRAN (R 4.4.2)\n#&gt;    labeling            0.4.3    2023-08-29 [1] CRAN (R 4.4.0)\n#&gt;    later               1.3.2    2023-12-06 [1] CRAN (R 4.4.2)\n#&gt;    lattice             0.22-6   2024-03-20 [2] CRAN (R 4.4.2)\n#&gt;    leafem              0.2.4    2025-05-01 [1] CRAN (R 4.4.3)\n#&gt;    leaflet             2.2.2    2024-03-26 [1] CRAN (R 4.4.2)\n#&gt;    leaflet.providers   2.0.0    2023-10-17 [1] CRAN (R 4.4.2)\n#&gt;    leafpop             0.1.0    2021-05-22 [1] CRAN (R 4.4.2)\n#&gt;    lifecycle           1.0.4    2023-11-07 [1] CRAN (R 4.4.2)\n#&gt;    lme4              * 1.1-35.5 2024-07-03 [1] CRAN (R 4.4.2)\n#&gt;    logspline           2.1.22   2024-05-10 [1] CRAN (R 4.4.0)\n#&gt;    loo                 2.8.0    2024-07-03 [1] CRAN (R 4.4.2)\n#&gt;    lubridate         * 1.9.4    2024-12-08 [1] CRAN (R 4.4.2)\n#&gt;    magrittr            2.0.3    2022-03-30 [1] CRAN (R 4.4.2)\n#&gt;    mapview           * 2.11.2   2023-10-13 [1] CRAN (R 4.4.2)\n#&gt;    markdown            1.13     2024-06-04 [1] CRAN (R 4.4.2)\n#&gt;    MASS                7.3-61   2024-06-13 [2] CRAN (R 4.4.2)\n#&gt;    Matrix            * 1.7-1    2024-10-18 [2] CRAN (R 4.4.2)\n#&gt;    matrixStats         1.5.0    2025-01-07 [1] CRAN (R 4.4.2)\n#&gt;    memoise             2.0.1    2021-11-26 [1] CRAN (R 4.4.2)\n#&gt;    mgcv                1.9-1    2023-12-21 [2] CRAN (R 4.4.2)\n#&gt;    mime                0.12     2021-09-28 [1] CRAN (R 4.4.0)\n#&gt;    miniUI              0.1.1.1  2018-05-18 [1] CRAN (R 4.4.2)\n#&gt;    minqa               1.2.8    2024-08-17 [1] CRAN (R 4.4.2)\n#&gt;    multcomp            1.4-28   2025-01-29 [1] CRAN (R 4.4.3)\n#&gt;    munsell             0.5.1    2024-04-01 [1] CRAN (R 4.4.2)\n#&gt;    mvtnorm             1.3-2    2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    nlme                3.1-166  2024-08-14 [2] CRAN (R 4.4.2)\n#&gt;    nloptr              2.1.1    2024-06-25 [1] CRAN (R 4.4.2)\n#&gt;    parameters          0.23.0   2024-10-18 [1] CRAN (R 4.4.2)\n#&gt;    patchwork         * 1.3.0    2024-09-16 [1] CRAN (R 4.4.2)\n#&gt;    pbapply             1.7-2    2023-06-27 [1] CRAN (R 4.4.2)\n#&gt;    pillar              1.10.1   2025-01-07 [1] CRAN (R 4.4.2)\n#&gt;    pkgbuild            1.4.5    2024-10-28 [1] CRAN (R 4.4.2)\n#&gt;    pkgconfig           2.0.3    2019-09-22 [1] CRAN (R 4.4.2)\n#&gt;    pkgload             1.4.0    2024-06-28 [1] CRAN (R 4.4.2)\n#&gt;    plyr                1.8.9    2023-10-02 [1] CRAN (R 4.4.2)\n#&gt;    png                 0.1-8    2022-11-29 [1] CRAN (R 4.4.0)\n#&gt;    posterior           1.6.1    2025-03-12 [1] Github (jgabry/posterior@307260e)\n#&gt;    processx            3.8.4    2024-03-16 [1] CRAN (R 4.4.2)\n#&gt;    profvis             0.4.0    2024-09-20 [1] CRAN (R 4.4.2)\n#&gt;    progressr           0.15.0   2024-10-29 [1] CRAN (R 4.4.2)\n#&gt;    promises            1.3.0    2024-04-05 [1] CRAN (R 4.4.2)\n#&gt;    proxy               0.4-27   2022-06-09 [1] CRAN (R 4.4.2)\n#&gt;    ps                  1.8.1    2024-10-28 [1] CRAN (R 4.4.2)\n#&gt;    purrr             * 1.0.2    2023-08-10 [1] CRAN (R 4.4.2)\n#&gt;    quarto            * 1.4.4    2024-07-20 [1] CRAN (R 4.4.2)\n#&gt;    QuickJSR            1.4.0    2024-10-01 [1] CRAN (R 4.4.2)\n#&gt;    R.cache             0.16.0   2022-07-21 [1] CRAN (R 4.4.2)\n#&gt;    R.methodsS3         1.8.2    2022-06-13 [1] CRAN (R 4.4.0)\n#&gt;    R.oo                1.27.0   2024-11-01 [1] CRAN (R 4.4.1)\n#&gt;    R.utils             2.12.3   2023-11-18 [1] CRAN (R 4.4.2)\n#&gt;    R6                  2.6.1    2025-02-15 [1] CRAN (R 4.4.2)\n#&gt;    raster              3.6-30   2024-10-02 [1] CRAN (R 4.4.2)\n#&gt;    rbibutils           2.3      2024-10-04 [1] CRAN (R 4.4.2)\n#&gt;    Rcpp              * 1.0.13-1 2024-11-02 [1] CRAN (R 4.4.2)\n#&gt;    RcppNumerical       0.6-0    2023-09-06 [1] CRAN (R 4.4.2)\n#&gt;  D RcppParallel        5.1.9    2024-08-19 [1] CRAN (R 4.4.2)\n#&gt;    Rdpack              2.6.2    2024-11-15 [1] CRAN (R 4.4.2)\n#&gt;    readr             * 2.1.5    2024-01-10 [1] CRAN (R 4.4.2)\n#&gt;    readxl            * 1.4.3    2023-07-06 [1] CRAN (R 4.4.2)\n#&gt;    reformulas          0.4.0    2024-11-03 [1] CRAN (R 4.4.2)\n#&gt;    remotes             2.5.0    2024-03-17 [1] CRAN (R 4.4.2)\n#&gt;    renv                1.0.11   2024-10-12 [1] CRAN (R 4.4.2)\n#&gt;    reshape2            1.4.4    2020-04-09 [1] CRAN (R 4.4.2)\n#&gt;    rlang               1.1.4    2024-06-04 [1] CRAN (R 4.4.2)\n#&gt;    rmarkdown           2.29     2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    RSpectra            0.16-2   2024-07-18 [1] CRAN (R 4.4.2)\n#&gt;    rstan               2.32.6   2024-03-05 [1] CRAN (R 4.4.2)\n#&gt;    rstanarm          * 2.32.1   2024-01-18 [1] CRAN (R 4.4.3)\n#&gt;    rstantools          2.4.0    2024-01-31 [1] CRAN (R 4.4.2)\n#&gt;    rstudioapi          0.17.1   2024-10-22 [1] CRAN (R 4.4.2)\n#&gt;    sandwich            3.1-1    2024-09-15 [1] CRAN (R 4.4.2)\n#&gt;    sass                0.4.9    2024-03-15 [1] CRAN (R 4.4.2)\n#&gt;    satellite           1.0.5    2024-02-10 [1] CRAN (R 4.4.2)\n#&gt;    scales              1.3.0    2023-11-28 [1] CRAN (R 4.4.2)\n#&gt;    secr                5.1.0    2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    sessioninfo         1.2.2    2021-12-06 [1] CRAN (R 4.4.2)\n#&gt;    sf                * 1.0-21   2025-05-15 [1] CRAN (R 4.4.3)\n#&gt;    shiny               1.9.1    2024-08-01 [1] CRAN (R 4.4.2)\n#&gt;    shinyjs             2.1.0    2021-12-23 [1] CRAN (R 4.4.3)\n#&gt;    shinystan           2.6.0    2022-03-03 [1] CRAN (R 4.4.3)\n#&gt;    shinythemes         1.2.0    2021-01-25 [1] CRAN (R 4.4.3)\n#&gt;    sp                  2.1-4    2024-04-30 [1] CRAN (R 4.4.2)\n#&gt;    StanHeaders         2.32.10  2024-07-15 [1] CRAN (R 4.4.2)\n#&gt;    stringi             1.8.4    2024-05-06 [1] CRAN (R 4.4.0)\n#&gt;    stringr           * 1.5.1    2023-11-14 [1] CRAN (R 4.4.2)\n#&gt;    styler            * 1.10.3   2024-04-07 [1] CRAN (R 4.4.2)\n#&gt;    survival            3.7-0    2024-06-05 [2] CRAN (R 4.4.2)\n#&gt;    svglite             2.1.3    2023-12-08 [1] CRAN (R 4.4.2)\n#&gt;    systemfonts         1.1.0    2024-05-15 [1] CRAN (R 4.4.2)\n#&gt;    tensorA             0.36.2.1 2023-12-13 [1] CRAN (R 4.4.0)\n#&gt;    terra             * 1.8-21   2025-02-10 [1] CRAN (R 4.4.2)\n#&gt;    TH.data             1.1-3    2025-01-17 [1] CRAN (R 4.4.3)\n#&gt;    threejs             0.3.4    2025-04-21 [1] CRAN (R 4.4.3)\n#&gt;    tibble            * 3.2.1    2023-03-20 [1] CRAN (R 4.4.2)\n#&gt;    tidyr             * 1.3.1    2024-01-24 [1] CRAN (R 4.4.2)\n#&gt;    tidyselect          1.2.1    2024-03-11 [1] CRAN (R 4.4.2)\n#&gt;    tidyverse         * 2.0.0    2023-02-22 [1] CRAN (R 4.4.2)\n#&gt;    timechange          0.3.0    2024-01-18 [1] CRAN (R 4.4.2)\n#&gt;    tzdb                0.4.0    2023-05-12 [1] CRAN (R 4.4.2)\n#&gt;    ubms              * 1.2.7    2024-10-01 [1] CRAN (R 4.4.2)\n#&gt;    units               0.8-5    2023-11-28 [1] CRAN (R 4.4.2)\n#&gt;    unmarked          * 1.5.0    2025-02-10 [1] CRAN (R 4.4.3)\n#&gt;    urlchecker          1.0.1    2021-11-30 [1] CRAN (R 4.4.2)\n#&gt;    usethis             3.0.0    2024-07-29 [1] CRAN (R 4.4.1)\n#&gt;    uuid                1.2-1    2024-07-29 [1] CRAN (R 4.4.1)\n#&gt;    V8                  6.0.0    2024-10-12 [1] CRAN (R 4.4.2)\n#&gt;    vctrs               0.6.5    2023-12-01 [1] CRAN (R 4.4.2)\n#&gt;    viridisLite         0.4.2    2023-05-02 [1] CRAN (R 4.4.2)\n#&gt;    vroom               1.6.5    2023-12-05 [1] CRAN (R 4.4.2)\n#&gt;    withr               3.0.2    2024-10-28 [1] CRAN (R 4.4.2)\n#&gt;    xfun                0.49     2024-10-31 [1] CRAN (R 4.4.2)\n#&gt;    xml2                1.3.6    2023-12-04 [1] CRAN (R 4.4.2)\n#&gt;    xtable              1.8-4    2019-04-21 [1] CRAN (R 4.4.2)\n#&gt;    xts                 0.14.1   2024-10-15 [1] CRAN (R 4.4.2)\n#&gt;    yaml                2.3.10   2024-07-26 [1] CRAN (R 4.4.1)\n#&gt;    zoo                 1.8-12   2023-04-13 [1] CRAN (R 4.4.2)\n#&gt; \n#&gt;  [1] C:/Users/usuario/AppData/Local/R/win-library/4.4\n#&gt;  [2] C:/Program Files/R/R-4.4.2/library\n#&gt; \n#&gt;  D ── DLL MD5 mismatch, broken installation.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#fitting-a-stacked-model-for-the-jaguar",
    "href": "posts/2024-07-17-stackmodel/index.html#fitting-a-stacked-model-for-the-jaguar",
    "title": "“Stacked” Models",
    "section": "Fitting a stacked model for the Jaguar",
    "text": "Fitting a stacked model for the Jaguar\nLets use the ubms package to make a stacked occupancy model pooling 2021 and 2022 data together and use the percent tree cover, the road density and the cattle density as covariates for the occupancy and the effort as the number of sampling days as covariate for detection.\nLoad the data\n\nCodejaguar &lt;- read.csv(\"C:/CodigoR/CameraTrapCesar/data/katios/stacked/y_jaguar_stacked.csv\")\n\n\nLook at the data\n\nCode\ndatatable(head(jaguar))\n\n\n\n\n\nNotice we collapsed the events to 15 days in the 2021 sampling season, and to 25 days in the 2022 sampling season, to end with 6 repeated observations in de matrix. In the matrix o1 to o6 are observations and e1 to e6 are sampling effort (observation-detection covariates). Land_cover, per_tree_cov and roads are site covariates (occupancy covariate).\nCreate an unmarked frame\nWith our stacked dataset constructed, we build the unmarkedFrame() object.\n\nCode\n# fix NA spread\n# yj &lt;- rbind(ylist[[62]][1:30,1:8], # 62 is Jaguar\n#             ylist[[62]][31:50,12:19])\n\n# ej &lt;- rbind(effortlist[[4]][1:30,1:8],\n#             effortlist[[4]][31:50,12:19])\n    \n  \njaguar_covs &lt;- jaguar[,c(8,9,16:19)]\njaguar_covs$year &lt;- as.factor(jaguar_covs$year)\n\numf &lt;- unmarkedFrameOccu(y=jaguar[,2:7], \n                         siteCovs=jaguar_covs,\n                         obsCovs=list(effort=jaguar[10:15])\n                      )\n\nplot(umf)\n\n\n\n\n\n\n\nFit models\nFit the Stacked Model\nWe’ll now we fit a model with fixed effects of percent tree cover road density and cattle density (per_tree_cov, roads and cattle) on occupancy, and a effort as the detection covariate. In addition, we will include random intercepts by site, since in stacking the data we have pseudoreplication by site. To remember, random effects are specified using the same notation used in with the lme4 package. For example, a random intercept for each level of the covariate site is specified with the formula component (1|site). Take in to account, Including random effects in a model in ubms usually significantly increases the run time, but at the end is worth the waiting time.\nNext we perform model selection.\n\nCode# fit_0 &lt;- occu(~1~1, data=umf) # unmarked\n\nfit_j0 &lt;- stan_occu(~1~1 + (1|site),\n                       data=umf, chains=3, iter=50000, cores=3)\nfit_j1 &lt;- stan_occu(~scale(effort) ~1 + (1|site), \n                       data=umf, chains=3, iter=50000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 0.000148 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.48 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 83.81 seconds (Warm-up)\n#&gt; Chain 1:                92.207 seconds (Sampling)\n#&gt; Chain 1:                176.017 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 0.000151 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.51 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 67.933 seconds (Warm-up)\n#&gt; Chain 2:                76.674 seconds (Sampling)\n#&gt; Chain 2:                144.607 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 9.7e-05 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.97 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 62.544 seconds (Warm-up)\n#&gt; Chain 3:                147.879 seconds (Sampling)\n#&gt; Chain 3:                210.423 seconds (Total)\n#&gt; Chain 3:\nfit_j2 &lt;- stan_occu(~scale(effort) ~scale(per_tree_cov) + (1|site), \n                       data=umf, chains=3, iter=50000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 0.000115 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.15 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 66.409 seconds (Warm-up)\n#&gt; Chain 1:                94.039 seconds (Sampling)\n#&gt; Chain 1:                160.448 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 0.000111 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.11 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 54.559 seconds (Warm-up)\n#&gt; Chain 2:                77.247 seconds (Sampling)\n#&gt; Chain 2:                131.806 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 0.000101 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.01 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 67.15 seconds (Warm-up)\n#&gt; Chain 3:                81.822 seconds (Sampling)\n#&gt; Chain 3:                148.972 seconds (Total)\n#&gt; Chain 3:\nfit_j3 &lt;- stan_occu(~scale(effort) ~scale(roads) + (1|site), \n                       data=umf, chains=3, iter=50000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 9.9e-05 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.99 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 62.839 seconds (Warm-up)\n#&gt; Chain 1:                78.627 seconds (Sampling)\n#&gt; Chain 1:                141.466 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 0.000152 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.52 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 46.363 seconds (Warm-up)\n#&gt; Chain 2:                56.899 seconds (Sampling)\n#&gt; Chain 2:                103.262 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 9.5e-05 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.95 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 54.824 seconds (Warm-up)\n#&gt; Chain 3:                63.61 seconds (Sampling)\n#&gt; Chain 3:                118.434 seconds (Total)\n#&gt; Chain 3:\nfit_j4 &lt;- stan_occu(~scale(effort) ~scale(cattle) + (1|site), \n                       data=umf, chains=3, iter=50000)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 9.6e-05 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 55.618 seconds (Warm-up)\n#&gt; Chain 1:                103.187 seconds (Sampling)\n#&gt; Chain 1:                158.805 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 9.8e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.98 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 65.019 seconds (Warm-up)\n#&gt; Chain 2:                97.077 seconds (Sampling)\n#&gt; Chain 2:                162.096 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'occu' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 0.000101 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.01 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:     1 / 50000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  5000 / 50000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 10000 / 50000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 15000 / 50000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 20000 / 50000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 25000 / 50000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 25001 / 50000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 30000 / 50000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 35000 / 50000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 40000 / 50000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 45000 / 50000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 50000 / 50000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 57.462 seconds (Warm-up)\n#&gt; Chain 3:                158.944 seconds (Sampling)\n#&gt; Chain 3:                216.406 seconds (Total)\n#&gt; Chain 3:\n# compare\nmodels &lt;- list(Null = fit_j0,\n                effort = fit_j1,\n                effort_treecov = fit_j2,\n                effort_road = fit_j3,\n                effort_cattle = fit_j4)\n\nmods &lt;- fitList(fits = models)\n\n\n## see model selection as a table\ndatatable( \n  round(modSel(mods), 3)\n  )\n\n\n\n\n\nInstead of AIC, models are compared using leave-one-out cross-validation (LOO) (Vehtari, Gelman, and Gabry 2017) via the loo package. Based on this cross-validation, the expected predictive accuracy (elpd) for each model is calculated. The model with the largest elpd (effort_cattle) performed best. The looic value is analogous to AIC.\n\nCodeloo(fit_j4)\n#&gt; \n#&gt; Computed from 75000 by 53 log-likelihood matrix.\n#&gt; \n#&gt;          Estimate   SE\n#&gt; elpd_loo    -54.8 13.5\n#&gt; p_loo         4.4  1.0\n#&gt; looic       109.6 27.0\n#&gt; ------\n#&gt; MCSE of elpd_loo is 0.0.\n#&gt; MCSE and ESS estimates assume MCMC draws (r_eff in [0.0, 0.7]).\n#&gt; \n#&gt; All Pareto k estimates are good (k &lt; 0.7).\n#&gt; See help('pareto-k-diagnostic') for details.\n\n\n\nBest model is effort_cattle (fit_j4) which has effort on detection and percent tree cover on occupancy.\n\n\nCodefit_j4\n#&gt; \n#&gt; Call:\n#&gt; stan_occu(formula = ~scale(effort) ~ scale(cattle) + (1 | site), \n#&gt;     data = umf, chains = 3, iter = 50000)\n#&gt; \n#&gt; Occupancy (logit-scale):\n#&gt;                Estimate    SD    2.5% 97.5% n_eff Rhat\n#&gt; (Intercept)      -1.299 0.764 -2.6867 0.399 13673    1\n#&gt; scale(cattle)    -1.224 1.050 -3.7065 0.373 14710    1\n#&gt; sigma [1|site]    0.705 0.677  0.0514 2.512   784    1\n#&gt; \n#&gt; Detection (logit-scale):\n#&gt;               Estimate    SD   2.5%  97.5% n_eff Rhat\n#&gt; (Intercept)     -1.625 0.485 -2.670 -0.768  8325    1\n#&gt; scale(effort)    0.324 0.354 -0.331  1.048 12612    1\n#&gt; \n#&gt; LOOIC: 109.552\n#&gt; Runtime: 8.955 min\n\n\nLooking at the summary of fit_j4, we conclude MCMC chains have converged if all R^&gt;1.05 To visualize convergence, look at the traceplots:\n\nCodetraceplot(fit_j4, pars=c(\"beta_state\", \"beta_det\"))\n\n\n\n\n\n\n\nEvaluate model fit\nStatistic should be near 0.5 if the model fits well.\n\nCode# eval\nfit_top_gof &lt;- gof(fit_j4, draws=500, quiet=TRUE)\nfit_top_gof\n#&gt; MacKenzie-Bailey Chi-square \n#&gt; Point estimate = 54.244\n#&gt; Posterior predictive p = 0.424\n\nplot(fit_top_gof)\n\n\n\n\n\n\n\nModel inference\nEffort in detection and cattle density in occupancy\n\nCodeubms::plot_effects(fit_j4, \"det\")\n\n\n\n\n\n\nCodeubms::plot_effects(fit_j4, \"state\")"
  },
  {
    "objectID": "posts/2024-12-10-RAI/index.html#foto-de-los-participantes-del-curso",
    "href": "posts/2024-12-10-RAI/index.html#foto-de-los-participantes-del-curso",
    "title": "Esfuerzo de muestreo y RAI en Fototrampeo",
    "section": "Foto de los participantes del curso:",
    "text": "Foto de los participantes del curso:"
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html",
    "href": "posts/2024-12-15-riqueza/index.html",
    "title": "Riqueza de especies",
    "section": "",
    "text": "Hay dos formas de uso común para tener en cuenta el esfuerzo de muestreo al estimar la riqueza de especies mediante cámaras trampa:\n\nUtilizando la rarefacción de la riqueza observada.\nUtilizando modelos de ocupación multiespecie para tener en cuenta las especies presentes pero no observadas (teniendo en cuenta la detección imperfecta).\n\nEn este post podemos ver un ejemplo del número 1 utilizando el enfoque clásico de la ecología de comunidades usando el paquete vegan. El paquete vegan (https://cran.r-project.org/package=vegan) proporciona herramientas para describir la ecología de las comunidades. Este paquete tiene funciones básicas de análisis de diversidad, ordenación de comunidades y análisis de disimilitud. El paquete vegan proporciona la mayoría de las herramientas estándar para el análisis descriptivo de comunidades principalmente de comunidades vegetales, pero sus conceptos tambien pueden ser aplicados a la fauna. Más adelante en este post realizamos otro análisis de diversidad utilizando algunas funciones del paquete iNEXT.\nEl enfoque moderno para medir la diversidad de especies incluye los “numeros de Hill”. La rarefacción y la extrapolación con números de Hill han ganado popularidad en la última década y se pueden calcular utilizando la función renyi en el paquete vegan (Oksanen 2016) y la función rarity en el paquete MeanRarity (Roswell y Dushoff 2020), y las diversidades de Hill de muestras de igual tamaño o igual cobertura se pueden comparar utilizando las funciones iNEXT y estimateD del paquete iNEXT (Hsieh et al. 2016). Las estimaciones para valores asintóticos de diversidad de Hill están tambien disponibles en el paquete SpadeR (Chao y Jost 2015, Chao et al. 2015)."
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#riqueza-de-especies-y-esfuerzo-de-muestreo",
    "href": "posts/2024-12-15-riqueza/index.html#riqueza-de-especies-y-esfuerzo-de-muestreo",
    "title": "Riqueza de especies",
    "section": "",
    "text": "Hay dos formas de uso común para tener en cuenta el esfuerzo de muestreo al estimar la riqueza de especies mediante cámaras trampa:\n\nUtilizando la rarefacción de la riqueza observada.\nUtilizando modelos de ocupación multiespecie para tener en cuenta las especies presentes pero no observadas (teniendo en cuenta la detección imperfecta).\n\nEn este post podemos ver un ejemplo del número 1 utilizando el enfoque clásico de la ecología de comunidades usando el paquete vegan. El paquete vegan (https://cran.r-project.org/package=vegan) proporciona herramientas para describir la ecología de las comunidades. Este paquete tiene funciones básicas de análisis de diversidad, ordenación de comunidades y análisis de disimilitud. El paquete vegan proporciona la mayoría de las herramientas estándar para el análisis descriptivo de comunidades principalmente de comunidades vegetales, pero sus conceptos tambien pueden ser aplicados a la fauna. Más adelante en este post realizamos otro análisis de diversidad utilizando algunas funciones del paquete iNEXT.\nEl enfoque moderno para medir la diversidad de especies incluye los “numeros de Hill”. La rarefacción y la extrapolación con números de Hill han ganado popularidad en la última década y se pueden calcular utilizando la función renyi en el paquete vegan (Oksanen 2016) y la función rarity en el paquete MeanRarity (Roswell y Dushoff 2020), y las diversidades de Hill de muestras de igual tamaño o igual cobertura se pueden comparar utilizando las funciones iNEXT y estimateD del paquete iNEXT (Hsieh et al. 2016). Las estimaciones para valores asintóticos de diversidad de Hill están tambien disponibles en el paquete SpadeR (Chao y Jost 2015, Chao et al. 2015)."
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#cargar-paquetes",
    "href": "posts/2024-12-15-riqueza/index.html#cargar-paquetes",
    "title": "Riqueza de especies",
    "section": "Cargar Paquetes",
    "text": "Cargar Paquetes\n\nCode\n\nlibrary(patchwork) # The Composer of Plots\nlibrary(readxl) # Read Excel Files\nlibrary(sf) # Simple Features for R\nlibrary(elevatr) # Access Elevation Data from Various APIs\nlibrary(mapview) # Interactive Viewing of Spatial Data in R\nlibrary(tmap) # nice maps in R\nlibrary(eks) # make countours\nlibrary(grateful) # Facilitate Citation of R Packages\nlibrary(camtrapR) # Camera Trap Data Management and Preparation of Occupancy and Spatial Capture-Recapture Analyses\nlibrary(vegan) # Community Ecology Package \nlibrary(ggvegan) # vegan adaptation for ggplot\n# library(BiodiversityR) # cause error!\nlibrary(ggordiplots)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(DT) # tablas en R\nlibrary(MeanRarity)\nlibrary(SpadeR)\nlibrary(iNEXT) # Interpolation and Extrapolation for Species Diversity\nlibrary(knitr) # A General-Purpose Package for Dynamic Report Generation in R\nlibrary(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax\nlibrary(tidyverse) # Easily Install and Load the 'Tidyverse'\nlibrary(ggforce) # Accelerating 'ggplot2'\nlibrary(plotly)"
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#cargar-los-datos",
    "href": "posts/2024-12-15-riqueza/index.html#cargar-los-datos",
    "title": "Riqueza de especies",
    "section": "Cargar los Datos",
    "text": "Cargar los Datos\n\nCode\ndatos &lt;- read_excel(\"C:/CodigoR/CameraTrapCesar/data/CT_Cesar.xlsx\")\n\n# habitat types extracted from Copernicus\nhabs &lt;- read.csv(\"C:/CodigoR/CameraTrapCesar/data/habitats.csv\")"
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#agrupación-de-varios-sitios",
    "href": "posts/2024-12-15-riqueza/index.html#agrupación-de-varios-sitios",
    "title": "Riqueza de especies",
    "section": "Agrupación de varios sitios",
    "text": "Agrupación de varios sitios\nPara este ejemplo usaremos datos tomados por la fundación Galictis en varias localidade de la Serrania del Perija. En este caso seleccioné un año para los sitios: Becerril 2021, LaPaz_Manaure 2019, MLJ, CL1, CL2 y PCF. Algunas veces, necesitamos crear códigos únicos por cámara y tabla que relaciona las fechas de operacion de cada camara, Sin embargo, este no fue el caso.\nPara este ejemplo, usamos el tipo de hábitat donde se instaló la cámara como una categoria para comparar el esfuerzo de muestreo (número de cámaras) por tipo de hábitat. El tipo de hábitat se extrajo superponiendo los puntos de la cámara sobre el conjunto de datos global de cobertura terrestre de 100 m de COPERNICUS utilizando el Google Earth Engine conectado a R. Cómo hacer esto se explicará en otra publicación.\n\nCode# make a new column Station\n# datos_PCF &lt;- datos |&gt; dplyr::filter(Proyecto==\"CT_LaPaz_Manaure\") |&gt; unite (\"Station\", ProyectoEtapa:Salida:CT, sep = \"-\")\n\n# fix dates\ndatos$Start &lt;- as.Date(datos$Start, \"%d/%m/%Y\")\ndatos$End &lt;- as.Date(datos$End, \"%d/%m/%Y\")\ndatos$eventDate &lt;- as.Date(datos$eventDate, \"%d/%m/%Y\")\ndatos$eventDateTime &lt;- ymd_hms(paste(datos$eventDate, \" \",\n                              datos$eventTime, \":00\", sep=\"\"))\n\n# filter Becerril\ndatos_Becerril &lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"CT_Becerril\") |&gt; mutate (Station=IdGeo) |&gt; filter(Year==2021)\n\n# filter LaPaz_Manaure\ndatos_LaPaz_Manaure&lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"CT_LaPaz_Manaure\") |&gt; mutate (Station=IdGeo) |&gt; filter(Year==2019)\n\n# filter MLJ\ndatos_MLJ &lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"MLJ_TH_TS_2021\") |&gt; mutate (Station=IdGeo)\n\n# filter CL\ndatos_CL1 &lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"CL-TH2022\") |&gt; mutate (Station=IdGeo)\n# filter CL\ndatos_CL2 &lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"CL-TS2022\") |&gt; mutate (Station=IdGeo)\n\n# filter PCF\ndatos_PCF &lt;- datos |&gt; dplyr::filter(Proyecto==\"PCF\") |&gt; mutate (Station=IdGeo)\n\ndata_south &lt;- rbind(datos_LaPaz_Manaure, datos_Becerril, datos_MLJ,datos_CL1, datos_CL2,datos_PCF)\n\n# filter 2021 and make uniques\nCToperation  &lt;- data_south |&gt; \n              # filter(Year==2021) |&gt; \n              group_by(Station) |&gt; \n              mutate(minStart=min(Start), maxEnd=max(End)) |&gt;  distinct(Longitude, Latitude, minStart, maxEnd, Year) |&gt; \n  ungroup()"
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#generar-la-tabla-cameraoperation-y-realizar-las-historias-de-detección-para-todas-las-especies",
    "href": "posts/2024-12-15-riqueza/index.html#generar-la-tabla-cameraoperation-y-realizar-las-historias-de-detección-para-todas-las-especies",
    "title": "Riqueza de especies",
    "section": "Generar la tabla cameraOperation y realizar las historias de detección para todas las especies",
    "text": "Generar la tabla cameraOperation y realizar las historias de detección para todas las especies\nEl paquete CamtrapR tiene la función cameraOperation que realiza una tabla de cámaras (estaciones) y fechas (setup, puck-up), esta tabla es la clave para generar las historias de detección utilizando la función detectionHistory en el siguiente paso. Para simplificar la matriz y facilitar la convergencia del modelo estamos colapsando los datos a 7 dias.\n\nCode# Generamos la matríz de operación de las cámaras\n\ncamop &lt;- cameraOperation(CTtable= CToperation, # Tabla de operación\n                         stationCol= \"Station\", # Columna que define la estación\n                         setupCol= \"minStart\", #Columna fecha de colocación\n                         retrievalCol= \"maxEnd\", #Columna fecha de retiro\n                         #hasProblems= T, # Hubo fallos de cámaras\n                         dateFormat= \"%Y-%m-%d\") #, # Formato de las fechas\n                         #cameraCol=\"CT\")\n                         # sessionCol= \"Year\")\n\n# Generar las historias de detección ---------------------------------------\n## remove plroblem species\n# ind &lt;- which(datos_PCF$Species==\"Marmosa sp.\")\n# datos_PCF &lt;- datos_PCF[-ind,]\n\nDetHist_list &lt;- lapply(unique(data_south$Species), FUN = function(x) {\n  detectionHistory(\n    recordTable         = data_south, # Tabla de registros\n    camOp                = camop, # Matriz de operación de cámaras\n    stationCol           = \"Station\",\n    speciesCol           = \"Species\",\n    recordDateTimeCol    = \"eventDateTime\",\n    recordDateTimeFormat  = \"%Y-%m-%d\",\n    species              = x,     # la función reemplaza x por cada una de las especies\n    occasionLength       = 7, # Colapso de las historias a 10 días\n    day1                 = \"station\", # (\"survey\"),or #inicia en la fecha de cada station\n    datesAsOccasionNames = FALSE,\n    includeEffort        = TRUE,\n    scaleEffort          = FALSE,\n    output               = (\"binary\"), # (\"binary\") or (\"count\")\n    #unmarkedMultFrameInput=TRUE\n    timeZone             = \"America/Bogota\" \n    )\n  }\n)\n\n# put names to the species \nnames(DetHist_list) &lt;- unique(data_south$Species)\n\n# Finally we make a new list to put all the detection histories.\nylist &lt;- lapply(DetHist_list, FUN = function(x) x$detection_history)"
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#usemos-los-historiales-de-detección-para-crear-una-matriz-para-vegan-y-la-matriz-de-incidencia-para-inext.",
    "href": "posts/2024-12-15-riqueza/index.html#usemos-los-historiales-de-detección-para-crear-una-matriz-para-vegan-y-la-matriz-de-incidencia-para-inext.",
    "title": "Riqueza de especies",
    "section": "Usemos los historiales de detección para crear una matriz para vegan y la matriz de incidencia para iNEXT.",
    "text": "Usemos los historiales de detección para crear una matriz para vegan y la matriz de incidencia para iNEXT.\nLas curvas de acumulación de especies, creadas con el paquete vegan, representan gráficamente el aumento de la riqueza de especies a medida que se añaden mas unidades de muestreo. Si la curva se estabiliza (se aplana o alcanza una asintota), esto indica que se ha muestreado la mayoría de las especies en el sitio de muestreo (en este caso la cámara o tipo de hábitat).\nPrimero creamos una matriz para vegan y luego modificamos la matriz para crear la matriz de incidencias que se usa en el paquete iNEXT, la cual es una lista.\n\nCode# loop to make vegan matrix\nmat_vegan &lt;- matrix(NA, dim(ylist[[1]])[1], length(unique(data_south$Species)))\nfor(i in 1:length(unique(data_south$Species))){\n  mat_vegan[,i] &lt;- apply(ylist[[i]], 1, sum, na.rm=TRUE)\n  mat_vegan[,i] &lt;- tidyr::replace_na(mat_vegan[,i], 0) # replace na with 0\n}\n\ncolnames(mat_vegan)  &lt;- unique(data_south$Species)\nrownames(mat_vegan) &lt;- rownames(ylist[[1]])\n\nmat_vegan2 &lt;- as.data.frame(mat_vegan)\nmat_vegan2$hab &lt;- habs$hab_code\n# mat_vegan3 &lt;-  mat_vegan2 |&gt; \n  \n# ver la estructura de la matriz\ndatatable(head(mat_vegan))\n\n\n\n\nCode\n# Select specific rows by row numbers\nclosed_forest_rows &lt;- which(mat_vegan2$hab==\"closed_forest_evergreen_broad\")\nherbaceous_rows &lt;- which(mat_vegan2$hab==\"herbaceous_wetland\")\nherbs_rows &lt;- which(mat_vegan2$hab==\"herbs\")\nopen_forest_rows &lt;- which(mat_vegan2$hab==\"open_forest_evergreen_broad\")\nopen_forest2_rows &lt;- which(mat_vegan2$hab==\"open_forest_other\")\n\n\nclosed_forest &lt;- apply(mat_vegan2[closed_forest_rows,1:22], MARGIN = 2, sum)\nherbaceous_wetland &lt;- apply(mat_vegan2[herbaceous_rows,1:22], MARGIN = 2, sum)\nherbs  &lt;- apply(mat_vegan2[herbs_rows,1:22], MARGIN = 2, sum)\nopen_forest_evergreen &lt;- apply(mat_vegan2[open_forest_rows,1:22], MARGIN = 2, sum)\nopen_forest_other &lt;- apply(mat_vegan2[open_forest2_rows,1:22], MARGIN = 2, sum)\n\n# tb_sp &lt;- mat_vegan2 |&gt; group_by(hab)\n# hab_list &lt;- group_split(tb_sp)\n\n# make list of dataframe per habitat\nsp_by_hab &lt;- mat_vegan2 |&gt; dplyr::group_by(hab) %&gt;% split (.$hab)\n# arrange abundance (detection frecuency) mat for INEXT \ncesar_sp &lt;- t(rbind(\nt(colSums(sp_by_hab[[1]][,1:33])),\nt(colSums(sp_by_hab[[2]][,1:33])),\nt(colSums(sp_by_hab[[3]][,1:33])),\nt(colSums(sp_by_hab[[4]][,1:33])),\nt(colSums(sp_by_hab[[5]][,1:33]))\n))\n \ncolnames(cesar_sp) &lt;- names(sp_by_hab)\n\n\n\n# function to Format data to incidence and use iNext\nf_incidences &lt;- function(habitat_rows=closed_forest_rows){ylist %&gt;%  # historias de detection\n  map(~rowSums(.,na.rm = T)) %&gt;% # sumo las detecciones en cada sitio\n  reduce(cbind) %&gt;% # unimos las listas\n  as_data_frame() %&gt;% #formato dataframe\n  filter(row_number() %in% habitat_rows) |&gt; \n  t() %&gt;% # trasponer la tabla\n  as_tibble() %&gt;% #formato tibble\n  mutate_if(is.numeric,~(.&gt;=1)*1) %&gt;%  #como es incidencia, formateo a 1 y 0\n  rowSums() %&gt;%  # ahora si la suma de las incidencias en cada sitio\n  sort(decreasing=T) |&gt; \n  as_tibble() %&gt;% \n  add_row(value= length(habitat_rows), .before = 1) %&gt;%  # requiere que el primer valor sea el número de sitios\n  filter(!if_any()==0) |&gt;  # filter ceros\n  as.matrix() # Requiere formato de matriz\n}\n\n# Make incidence frequency table (is a list whit 5 habitats)\n# Make an empty list to store our data\nincidence_cesar &lt;- list() \nincidence_cesar[[1]] &lt;- f_incidences(closed_forest_rows)\nincidence_cesar[[2]] &lt;- f_incidences(herbaceous_rows)\nincidence_cesar[[3]] &lt;- f_incidences(herbs_rows)\nincidence_cesar[[4]] &lt;- f_incidences(open_forest_rows)\nincidence_cesar[[5]] &lt;- f_incidences(open_forest_other)\n\n# put names\nnames(incidence_cesar) &lt;- names(sp_by_hab)\n\n# we deleted this habitat type for making error\nincidence_cesar &lt;- within(incidence_cesar, rm(\"herbaceous_wetland\")) \n\n# ver la estructura de la matriz de incidencias\nstr(incidence_cesar)\n#&gt; List of 4\n#&gt;  $ closed_forest_evergreen_broad: num [1:31, 1] 41 20 16 15 15 15 14 13 13 10 ...\n#&gt;   ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. ..$ : NULL\n#&gt;   .. ..$ : chr \"value\"\n#&gt;  $ herbs                        : num [1:29, 1] 35 19 19 16 13 12 11 10 9 8 ...\n#&gt;   ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. ..$ : NULL\n#&gt;   .. ..$ : chr \"value\"\n#&gt;  $ open_forest_evergreen_broad  : num [1:18, 1] 7 5 5 4 3 3 2 2 2 2 ...\n#&gt;   ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. ..$ : NULL\n#&gt;   .. ..$ : chr \"value\"\n#&gt;  $ open_forest_other            : num [1:16, 1] 22 4 3 2 2 1 1 1 1 1 ...\n#&gt;   ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. ..$ : NULL\n#&gt;   .. ..$ : chr \"value\""
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#para-comenzar-graficaremos-las-especies-versus-los-sitios",
    "href": "posts/2024-12-15-riqueza/index.html#para-comenzar-graficaremos-las-especies-versus-los-sitios",
    "title": "Riqueza de especies",
    "section": "Para comenzar, graficaremos las especies versus los sitios",
    "text": "Para comenzar, graficaremos las especies versus los sitios\n\nCode# Transpose if needed to have sample site names on rows\nabund_table&lt;-mat_vegan\n# Convert to relative frequencies\nabund_table &lt;- abund_table/rowSums(abund_table)\nlibrary(reshape2)\ndf&lt;-melt(abund_table)\ncolnames(df)&lt;-c(\"Sampled_site\",\"Species\",\"Value\")\nlibrary(plyr)\nlibrary(scales)\n \n# We are going to apply transformation to our data to make it\n# easier on eyes \n \n#df&lt;-ddply(df,.(Samples),transform,rescale=scale(Value))\ndf&lt;-ddply(df,.(Sampled_site),transform,rescale=sqrt(Value))\n \n# Plot heatmap\np &lt;- ggplot(df, aes(Species, Sampled_site)) + \n  geom_tile(aes(fill = rescale),colour = \"white\") + \n  scale_fill_gradient(low = \"white\",high = \"#1E5A8C\")+\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) + theme(legend.position = \"none\",axis.ticks = element_blank(),axis.text.x = element_text(angle = 90, hjust = 1,size=6),axis.text.y = element_text(size=4))\n\n# ggplotly(p) # see interactive\n# View the plot\np\n\n\n\n\n\n\n\n\nObserve cómo algunas cámaras no registraron ninguna especie. Aquí se muestra como la línea horizontal gis. Tal vez debamos eliminar esas cámaras."
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#rarefacción-usando-vegan",
    "href": "posts/2024-12-15-riqueza/index.html#rarefacción-usando-vegan",
    "title": "Riqueza de especies",
    "section": "Rarefacción usando vegan\n",
    "text": "Rarefacción usando vegan\n\n\nTenga en cuenta que los sitios son cámaras y la acumulación es de especies por cámara, no de tiempo.\n\nLa rarefacción es una técnica para evaluar la riqueza de especies esperada. La rarefacción permite calcular la riqueza de especies para un número determinado de muestras individuales, basándose en la construcción de curvas de rarefacción.\nEl problema que se produce al muestrear varias especies en una comunidad es que cuanto mayor sea el número de individuos muestreados, más especies se encontrarán. Las curvas de rarefacción se crean muestreando aleatoriamente el conjunto de N muestras varias veces y luego trazando el número promedio de especies encontradas en cada muestra (1,2, … N). “Por lo tanto, la rarefacción genera el número esperado de especies en una pequeña colección de n individuos (o n muestras) extraídos al azar del gran conjunto de N muestras”. Las curvas de rarefacción generalmente crecen rápidamente al principio, a medida que se encuentran las especies más comunes, pero las curvas se estabilizan a medida que solo quedan por muestrear las especies más raras.\n\nCode\nrarecurve(mat_vegan, col = \"blue\") \n\n\n\n\n\n\nCoderarecurve(t(cesar_sp), col = \"blue\") \n\n\n\n\n\n\nCode\nsp1 &lt;- specaccum(mat_vegan)\nsp2 &lt;- specaccum(mat_vegan, \"random\")\n# sp2\n# summary(sp2)\nplot(sp1, ci.type=\"poly\", col=\"blue\", lwd=2, ci.lty=0, ci.col=\"lightblue\")\n\n\n\n\n\n\nCode# boxplot(sp2, col=\"yellow\", add=TRUE, pch=\"+\")\n\n\nmods &lt;- fitspecaccum(sp1, \"gleason\")\nplot(mods, col=\"hotpink\")\nboxplot(sp2, col = \"yellow\", border = \"blue\", lty=1, cex=0.3, add= TRUE)\n\n\n\n\n\n\nCode\n\n## Accumulation model\npool &lt;- poolaccum(mat_vegan)\n# summary(pool, display = \"chao\")\nplot(pool)\n\n\n\n\n\n\n\nDistribución de abundancia clasificada\nUn enfoque alternativo para la distribución de la abundancia de especies es representar gráficamente las abundancias logarítmicas en orden decreciente o en función de los rangos de especies.\n\nCodek &lt;- sample(nrow(mat_vegan), 1)\nrad &lt;- radfit(mat_vegan[22,]) # species 22\n# plot(rad)\nradlattice(rad)\n\n\n\n\n\n\n\nDiversidad de Hill usando el paquete vegan\n\n\nCode# data(BCI)\ni &lt;- sample(nrow(mat_vegan), 20)\nmod &lt;- renyi(mat_vegan) #selecting sites with more than one record\nplot(mod)\n\n\n\n\n\n\nCodemod &lt;- renyiaccum(mat_vegan[55:89,])\nplot(mod, as.table=TRUE, col = c(1, 2, 2))\n\n\n\n\n\n\nCodepersp(mod)\n\n\n\n\n\n\n\nNúmero total de especies\n\nCodeDT::datatable(round(specpool(mat_vegan),3))\n\n\n\n\n\nNumero de especies no vistas en cada camara\nLook at S.chao1\n\nCodeDT::datatable(\nt(round(as.data.frame(estimateR(mat_vegan[,])),3))\n)\n\n\n\n\nCode\n# save as dataframe\nS_per_site &lt;- as.data.frame(t(round(as.data.frame(estimateR(mat_vegan[,])),3)))\n# add sites\nS_per_site$Station &lt;- rownames(S_per_site)\n\n\nIt is weird to have .5 species in some sites."
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#creemos-un-mapa-al-convertir-la-tabla-de-operación-de-la-cámara-trampa-a-un-objeto-sf",
    "href": "posts/2024-12-15-riqueza/index.html#creemos-un-mapa-al-convertir-la-tabla-de-operación-de-la-cámara-trampa-a-un-objeto-sf",
    "title": "Riqueza de especies",
    "section": "Creemos un mapa al convertir la tabla de operación de la cámara trampa a un objeto sf\n",
    "text": "Creemos un mapa al convertir la tabla de operación de la cámara trampa a un objeto sf\n\nEn este paso, convertimos la tabla de operación de la cámara trampa a un objeto sf. Luego agregamos la elevación con una busqueda rapida en los datos de la nube de Amazon (AWS), luego agregamos el tipo de hábitat y las especies por sitio (S.chao1), para finalmente visualizar el mapa, el cuál muestra la cantidad de especies como el tamaño del punto.\n\nCode\n# datos_distinct &lt;- datos |&gt; distinct(Longitude, Latitude, CT, Proyecto)\n\nprojlatlon &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\nCToperation_sf &lt;-  st_as_sf(x = CToperation,\n                         coords = c(\"Longitude\", \n                                    \"Latitude\"),\n                         crs = projlatlon)\n\n# write.csv(habs, \"C:/CodigoR/CameraTrapCesar/data/habitats.csv\")\nhabs &lt;- read.csv(\"C:/CodigoR/CameraTrapCesar/data/habitats.csv\")\n\nCToperation_elev_sf &lt;- get_elev_point(CToperation_sf, src = \"aws\") # get elevation from AWS\n\nCToperation_elev_sf &lt;- CToperation_elev_sf |&gt; left_join(habs, by='Station') |&gt; left_join(S_per_site, by='Station') |&gt; select(\"Station\", \"elevation\", \"minStart.x\",\"maxEnd.x\", \"Year.x\", \"hab_code\" , \"S.obs\", \"S.chao1\")\n\n# add habitat \n# CToperation_elev_sf$habs &lt;- habs$hab_code\n# see the map\nmapview(CToperation_elev_sf, zcol=\"hab_code\", cex = \"S.chao1\", alpha = 0)"
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#mapa-de-contornos",
    "href": "posts/2024-12-15-riqueza/index.html#mapa-de-contornos",
    "title": "Riqueza de especies",
    "section": "Mapa de contornos",
    "text": "Mapa de contornos\nUna ventaja de utilizar la estimación de densidad es que podemos sobreponerla a un mapa y usar Los suavizadores de kernel, los cuales son herramientas esenciales para el análisis de datos geograficos. Gracias a su capacidad para transmitir información estadística compleja con visualizaciones gráficas concisas, podemos vcisualizar un mapa de riqueza de especies con contornos. El suavizador de kernel más utilizado es el estimador de densidad de kernel (KDE), que se calcula con la funcion st_kde del paquete eks.\nEl contorno del 20 % significa que “el 20 % de las mediciones se encuentran dentro de este contorno”. La documentación de eks no está de acuerdo con la forma en que stat_density_2d del paquete ggplot2 realiza su cálculo. No sé quién tiene razón porque el valor estimado es la especie y los resultados son similares. En todo caso usemos eks.\n\nCode# select chao\nspecies &lt;- dplyr::select(CToperation_elev_sf, \"S.chao1\")\n# hakeoides_coord &lt;- data.frame(sf::st_coordinates(hakeoides))\nSta_den &lt;- eks::st_kde(species) # calculate density\n\n# VERY conveniently, eks can generate an sf file of contour lines\ncontours &lt;- eks::st_get_contour(Sta_den, cont=c( 10,20,30,40,50,60,70,80, 90)) %&gt;% \n  mutate(value=as.numeric(levels(contlabel)))\n\n\n# pal_fun &lt;- leaflet::colorQuantile(\"YlOrRd\", NULL, n = 5)\n\np_popup &lt;- paste(\"Species\", as.numeric(levels(contours$estimate)), \"number\")\n\n\ntmap::tmap_mode(\"view\") # set mode to interactive plots\n\ntmap::tm_shape(species) + \n    tmap::tm_sf(col=\"black\", size=0.2) +\n  #   contours from eks\n  tmap::tm_shape(contours) +\n    tmap::tm_polygons(\"estimate\",\n                      palette=\"Reds\",\n                      alpha=0.5 )\n\n\n\n\n\nCode\n\n## geom_sf plot\n# ## suitable smoothing matrix gives optimally smoothed contours\n# gs1 &lt;- ggplot(Sta_den) + geom_sf(data=CToperation_elev_sf, fill=NA) + ggthemes::theme_map() +\n#     colorspace::scale_fill_discrete_sequential(palette=\"Heat2\") \n# gs1 + geom_sf(data=st_get_contour(Sta_den), aes(fill=label_percent(contlabel))) +\n#     coord_sf(xlim=xlim, ylim=ylim) \n\n\n\nEn términos generales, la estimación de riqueza de especies por sitio muestra que al parecer es mayor cerca de la mina de carbon y la riqueza disminuye con la distancia a la mina. Que curioso… Observe también que las estimaciones de densidad de núcleo son mayores que las de s.chao1."
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#escalamiento-multidimensional-no-métrico-nmds",
    "href": "posts/2024-12-15-riqueza/index.html#escalamiento-multidimensional-no-métrico-nmds",
    "title": "Riqueza de especies",
    "section": "Escalamiento multidimensional no métrico (NMDS)",
    "text": "Escalamiento multidimensional no métrico (NMDS)\nEn la investigación ecológica, a menudo nos interesa no solo comparar descriptores univariados de comunidades, como la diversidad, sino también cómo las especies constituyentes (o la composición de especies) cambian de una comunidad a la siguiente. Una herramienta común para hacer esto es el escalamiento multidimensional no métrico, o NMDS. El objetivo del NMDS es agrupar la información de múltiples dimensiones (por ejemplo, de múltiples comunidades, sitios donde se instaló la cámara trampa, etc.) en solo unas pocas, de modo que se puedan visualizar e interpretar. A diferencia de otras técnicas de ordenación que se basan en distancias (principalmente euclidianas), como el análisis de coordenadas principales, el NMDS utiliza órdenes de rango y, por lo tanto, es una técnica extremadamente flexible que puede adaptarse a una variedad de diferentes tipos de datos.\nSi el tratamiento es continuo, como un gradiente ambiental, entonces puede ser útil trazar líneas de contorno en lugar de envolturas convexas. Podemos obtener algunos datos de elevación para nuestra matriz comunitaria original y superponerlos en el gráfico NMDS usando ordisurf.\n\nCode\nexample_NMDS=metaMDS(as.data.frame(mat_vegan), \n                     distance=\"euclidean\",\n                     zerodist = \"ignore\",\n                     trymax=300,\n                     k=5) # T\n#&gt; Wisconsin double standardization\n#&gt; Run 0 stress 0.1177774 \n#&gt; Run 1 stress 0.1195423 \n#&gt; Run 2 stress 0.1191379 \n#&gt; Run 3 stress 0.1198325 \n#&gt; Run 4 stress 0.1198226 \n#&gt; Run 5 stress 0.1180091 \n#&gt; ... Procrustes: rmse 0.06524029  max resid 0.3072442 \n#&gt; Run 6 stress 0.1210829 \n#&gt; Run 7 stress 0.118651 \n#&gt; Run 8 stress 0.1178291 \n#&gt; ... Procrustes: rmse 0.04330386  max resid 0.1586146 \n#&gt; Run 9 stress 0.118235 \n#&gt; ... Procrustes: rmse 0.06590823  max resid 0.3413579 \n#&gt; Run 10 stress 0.1198244 \n#&gt; Run 11 stress 0.1196112 \n#&gt; Run 12 stress 0.1194597 \n#&gt; Run 13 stress 0.1184506 \n#&gt; Run 14 stress 0.1187928 \n#&gt; Run 15 stress 0.1179514 \n#&gt; ... Procrustes: rmse 0.0702729  max resid 0.2078192 \n#&gt; Run 16 stress 0.1185014 \n#&gt; Run 17 stress 0.1194883 \n#&gt; Run 18 stress 0.1189131 \n#&gt; Run 19 stress 0.1184935 \n#&gt; Run 20 stress 0.119233 \n#&gt; Run 21 stress 0.1194748 \n#&gt; Run 22 stress 0.1202362 \n#&gt; Run 23 stress 0.1201048 \n#&gt; Run 24 stress 0.1194274 \n#&gt; Run 25 stress 0.1188908 \n#&gt; Run 26 stress 0.1189818 \n#&gt; Run 27 stress 0.1186987 \n#&gt; Run 28 stress 0.1181385 \n#&gt; ... Procrustes: rmse 0.06559254  max resid 0.2378787 \n#&gt; Run 29 stress 0.1219858 \n#&gt; Run 30 stress 0.1199 \n#&gt; Run 31 stress 0.1174144 \n#&gt; ... New best solution\n#&gt; ... Procrustes: rmse 0.04849005  max resid 0.3210962 \n#&gt; Run 32 stress 0.119731 \n#&gt; Run 33 stress 0.1181303 \n#&gt; Run 34 stress 0.119797 \n#&gt; Run 35 stress 0.1205275 \n#&gt; Run 36 stress 0.1198447 \n#&gt; Run 37 stress 0.1194153 \n#&gt; Run 38 stress 0.1185148 \n#&gt; Run 39 stress 0.1193037 \n#&gt; Run 40 stress 0.119097 \n#&gt; Run 41 stress 0.118962 \n#&gt; Run 42 stress 0.1191344 \n#&gt; Run 43 stress 0.1201702 \n#&gt; Run 44 stress 0.1198199 \n#&gt; Run 45 stress 0.1206558 \n#&gt; Run 46 stress 0.1200506 \n#&gt; Run 47 stress 0.1185052 \n#&gt; Run 48 stress 0.1180388 \n#&gt; Run 49 stress 0.1203547 \n#&gt; Run 50 stress 0.1179357 \n#&gt; Run 51 stress 0.1179688 \n#&gt; Run 52 stress 0.1179977 \n#&gt; Run 53 stress 0.1208276 \n#&gt; Run 54 stress 0.1193111 \n#&gt; Run 55 stress 0.1185373 \n#&gt; Run 56 stress 0.1189751 \n#&gt; Run 57 stress 0.1195666 \n#&gt; Run 58 stress 0.1195512 \n#&gt; Run 59 stress 0.1206369 \n#&gt; Run 60 stress 0.1183948 \n#&gt; Run 61 stress 0.1187466 \n#&gt; Run 62 stress 0.1202465 \n#&gt; Run 63 stress 0.1183039 \n#&gt; Run 64 stress 0.1203623 \n#&gt; Run 65 stress 0.1223139 \n#&gt; Run 66 stress 0.1202813 \n#&gt; Run 67 stress 0.1191234 \n#&gt; Run 68 stress 0.1192112 \n#&gt; Run 69 stress 0.1232189 \n#&gt; Run 70 stress 0.12036 \n#&gt; Run 71 stress 0.1204231 \n#&gt; Run 72 stress 0.1196363 \n#&gt; Run 73 stress 0.1206383 \n#&gt; Run 74 stress 0.119228 \n#&gt; Run 75 stress 0.1197685 \n#&gt; Run 76 stress 0.1196706 \n#&gt; Run 77 stress 0.118941 \n#&gt; Run 78 stress 0.1196978 \n#&gt; Run 79 stress 0.119377 \n#&gt; Run 80 stress 0.1208629 \n#&gt; Run 81 stress 0.1196318 \n#&gt; Run 82 stress 0.1184519 \n#&gt; Run 83 stress 0.119728 \n#&gt; Run 84 stress 0.1188257 \n#&gt; Run 85 stress 0.1200341 \n#&gt; Run 86 stress 0.1186837 \n#&gt; Run 87 stress 0.1200737 \n#&gt; Run 88 stress 0.1187235 \n#&gt; Run 89 stress 0.1207668 \n#&gt; Run 90 stress 0.1206542 \n#&gt; Run 91 stress 0.1181751 \n#&gt; Run 92 stress 0.1192226 \n#&gt; Run 93 stress 0.1202938 \n#&gt; Run 94 stress 0.1216142 \n#&gt; Run 95 stress 0.1186663 \n#&gt; Run 96 stress 0.1189068 \n#&gt; Run 97 stress 0.1180709 \n#&gt; Run 98 stress 0.1191901 \n#&gt; Run 99 stress 0.1183512 \n#&gt; Run 100 stress 0.1221186 \n#&gt; Run 101 stress 0.1195133 \n#&gt; Run 102 stress 0.1183453 \n#&gt; Run 103 stress 0.1191753 \n#&gt; Run 104 stress 0.118383 \n#&gt; Run 105 stress 0.1190571 \n#&gt; Run 106 stress 0.1184705 \n#&gt; Run 107 stress 0.1185067 \n#&gt; Run 108 stress 0.1192077 \n#&gt; Run 109 stress 0.1201531 \n#&gt; Run 110 stress 0.117499 \n#&gt; ... Procrustes: rmse 0.06527603  max resid 0.2835641 \n#&gt; Run 111 stress 0.1184804 \n#&gt; Run 112 stress 0.1189512 \n#&gt; Run 113 stress 0.1205725 \n#&gt; Run 114 stress 0.1184619 \n#&gt; Run 115 stress 0.1187501 \n#&gt; Run 116 stress 0.1180191 \n#&gt; Run 117 stress 0.1203417 \n#&gt; Run 118 stress 0.1180138 \n#&gt; Run 119 stress 0.1184571 \n#&gt; Run 120 stress 0.1190589 \n#&gt; Run 121 stress 0.1179745 \n#&gt; Run 122 stress 0.1187765 \n#&gt; Run 123 stress 0.1186773 \n#&gt; Run 124 stress 0.1202027 \n#&gt; Run 125 stress 0.1177355 \n#&gt; ... Procrustes: rmse 0.06712414  max resid 0.2518167 \n#&gt; Run 126 stress 0.1188758 \n#&gt; Run 127 stress 0.1188391 \n#&gt; Run 128 stress 0.1198934 \n#&gt; Run 129 stress 0.119144 \n#&gt; Run 130 stress 0.1181279 \n#&gt; Run 131 stress 0.1187219 \n#&gt; Run 132 stress 0.1172328 \n#&gt; ... New best solution\n#&gt; ... Procrustes: rmse 0.07064113  max resid 0.2745356 \n#&gt; Run 133 stress 0.1200621 \n#&gt; Run 134 stress 0.1197812 \n#&gt; Run 135 stress 0.1191899 \n#&gt; Run 136 stress 0.1193668 \n#&gt; Run 137 stress 0.1203229 \n#&gt; Run 138 stress 0.1183415 \n#&gt; Run 139 stress 0.1186813 \n#&gt; Run 140 stress 0.118479 \n#&gt; Run 141 stress 0.1181765 \n#&gt; Run 142 stress 0.1204436 \n#&gt; Run 143 stress 0.1176313 \n#&gt; ... Procrustes: rmse 0.05842401  max resid 0.1732576 \n#&gt; Run 144 stress 0.1200614 \n#&gt; Run 145 stress 0.1173113 \n#&gt; ... Procrustes: rmse 0.07172602  max resid 0.3251423 \n#&gt; Run 146 stress 0.1193847 \n#&gt; Run 147 stress 0.1192655 \n#&gt; Run 148 stress 0.118462 \n#&gt; Run 149 stress 0.1175735 \n#&gt; ... Procrustes: rmse 0.07032045  max resid 0.2382015 \n#&gt; Run 150 stress 0.1191411 \n#&gt; Run 151 stress 0.1193411 \n#&gt; Run 152 stress 0.1184718 \n#&gt; Run 153 stress 0.1188012 \n#&gt; Run 154 stress 0.1182784 \n#&gt; Run 155 stress 0.1206972 \n#&gt; Run 156 stress 0.1186205 \n#&gt; Run 157 stress 0.1178141 \n#&gt; Run 158 stress 0.1193271 \n#&gt; Run 159 stress 0.1200008 \n#&gt; Run 160 stress 0.118016 \n#&gt; Run 161 stress 0.1206675 \n#&gt; Run 162 stress 0.1187126 \n#&gt; Run 163 stress 0.1209422 \n#&gt; Run 164 stress 0.1179177 \n#&gt; Run 165 stress 0.1196855 \n#&gt; Run 166 stress 0.1216033 \n#&gt; Run 167 stress 0.1194743 \n#&gt; Run 168 stress 0.119057 \n#&gt; Run 169 stress 0.1211887 \n#&gt; Run 170 stress 0.1200397 \n#&gt; Run 171 stress 0.1191723 \n#&gt; Run 172 stress 0.1191287 \n#&gt; Run 173 stress 0.1186945 \n#&gt; Run 174 stress 0.1197061 \n#&gt; Run 175 stress 0.1191448 \n#&gt; Run 176 stress 0.1203891 \n#&gt; Run 177 stress 0.1182641 \n#&gt; Run 178 stress 0.1179199 \n#&gt; Run 179 stress 0.1189815 \n#&gt; Run 180 stress 0.1197362 \n#&gt; Run 181 stress 0.119759 \n#&gt; Run 182 stress 0.1198242 \n#&gt; Run 183 stress 0.1189831 \n#&gt; Run 184 stress 0.1193678 \n#&gt; Run 185 stress 0.1192132 \n#&gt; Run 186 stress 0.12124 \n#&gt; Run 187 stress 0.1194051 \n#&gt; Run 188 stress 0.1192667 \n#&gt; Run 189 stress 0.1180187 \n#&gt; Run 190 stress 0.1206884 \n#&gt; Run 191 stress 0.119382 \n#&gt; Run 192 stress 0.1188378 \n#&gt; Run 193 stress 0.1195598 \n#&gt; Run 194 stress 0.1196459 \n#&gt; Run 195 stress 0.1209624 \n#&gt; Run 196 stress 0.1182844 \n#&gt; Run 197 stress 0.1201092 \n#&gt; Run 198 stress 0.1184119 \n#&gt; Run 199 stress 0.1196753 \n#&gt; Run 200 stress 0.1180458 \n#&gt; Run 201 stress 0.1180554 \n#&gt; Run 202 stress 0.1201035 \n#&gt; Run 203 stress 0.119592 \n#&gt; Run 204 stress 0.1197981 \n#&gt; Run 205 stress 0.1198532 \n#&gt; Run 206 stress 0.1189415 \n#&gt; Run 207 stress 0.1195722 \n#&gt; Run 208 stress 0.1193887 \n#&gt; Run 209 stress 0.1203173 \n#&gt; Run 210 stress 0.1195147 \n#&gt; Run 211 stress 0.1200148 \n#&gt; Run 212 stress 0.1184625 \n#&gt; Run 213 stress 0.1180119 \n#&gt; Run 214 stress 0.1185966 \n#&gt; Run 215 stress 0.1175312 \n#&gt; ... Procrustes: rmse 0.06454087  max resid 0.1743286 \n#&gt; Run 216 stress 0.1198926 \n#&gt; Run 217 stress 0.118124 \n#&gt; Run 218 stress 0.119083 \n#&gt; Run 219 stress 0.1205559 \n#&gt; Run 220 stress 0.1193963 \n#&gt; Run 221 stress 0.1180641 \n#&gt; Run 222 stress 0.1201715 \n#&gt; Run 223 stress 0.1199311 \n#&gt; Run 224 stress 0.119174 \n#&gt; Run 225 stress 0.1197605 \n#&gt; Run 226 stress 0.1213194 \n#&gt; Run 227 stress 0.1199168 \n#&gt; Run 228 stress 0.1192093 \n#&gt; Run 229 stress 0.1196373 \n#&gt; Run 230 stress 0.1184441 \n#&gt; Run 231 stress 0.1205332 \n#&gt; Run 232 stress 0.1203975 \n#&gt; Run 233 stress 0.1175311 \n#&gt; ... Procrustes: rmse 0.04016621  max resid 0.1523985 \n#&gt; Run 234 stress 0.1205168 \n#&gt; Run 235 stress 0.1202749 \n#&gt; Run 236 stress 0.1190829 \n#&gt; Run 237 stress 0.1188097 \n#&gt; Run 238 stress 0.118695 \n#&gt; Run 239 stress 0.1208031 \n#&gt; Run 240 stress 0.1183503 \n#&gt; Run 241 stress 0.1196354 \n#&gt; Run 242 stress 0.1183296 \n#&gt; Run 243 stress 0.1197259 \n#&gt; Run 244 stress 0.1196722 \n#&gt; Run 245 stress 0.1194817 \n#&gt; Run 246 stress 0.1183007 \n#&gt; Run 247 stress 0.1200192 \n#&gt; Run 248 stress 0.1203804 \n#&gt; Run 249 stress 0.1190264 \n#&gt; Run 250 stress 0.1205513 \n#&gt; Run 251 stress 0.1193036 \n#&gt; Run 252 stress 0.1201817 \n#&gt; Run 253 stress 0.1194547 \n#&gt; Run 254 stress 0.1182097 \n#&gt; Run 255 stress 0.1182382 \n#&gt; Run 256 stress 0.1190007 \n#&gt; Run 257 stress 0.1190836 \n#&gt; Run 258 stress 0.1177959 \n#&gt; Run 259 stress 0.1184679 \n#&gt; Run 260 stress 0.1185746 \n#&gt; Run 261 stress 0.1200334 \n#&gt; Run 262 stress 0.1194108 \n#&gt; Run 263 stress 0.1176355 \n#&gt; ... Procrustes: rmse 0.06803017  max resid 0.2928558 \n#&gt; Run 264 stress 0.1196781 \n#&gt; Run 265 stress 0.1227222 \n#&gt; Run 266 stress 0.1190069 \n#&gt; Run 267 stress 0.119661 \n#&gt; Run 268 stress 0.1181562 \n#&gt; Run 269 stress 0.1183829 \n#&gt; Run 270 stress 0.1204617 \n#&gt; Run 271 stress 0.1187249 \n#&gt; Run 272 stress 0.1188378 \n#&gt; Run 273 stress 0.1180025 \n#&gt; Run 274 stress 0.1186805 \n#&gt; Run 275 stress 0.11921 \n#&gt; Run 276 stress 0.1209608 \n#&gt; Run 277 stress 0.1209839 \n#&gt; Run 278 stress 0.1211942 \n#&gt; Run 279 stress 0.1198645 \n#&gt; Run 280 stress 0.1183013 \n#&gt; Run 281 stress 0.1180577 \n#&gt; Run 282 stress 0.1181384 \n#&gt; Run 283 stress 0.12016 \n#&gt; Run 284 stress 0.1213227 \n#&gt; Run 285 stress 0.1183602 \n#&gt; Run 286 stress 0.1197058 \n#&gt; Run 287 stress 0.1193404 \n#&gt; Run 288 stress 0.1194831 \n#&gt; Run 289 stress 0.1190676 \n#&gt; Run 290 stress 0.1207159 \n#&gt; Run 291 stress 0.120434 \n#&gt; Run 292 stress 0.1202456 \n#&gt; Run 293 stress 0.1206478 \n#&gt; Run 294 stress 0.1176725 \n#&gt; ... Procrustes: rmse 0.06579052  max resid 0.251531 \n#&gt; Run 295 stress 0.1203448 \n#&gt; Run 296 stress 0.1182932 \n#&gt; Run 297 stress 0.1195878 \n#&gt; Run 298 stress 0.1209131 \n#&gt; Run 299 stress 0.1181634 \n#&gt; Run 300 stress 0.1184255 \n#&gt; *** Best solution was not repeated -- monoMDS stopping criteria:\n#&gt;    300: no. of iterations &gt;= maxit\n\n# plot the graph\nvegan::ordisurf((example_NMDS),CToperation_elev_sf$elevation,main=\"\",col=\"forestgreen\", trymax=100) # bubble = 2\n#&gt; \n#&gt; Family: gaussian \n#&gt; Link function: identity \n#&gt; \n#&gt; Formula:\n#&gt; y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n#&gt; \n#&gt; Estimated degrees of freedom:\n#&gt; 5.22  total = 6.22 \n#&gt; \n#&gt; REML score: 726.9891\nvegan::orditorp(example_NMDS,display=\"species\",col=\"blue\",air=0.1,\n   cex=0.5)\n\n\n\n\n\n\n\nPodemos hacer una gráfica similar usando gg_ordisurf del paquete ggordiplots pero incorporando también el tipo de hábitat.\n\nCode# ggordiplots::gg_ordisurf()\n# To fit a surface with ggordiplots:\n\n \nordiplot &lt;- gg_ordisurf(ord = example_NMDS, \n                        env.var = CToperation_elev_sf$elevation,\n                        var.label = \"Elevation\",\n                        pt.size = 2,\n                        groups = CToperation_elev_sf$hab_code,\n                        binwidth = 50)\n\n\n\n\n\n\nCode\n# ggplotly(ordiplot$plot) # see interactive\n\n# # alternative using biodiversityR\n# \n# A1.surface &lt;- ordisurf( y=example_NMDS)\n# A1.grid &lt;- ordisurfgrid.long(A1.surface)\n# # Preparing the plot\n# \n# plotgg4 &lt;- ggplot() + \n#     geom_contour_filled(data=A1.grid, \n#                         aes(x=x, y=y, z=z)) +\n#     geom_vline(xintercept = c(0), color = \"grey70\", linetype = 2) +\n#     geom_hline(yintercept = c(0), color = \"grey70\", linetype = 2) +  \n#     xlab(axis.long2[1, \"label\"]) +\n#     ylab(axis.long2[2, \"label\"]) +  \n#     scale_x_continuous(sec.axis = dup_axis(labels=NULL, name=NULL)) +\n#     scale_y_continuous(sec.axis = dup_axis(labels=NULL, name=NULL)) +\n#     geom_point(data=sites.long2, \n#                aes(x=axis1, y=axis2, shape=Management), \n#                colour=\"red\", size=4) +\n#     BioR.theme +\n#     scale_fill_viridis_d() +\n#     labs(fill=\"A1\") +\n#     coord_fixed(ratio=1)\n# # and seeing the plot.\n# \n# plotgg4\n\n\nLos contornos conectan especies en el espacio de ordenación que se predice que tendrán la misma elevación. Note que es una representación en el espacio multivariado, no una representación geográfica."
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#rarefaction-usando-inext",
    "href": "posts/2024-12-15-riqueza/index.html#rarefaction-usando-inext",
    "title": "Riqueza de especies",
    "section": "Rarefaction usando iNEXT\n",
    "text": "Rarefaction usando iNEXT\n\n\nCode\n\n\nout &lt;- iNEXT(incidence_cesar, # The data frame\n             q=0,# The type of diversity estimator \n             datatype=\"incidence_freq\",   # The type of analysis\n             knots=40,                    # The number of data points \n             se=TRUE,                     # confidence intervals\n             conf=0.95,                   # The level of confidence intervals\n             nboot=100)                    # The number of bootstraps \n\nggiNEXT(out, type=1)\n\n\n\n\n\n\nCodeggiNEXT(out, type=2)\n\n\n\n\n\n\nCodeggiNEXT(out, type=3)\n\n\n\n\n\n\nCode\np1 &lt;- ggiNEXT(out, type=1)+ theme_classic() +   #  type 1 = the diversity estimator\n        labs(x = \"Survey sites\", y = \"Richness\")\n  \np2 &lt;- ggiNEXT(out, type=2)+ theme_classic() +    #  type 2 = the survey coverage\n        labs(x = \"Survey sites\")\n    \ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\nCode##############\nout2 &lt;- iNEXT(incidence_cesar, q=c(0,1,2) ,datatype=\"incidence_freq\" )\n\nggiNEXT(out2, type=1, facet.var=\"Order.q\", color.var=\"Assemblage\") + theme_classic() \n\n\n\n\n\n\n\nEl paquete iNEXT es adecuado para comparaciones de índices de diversidad mediante el uso de números de Hill, de los cuales el valor q representa la riqueza y los índices de diversidad tradicionales:\n\nLa riqueza de especies es q = 0.\nEl índice de Shannon es q=1\nEl índice de Simpson es q=2.\n\n\nNota: el aumento de los valores de q reduce la influencia de las especies raras en nuestra estimación de la diversidad de la comunidad.\n\nFacil no?… tenga en cuenta que en los datos de fototrampeo, tal vez debamos separar en los datos aves de los de mamíferos."
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#package-citation",
    "href": "posts/2024-12-15-riqueza/index.html#package-citation",
    "title": "Riqueza de especies",
    "section": "Package Citation",
    "text": "Package Citation\n\nCodepkgs &lt;- cite_packages(output = \"paragraph\", out.dir = \".\") #knitr::kable(pkgs)\npkgs\n\nWe used R version 4.4.2 (R Core Team 2024) and the following R packages: camtrapR v. 2.3.0 (Niedballa et al. 2016), DT v. 0.33 (Xie, Cheng, and Tan 2024), eks v. 1.0.5 (Duong 2024), elevatr v. 0.99.0 (Hollister et al. 2023), ggforce v. 0.4.2 (Pedersen 2024a), ggordiplots v. 0.4.3 (Quensen, Simpson, and Oksanen 2024), ggvegan v. 0.1.999 (Simpson and Oksanen 2023), gridExtra v. 2.3 (Auguie 2017), iNEXT v. 3.0.1 (Chao et al. 2014; Hsieh, Ma, and Chao 2024), kableExtra v. 1.4.0 (Zhu 2024), knitr v. 1.49 (Xie 2014, 2015, 2024), mapview v. 2.11.2 (Appelhans et al. 2023), MeanRarity v. 0.0.1.5 (Roswell and Dushoff 2023), patchwork v. 1.3.0 (Pedersen 2024b), plotly v. 4.10.4 (Sievert 2020), plyr v. 1.8.9 (Wickham 2011), reshape2 v. 1.4.4 (Wickham 2007), rmarkdown v. 2.29 (Xie, Allaire, and Grolemund 2018; Xie, Dervieux, and Riederer 2020; Allaire et al. 2024), scales v. 1.3.0 (Wickham, Pedersen, and Seidel 2023), sf v. 1.0.19 (Pebesma 2018; Pebesma and Bivand 2023), SpadeR v. 0.1.1 (Chao et al. 2016), tidyverse v. 2.0.0 (Wickham et al. 2019), tmap v. 4.0 (Tennekes 2018), vegan v. 2.6.8 (Oksanen et al. 2024)."
  },
  {
    "objectID": "posts/2024-12-15-riqueza/index.html#sesion-info",
    "href": "posts/2024-12-15-riqueza/index.html#sesion-info",
    "title": "Riqueza de especies",
    "section": "Sesion info",
    "text": "Sesion info\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nCodeprint(sessionInfo(), locale = FALSE)\n#&gt; R version 4.4.2 (2024-10-31 ucrt)\n#&gt; Platform: x86_64-w64-mingw32/x64\n#&gt; Running under: Windows 10 x64 (build 19045)\n#&gt; \n#&gt; Matrix products: internal\n#&gt; \n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] scales_1.3.0          plyr_1.8.9            reshape2_1.4.4       \n#&gt;  [4] plotly_4.10.4         ggforce_0.4.2         lubridate_1.9.4      \n#&gt;  [7] forcats_1.0.0         stringr_1.5.1         dplyr_1.1.4          \n#&gt; [10] purrr_1.0.2           readr_2.1.5           tidyr_1.3.1          \n#&gt; [13] tibble_3.2.1          tidyverse_2.0.0       kableExtra_1.4.0     \n#&gt; [16] knitr_1.49            iNEXT_3.0.1           SpadeR_0.1.1         \n#&gt; [19] MeanRarity_0.0.1.0005 DT_0.33               gridExtra_2.3        \n#&gt; [22] ggordiplots_0.4.3     glue_1.8.0            ggvegan_0.1.999      \n#&gt; [25] ggplot2_3.5.2         vegan_2.6-8           lattice_0.22-6       \n#&gt; [28] permute_0.9-7         camtrapR_2.3.0        grateful_0.2.10      \n#&gt; [31] eks_1.0.5             tmap_4.0              mapview_2.11.2       \n#&gt; [34] elevatr_0.99.0        sf_1.0-19             readxl_1.4.3         \n#&gt; [37] patchwork_1.3.0      \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] RColorBrewer_1.1-3      rstudioapi_0.17.1       jsonlite_1.8.9         \n#&gt;   [4] wk_0.9.4                magrittr_2.0.3          farver_2.1.2           \n#&gt;   [7] rmarkdown_2.29          vctrs_0.6.5             base64enc_0.1-3        \n#&gt;  [10] terra_1.8-21            RcppNumerical_0.6-0     progress_1.2.3         \n#&gt;  [13] htmltools_0.5.8.1       leafsync_0.1.0          curl_6.0.0             \n#&gt;  [16] raster_3.6-30           cellranger_1.1.0        s2_1.1.7               \n#&gt;  [19] sass_0.4.9              pracma_2.4.4            slippymath_0.3.1       \n#&gt;  [22] bslib_0.8.0             KernSmooth_2.23-24      htmlwidgets_1.6.4      \n#&gt;  [25] cachem_1.1.0            stars_0.6-8             uuid_1.2-1             \n#&gt;  [28] lifecycle_1.0.4         pkgconfig_2.0.3         cols4all_0.8           \n#&gt;  [31] Matrix_1.7-1            R6_2.6.1                fastmap_1.2.0          \n#&gt;  [34] rbibutils_2.3           digest_0.6.37           colorspace_2.1-1       \n#&gt;  [37] leafem_0.2.3            crosstalk_1.2.1         maplegend_0.2.0        \n#&gt;  [40] labeling_0.4.3          lwgeom_0.2-14           progressr_0.15.0       \n#&gt;  [43] spacesXYZ_1.3-0         timechange_0.3.0        httr_1.4.7             \n#&gt;  [46] polyclip_1.10-7         abind_1.4-8             mgcv_1.9-1             \n#&gt;  [49] compiler_4.4.2          microbenchmark_1.5.0    proxy_0.4-27           \n#&gt;  [52] withr_3.0.2             brew_1.0-10             DBI_1.2.3              \n#&gt;  [55] MASS_7.3-61             tmaptools_3.2           leaflet_2.2.2          \n#&gt;  [58] classInt_0.4-10         tools_4.4.2             units_0.8-5            \n#&gt;  [61] leaflegend_1.2.1        satellite_1.0.5         nlme_3.1-166           \n#&gt;  [64] cluster_2.1.6           generics_0.1.3          isoband_0.2.7          \n#&gt;  [67] gtable_0.3.6            leaflet.providers_2.0.0 tzdb_0.4.0             \n#&gt;  [70] class_7.3-22            data.table_1.16.4       hms_1.1.3              \n#&gt;  [73] sp_2.1-4                xml2_1.3.6              ggrepel_0.9.6          \n#&gt;  [76] pillar_1.10.1           splines_4.4.2           tweenr_2.0.3           \n#&gt;  [79] mapsf_0.12.0            renv_1.0.11             ks_1.14.3              \n#&gt;  [82] tidyselect_1.2.1        svglite_2.1.3           stats4_4.4.2           \n#&gt;  [85] xfun_0.49               leafpop_0.1.0           stringi_1.8.4          \n#&gt;  [88] lazyeval_0.2.2          yaml_2.3.10             evaluate_1.0.1         \n#&gt;  [91] codetools_0.2-20        cli_3.6.3               RcppParallel_5.1.9     \n#&gt;  [94] systemfonts_1.1.0       Rdpack_2.6.2            jquerylib_0.1.4        \n#&gt;  [97] munsell_0.5.1           secr_5.1.0              dichromat_2.0-0.1      \n#&gt; [100] Rcpp_1.0.13-1           png_0.1-8               XML_3.99-0.17          \n#&gt; [103] parallel_4.4.2          prettyunits_1.2.0       mclust_6.1.1           \n#&gt; [106] viridisLite_0.4.2       mvtnorm_1.3-2           e1071_1.7-16           \n#&gt; [109] crayon_1.5.3            rlang_1.1.4"
  },
  {
    "objectID": "biodiversity.html",
    "href": "biodiversity.html",
    "title": "Biodiversity",
    "section": "",
    "text": "In the vast, wild places of our planet, a quiet revolution is happening. From the dense rainforests of the Amazon to the remote mountain ranges of Asia, biologist are deploying a simple and powerful tool that is changing the way we see and protect wildlife: the camera trap.\nThese are not average cameras. They are activated by motion and heat, and are designed to be left in the field for weeks or even months, capturing candid, undisturbed moments of animal life. Without a human in sight, these silent witnesses provide a priceless window into the secret world of biodiversity.\nHere’s why camera traps have become such a critical tool for conservation:\n\n\nMany of the world’s most vulnerable species are also the most elusive. Nocturnal hunters, shy forest dwellers, and rare, solitary animals often go undetected by traditional survey methods. Camera traps excel at capturing these “ghosts” of the animal kingdom, providing irrefutable evidence of their presence. This has led to remarkable discoveries, including the first-ever photographs of species thought to be extinct, like the silver-backed chevrotain.\n\n\n\nBeyond just finding a single animal, a network of camera traps can paint a comprehensive picture of an entire ecosystem. By analyzing the data—how many different species are present, their activity patterns, and their interactions—scientists can assess the health of a habitat. A high diversity of species, including top predators, is a strong indicator of a thriving environment.\n\n\n\nCamera trap data is a goldmine of information. It helps conservationists answer key questions such as:\n\nWhere are animals moving? This helps identify crucial wildlife corridors that need to be protected.\nWhat is a species’ population size? This is essential for monitoring endangered populations and evaluating the success of conservation efforts.\nHow are animals interacting with their environment and each other? This provides insight into predator-prey dynamics and competition for resources.\n\nThis information is not just interesting—it’s the foundation for effective, science and data-driven conservation strategies.\n\n\n\nFinally, camera traps serve as a powerful tool for public outreach. The captivating images and videos they capture a mother Jaguar with her cubs, a majestic tiger, a tapir are shared on social media, in documentaries, and in educational programs. They help people feel a personal connection to wildlife, inspiring a sense of wonder and urgency to protect these magnificent creatures.\nIn a world facing a global biodiversity crisis, camera traps offer hope. They provide us with the evidence, insights, and inspiration we need to make informed decisions and ensure that the planet’s incredible wildlife will thrive for generations to come."
  },
  {
    "objectID": "biodiversity.html#how-camera-traps-are-revolutionizing-biodiversity-conservation",
    "href": "biodiversity.html#how-camera-traps-are-revolutionizing-biodiversity-conservation",
    "title": "Biodiversity",
    "section": "",
    "text": "In the vast, wild places of our planet, a quiet revolution is happening. From the dense rainforests of the Amazon to the remote mountain ranges of Asia, biologist are deploying a simple and powerful tool that is changing the way we see and protect wildlife: the camera trap.\nThese are not average cameras. They are activated by motion and heat, and are designed to be left in the field for weeks or even months, capturing candid, undisturbed moments of animal life. Without a human in sight, these silent witnesses provide a priceless window into the secret world of biodiversity.\nHere’s why camera traps have become such a critical tool for conservation:\n\n\nMany of the world’s most vulnerable species are also the most elusive. Nocturnal hunters, shy forest dwellers, and rare, solitary animals often go undetected by traditional survey methods. Camera traps excel at capturing these “ghosts” of the animal kingdom, providing irrefutable evidence of their presence. This has led to remarkable discoveries, including the first-ever photographs of species thought to be extinct, like the silver-backed chevrotain.\n\n\n\nBeyond just finding a single animal, a network of camera traps can paint a comprehensive picture of an entire ecosystem. By analyzing the data—how many different species are present, their activity patterns, and their interactions—scientists can assess the health of a habitat. A high diversity of species, including top predators, is a strong indicator of a thriving environment.\n\n\n\nCamera trap data is a goldmine of information. It helps conservationists answer key questions such as:\n\nWhere are animals moving? This helps identify crucial wildlife corridors that need to be protected.\nWhat is a species’ population size? This is essential for monitoring endangered populations and evaluating the success of conservation efforts.\nHow are animals interacting with their environment and each other? This provides insight into predator-prey dynamics and competition for resources.\n\nThis information is not just interesting—it’s the foundation for effective, science and data-driven conservation strategies.\n\n\n\nFinally, camera traps serve as a powerful tool for public outreach. The captivating images and videos they capture a mother Jaguar with her cubs, a majestic tiger, a tapir are shared on social media, in documentaries, and in educational programs. They help people feel a personal connection to wildlife, inspiring a sense of wonder and urgency to protect these magnificent creatures.\nIn a world facing a global biodiversity crisis, camera traps offer hope. They provide us with the evidence, insights, and inspiration we need to make informed decisions and ensure that the planet’s incredible wildlife will thrive for generations to come."
  },
  {
    "objectID": "dataexploration.html",
    "href": "dataexploration.html",
    "title": "Data exploration",
    "section": "",
    "text": "The natural world is complex full of intricate interactions. Understanding this complexity is vital for conservation efforts. Technology is providing us with powerful tools to gather data. Among the most popular and effective are camera traps – silent sentinels capturing candid moments of wildlife, day and night.\nBut collecting data is only the first step. Imagine a vast library filled with millions of books, uncatalogued and unread. That’s what raw camera trap data can be without proper exploration. Data exploration is the process of sifting through, visualizing, and understanding your data before diving into complex analyses. And when it comes to biodiversity and camera trap projects, it’s not just important – it’s absolutely critical.\nHere’s why:\n\n\nThink of data exploration as shining a spotlight into the dark corners of your dataset. Before you even formulate a hypothesis, exploring your data can reveal surprising patterns, trends, and anomalies you might otherwise miss.\n\nSpecies Activity Patterns: When are certain animals most active? Are they nocturnal, diurnal, or crepuscular? Exploring temporal data can show peaks and troughs in activity, informing our understanding of their ecology and potential human-wildlife conflict.\nSpatial Distribution: Where are different species congregating? Are there areas they avoid? Visualizing spatial data can highlight critical habitats, movement corridors, or barriers.\nInter-species Relationships: Do certain species appear together or avoid each other? Preliminary exploration can hint at predator-prey dynamics or competitive exclusion.\n\n\n\n\nNo dataset is perfect. Misidentifications, incorrect timestamps, camera malfunctions or excel dates formats can all introduce errors. Data exploration acts as your first line of defense against these inaccuracies.\n\nSpotting Outliers: An unusually high number of detections for a particular species in one location, or a sudden drop-off, could indicate a data entry error or a camera malfunction.\nChecking Data Integrity: Visualizing distributions of variables can quickly highlight impossible values (e.g., a camera trap recording an animal at a negative temperature) or inconsistencies. Catching these early saves immense time and ensures the validity of your subsequent analyses.\n\n\n\n\nEffective research begins with well-formed questions. Data exploration helps you refine existing hypotheses and generate new, more targeted ones.\n\nIf your exploration shows a strong correlation between a specific habitat type and a rare species, you might formulate a hypothesis about habitat preference.\nDiscovering an unexpected species in an area could lead to new questions about range expansion or previously unrecorded populations.\n\n\n\n\nUnderstanding the nuances of your current data can significantly improve future data collection efforts.\n\nOptimizing Camera Placement: If exploration reveals certain areas consistently yield more valuable data, you can adjust your camera trap placement in subsequent deployments.\nRefining Survey Timings: If your data shows that a target species is primarily active during a specific short window, you might focus your efforts during those times.\nIdentifying Data Gaps: Exploration can highlight areas or time periods where you have insufficient data, prompting you to adjust your sampling strategy.\n\n\n\n\nVisualizations born from data exploration and are a powerful tool for communicating your findings to a wider audience – stakeholders, policymakers, or the general public.\n\nEngaging Infographics: Simple bar charts showing species richness or heat maps illustrating animal density can be far more impactful than raw numbers.\nHighlighting Key Insights: Visualizing your data allows you to tell a compelling story about the wildlife you’re studying and the conservation challenges they face."
  },
  {
    "objectID": "dataexploration.html#why-data-exploration-is-crucial-for-biodiversity-and-camera-trap-projects",
    "href": "dataexploration.html#why-data-exploration-is-crucial-for-biodiversity-and-camera-trap-projects",
    "title": "Data exploration",
    "section": "",
    "text": "The natural world is complex full of intricate interactions. Understanding this complexity is vital for conservation efforts. Technology is providing us with powerful tools to gather data. Among the most popular and effective are camera traps – silent sentinels capturing candid moments of wildlife, day and night.\nBut collecting data is only the first step. Imagine a vast library filled with millions of books, uncatalogued and unread. That’s what raw camera trap data can be without proper exploration. Data exploration is the process of sifting through, visualizing, and understanding your data before diving into complex analyses. And when it comes to biodiversity and camera trap projects, it’s not just important – it’s absolutely critical.\nHere’s why:\n\n\nThink of data exploration as shining a spotlight into the dark corners of your dataset. Before you even formulate a hypothesis, exploring your data can reveal surprising patterns, trends, and anomalies you might otherwise miss.\n\nSpecies Activity Patterns: When are certain animals most active? Are they nocturnal, diurnal, or crepuscular? Exploring temporal data can show peaks and troughs in activity, informing our understanding of their ecology and potential human-wildlife conflict.\nSpatial Distribution: Where are different species congregating? Are there areas they avoid? Visualizing spatial data can highlight critical habitats, movement corridors, or barriers.\nInter-species Relationships: Do certain species appear together or avoid each other? Preliminary exploration can hint at predator-prey dynamics or competitive exclusion.\n\n\n\n\nNo dataset is perfect. Misidentifications, incorrect timestamps, camera malfunctions or excel dates formats can all introduce errors. Data exploration acts as your first line of defense against these inaccuracies.\n\nSpotting Outliers: An unusually high number of detections for a particular species in one location, or a sudden drop-off, could indicate a data entry error or a camera malfunction.\nChecking Data Integrity: Visualizing distributions of variables can quickly highlight impossible values (e.g., a camera trap recording an animal at a negative temperature) or inconsistencies. Catching these early saves immense time and ensures the validity of your subsequent analyses.\n\n\n\n\nEffective research begins with well-formed questions. Data exploration helps you refine existing hypotheses and generate new, more targeted ones.\n\nIf your exploration shows a strong correlation between a specific habitat type and a rare species, you might formulate a hypothesis about habitat preference.\nDiscovering an unexpected species in an area could lead to new questions about range expansion or previously unrecorded populations.\n\n\n\n\nUnderstanding the nuances of your current data can significantly improve future data collection efforts.\n\nOptimizing Camera Placement: If exploration reveals certain areas consistently yield more valuable data, you can adjust your camera trap placement in subsequent deployments.\nRefining Survey Timings: If your data shows that a target species is primarily active during a specific short window, you might focus your efforts during those times.\nIdentifying Data Gaps: Exploration can highlight areas or time periods where you have insufficient data, prompting you to adjust your sampling strategy.\n\n\n\n\nVisualizations born from data exploration and are a powerful tool for communicating your findings to a wider audience – stakeholders, policymakers, or the general public.\n\nEngaging Infographics: Simple bar charts showing species richness or heat maps illustrating animal density can be far more impactful than raw numbers.\nHighlighting Key Insights: Visualizing your data allows you to tell a compelling story about the wildlife you’re studying and the conservation challenges they face."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html",
    "href": "posts/2024-06-25-species-diversity/index.html",
    "title": "Species diversity",
    "section": "",
    "text": "There are two commonly used ways to account for survey effort when estimating species richness using camera traps:\n\nusing the rarefaction of observed richness.\nusing multispecies occupancy models to account for the species present but not observed (occupancy model, taking in to account imperfect detection).\n\nIn this post we can see an example of No 1. using the classical approach of community ecology using the vegan package. The vegan package (https://cran.r-project.org/package=vegan) provides tools for descriptive community ecology. It has basic functions of diversity analysis, community ordination and dissimilarity analysis. The vegan package provides most standard tools of descriptive community analysis. Later in the post we carry out another diversity analysis using functions of the package iNEXT.\nThe modern approach to measure species diversity include the “Sample Hill diversities” also known as Hill numbers. Rarefaction and extrapolation with Hill numbers have gain popularity in the last decade and can be computed using the function renyi in the R package vegan (Oksanen 2016) and the function rarity in the R package MeanRarity (Roswell and Dushoff 2020), and Hill diversities of equal-sized or equal-coverage samples can be approximately compared using the functions iNEXT and estimateD in the R package iNEXT (Hsieh et al. 2016). Estimates for asymptotic values of Hill diversity are available in SpadeR (Chao and Jost 2015, Chao et al. 2015)."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#species-richness-and-sampling-effort",
    "href": "posts/2024-06-25-species-diversity/index.html#species-richness-and-sampling-effort",
    "title": "Species diversity",
    "section": "",
    "text": "There are two commonly used ways to account for survey effort when estimating species richness using camera traps:\n\nusing the rarefaction of observed richness.\nusing multispecies occupancy models to account for the species present but not observed (occupancy model, taking in to account imperfect detection).\n\nIn this post we can see an example of No 1. using the classical approach of community ecology using the vegan package. The vegan package (https://cran.r-project.org/package=vegan) provides tools for descriptive community ecology. It has basic functions of diversity analysis, community ordination and dissimilarity analysis. The vegan package provides most standard tools of descriptive community analysis. Later in the post we carry out another diversity analysis using functions of the package iNEXT.\nThe modern approach to measure species diversity include the “Sample Hill diversities” also known as Hill numbers. Rarefaction and extrapolation with Hill numbers have gain popularity in the last decade and can be computed using the function renyi in the R package vegan (Oksanen 2016) and the function rarity in the R package MeanRarity (Roswell and Dushoff 2020), and Hill diversities of equal-sized or equal-coverage samples can be approximately compared using the functions iNEXT and estimateD in the R package iNEXT (Hsieh et al. 2016). Estimates for asymptotic values of Hill diversity are available in SpadeR (Chao and Jost 2015, Chao et al. 2015)."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#load-packages",
    "href": "posts/2024-06-25-species-diversity/index.html#load-packages",
    "title": "Species diversity",
    "section": "Load packages",
    "text": "Load packages\n\nCode\n\nlibrary(patchwork) # The Composer of Plots\nlibrary(readxl) # Read Excel Files\nlibrary(sf) # Simple Features for R\nlibrary(elevatr) # Access Elevation Data from Various APIs\nlibrary(mapview) # Interactive Viewing of Spatial Data in R\nlibrary(tmap)\nlibrary(eks) # make countours\nlibrary(grateful) # Facilitate Citation of R Packages\nlibrary(camtrapR) # Camera Trap Data Management and Preparation of Occupancy and Spatial Capture-Recapture Analyses\nlibrary(vegan) # Community Ecology Package \nlibrary(ggvegan)\n# library(BiodiversityR) # cause error!\nlibrary(ggordiplots)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(DT)\nlibrary(MeanRarity)\nlibrary(SpadeR)\nlibrary(iNEXT) # Interpolation and Extrapolation for Species Diversity\nlibrary(knitr) # A General-Purpose Package for Dynamic Report Generation in R\nlibrary(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax\nlibrary(tidyverse) # Easily Install and Load the 'Tidyverse'\nlibrary(ggforce) # Accelerating 'ggplot2'\nlibrary(plotly)"
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#load-data",
    "href": "posts/2024-06-25-species-diversity/index.html#load-data",
    "title": "Species diversity",
    "section": "Load data",
    "text": "Load data\n\nCode\ndatos &lt;- read_excel(\"C:/CodigoR/CameraTrapCesar/data/CT_Cesar.xlsx\")\n\n# habitat types extracted from Copernicus\nhabs &lt;- read.csv(\"C:/CodigoR/CameraTrapCesar/data/habitats.csv\")"
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#pooling-together-several-sites",
    "href": "posts/2024-06-25-species-diversity/index.html#pooling-together-several-sites",
    "title": "Species diversity",
    "section": "Pooling together several sites",
    "text": "Pooling together several sites\nFor this example I used data gathered by fundación Galictis in Perija, Colombia. I selected one year for the sites: Becerril 2021, LaPaz_Manaure 2019, MLJ, CL1, CL2 and PCF. Sometimes we need to make unique codes per camera and cameraOperation table. This was not the case.\nFor this example we are using the habitat type were the camera was installed as a way to see the sampling effort (number of cameras) per habitat type. The habitat type was extracted overlaying the camera points on top of the Land Cover 100m global dataset from COPERNICUS using Google Earth engine connected to R. How to do this will be in another post.\n\nCode# make a new column Station\n# datos_PCF &lt;- datos |&gt; dplyr::filter(Proyecto==\"CT_LaPaz_Manaure\") |&gt; unite (\"Station\", ProyectoEtapa:Salida:CT, sep = \"-\")\n\n# fix dates\ndatos$Start &lt;- as.Date(datos$Start, \"%d/%m/%Y\")\ndatos$End &lt;- as.Date(datos$End, \"%d/%m/%Y\")\ndatos$eventDate &lt;- as.Date(datos$eventDate, \"%d/%m/%Y\")\ndatos$eventDateTime &lt;- ymd_hms(paste(datos$eventDate, \" \",\n                              datos$eventTime, \":00\", sep=\"\"))\n\n# filter Becerril\ndatos_Becerril &lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"CT_Becerril\") |&gt; mutate (Station=IdGeo) |&gt; filter(Year==2021)\n\n# filter LaPaz_Manaure\ndatos_LaPaz_Manaure&lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"CT_LaPaz_Manaure\") |&gt; mutate (Station=IdGeo) |&gt; filter(Year==2019)\n\n# filter MLJ\ndatos_MLJ &lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"MLJ_TH_TS_2021\") |&gt; mutate (Station=IdGeo)\n\n# filter CL\ndatos_CL1 &lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"CL-TH2022\") |&gt; mutate (Station=IdGeo)\n# filter CL\ndatos_CL2 &lt;- datos |&gt; dplyr::filter(ProyectoEtapa==\"CL-TS2022\") |&gt; mutate (Station=IdGeo)\n\n# filter PCF\ndatos_PCF &lt;- datos |&gt; dplyr::filter(Proyecto==\"PCF\") |&gt; mutate (Station=IdGeo)\n\ndata_south &lt;- rbind(datos_LaPaz_Manaure, datos_Becerril, datos_MLJ,datos_CL1, datos_CL2,datos_PCF)\n\n# filter 2021 and make uniques\nCToperation  &lt;- data_south |&gt; \n              # filter(Year==2021) |&gt; \n              group_by(Station) |&gt; \n              mutate(minStart=min(Start), maxEnd=max(End)) |&gt;  distinct(Longitude, Latitude, minStart, maxEnd, Year) |&gt; \n  ungroup()"
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#generating-the-cameraoperation-table-and-making-detection-histories-for-all-the-species.",
    "href": "posts/2024-06-25-species-diversity/index.html#generating-the-cameraoperation-table-and-making-detection-histories-for-all-the-species.",
    "title": "Species diversity",
    "section": "Generating the cameraOperation table and making detection histories for all the species.",
    "text": "Generating the cameraOperation table and making detection histories for all the species.\nThe package CamtrapR has the function ‘cameraOperation’ which makes a table of cameras (stations) and dates (setup, puck-up), this table is key to generate the detection histories using the function ‘detectionHistory’ in the next step.\n\nCode# Generamos la matríz de operación de las cámaras\n\ncamop &lt;- cameraOperation(CTtable= CToperation, # Tabla de operación\n                         stationCol= \"Station\", # Columna que define la estación\n                         setupCol= \"minStart\", #Columna fecha de colocación\n                         retrievalCol= \"maxEnd\", #Columna fecha de retiro\n                         #hasProblems= T, # Hubo fallos de cámaras\n                         dateFormat= \"%Y-%m-%d\") #, # Formato de las fechas\n                         #cameraCol=\"CT\")\n                         # sessionCol= \"Year\")\n\n# Generar las historias de detección ---------------------------------------\n## remove plroblem species\n# ind &lt;- which(datos_PCF$Species==\"Marmosa sp.\")\n# datos_PCF &lt;- datos_PCF[-ind,]\n\nDetHist_list &lt;- lapply(unique(data_south$Species), FUN = function(x) {\n  detectionHistory(\n    recordTable         = data_south, # Tabla de registros\n    camOp                = camop, # Matriz de operación de cámaras\n    stationCol           = \"Station\",\n    speciesCol           = \"Species\",\n    recordDateTimeCol    = \"eventDateTime\",\n    recordDateTimeFormat  = \"%Y-%m-%d\",\n    species              = x,     # la función reemplaza x por cada una de las especies\n    occasionLength       = 7, # Colapso de las historias a 10 días\n    day1                 = \"station\", # (\"survey\"),or #inicia en la fecha de cada station\n    datesAsOccasionNames = FALSE,\n    includeEffort        = TRUE,\n    scaleEffort          = FALSE,\n    output               = (\"binary\"), # (\"binary\") or (\"count\")\n    #unmarkedMultFrameInput=TRUE\n    timeZone             = \"America/Bogota\" \n    )\n  }\n)\n\n# put names to the species \nnames(DetHist_list) &lt;- unique(data_south$Species)\n\n# Finally we make a new list to put all the detection histories.\nylist &lt;- lapply(DetHist_list, FUN = function(x) x$detection_history)"
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#use-the-detection-histories-to-make-the-a-matrix-for-vegan-and-the-incidence-for-inext.",
    "href": "posts/2024-06-25-species-diversity/index.html#use-the-detection-histories-to-make-the-a-matrix-for-vegan-and-the-incidence-for-inext.",
    "title": "Species diversity",
    "section": "Use the detection histories to make the a matrix for vegan and the incidence for iNEXT.",
    "text": "Use the detection histories to make the a matrix for vegan and the incidence for iNEXT.\nSpecies accumulation curves made using the package vegan, plot the increase in species richness as we add survey units. If the curve plateaus (flattens), then that suggests you have sampled the majority of the species in your survey site (camera or habitat type).\n\nCode# loop to make vegan matrix\nmat_vegan &lt;- matrix(NA, dim(ylist[[1]])[1], length(unique(data_south$Species)))\nfor(i in 1:length(unique(data_south$Species))){\n  mat_vegan[,i] &lt;- apply(ylist[[i]], 1, sum, na.rm=TRUE)\n  mat_vegan[,i] &lt;- tidyr::replace_na(mat_vegan[,i], 0) # replace na with 0\n}\n\ncolnames(mat_vegan)  &lt;- unique(data_south$Species)\nrownames(mat_vegan) &lt;- rownames(ylist[[1]])\n\nmat_vegan2 &lt;- as.data.frame(mat_vegan)\nmat_vegan2$hab &lt;- habs$hab_code\n# mat_vegan3 &lt;-  mat_vegan2 |&gt; \n  \n# Select specific rows by row numbers\nclosed_forest_rows &lt;- which(mat_vegan2$hab==\"closed_forest_evergreen_broad\")\nherbaceous_rows &lt;- which(mat_vegan2$hab==\"herbaceous_wetland\")\nherbs_rows &lt;- which(mat_vegan2$hab==\"herbs\")\nopen_forest_rows &lt;- which(mat_vegan2$hab==\"open_forest_evergreen_broad\")\nopen_forest2_rows &lt;- which(mat_vegan2$hab==\"open_forest_other\")\n\n\nclosed_forest &lt;- apply(mat_vegan2[closed_forest_rows,1:22], MARGIN = 2, sum)\nherbaceous_wetland &lt;- apply(mat_vegan2[herbaceous_rows,1:22], MARGIN = 2, sum)\nherbs  &lt;- apply(mat_vegan2[herbs_rows,1:22], MARGIN = 2, sum)\nopen_forest_evergreen &lt;- apply(mat_vegan2[open_forest_rows,1:22], MARGIN = 2, sum)\nopen_forest_other &lt;- apply(mat_vegan2[open_forest2_rows,1:22], MARGIN = 2, sum)\n\n# tb_sp &lt;- mat_vegan2 |&gt; group_by(hab)\n# hab_list &lt;- group_split(tb_sp)\n\n# make list of dataframe per habitat\nsp_by_hab &lt;- mat_vegan2 |&gt; dplyr::group_by(hab) %&gt;% split (.$hab)\n# arrange abundance (detection frecuency) mat for INEXT \ncesar_sp &lt;- t(rbind(\nt(colSums(sp_by_hab[[1]][,1:33])),\nt(colSums(sp_by_hab[[2]][,1:33])),\nt(colSums(sp_by_hab[[3]][,1:33])),\nt(colSums(sp_by_hab[[4]][,1:33])),\nt(colSums(sp_by_hab[[5]][,1:33]))\n))\n \ncolnames(cesar_sp) &lt;- names(sp_by_hab)\n\n\n\n# function to Format data to incidence and use iNext\nf_incidences &lt;- function(habitat_rows=closed_forest_rows){ylist %&gt;%  # historias de detection\n  map(~rowSums(.,na.rm = T)) %&gt;% # sumo las detecciones en cada sitio\n  reduce(cbind) %&gt;% # unimos las listas\n  as_data_frame() %&gt;% #formato dataframe\n  filter(row_number() %in% habitat_rows) |&gt; \n  t() %&gt;% # trasponer la tabla\n  as_tibble() %&gt;% #formato tibble\n  mutate_if(is.numeric,~(.&gt;=1)*1) %&gt;%  #como es incidencia, formateo a 1 y 0\n  rowSums() %&gt;%  # ahora si la suma de las incidencias en cada sitio\n  sort(decreasing=T) |&gt; \n  as_tibble() %&gt;% \n  add_row(value= length(habitat_rows), .before = 1) %&gt;%  # requiere que el primer valor sea el número de sitios\n  filter(!if_any()==0) |&gt;  # filter ceros\n  as.matrix() # Requiere formato de matriz\n}\n\n# Make incidence frequency table (is a list whit 5 habitats)\n# Make an empty list to store our data\nincidence_cesar &lt;- list() \nincidence_cesar[[1]] &lt;- f_incidences(closed_forest_rows)\nincidence_cesar[[2]] &lt;- f_incidences(herbaceous_rows)\nincidence_cesar[[3]] &lt;- f_incidences(herbs_rows)\nincidence_cesar[[4]] &lt;- f_incidences(open_forest_rows)\nincidence_cesar[[5]] &lt;- f_incidences(open_forest_other)\n\n# put names\nnames(incidence_cesar) &lt;- names(sp_by_hab)\n\n# we deleted this habitat type for making error\nincidence_cesar &lt;- within(incidence_cesar, rm(\"herbaceous_wetland\"))"
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#to-start-lets-plot-the-species-vs-sites",
    "href": "posts/2024-06-25-species-diversity/index.html#to-start-lets-plot-the-species-vs-sites",
    "title": "Species diversity",
    "section": "To start lets plot the species vs sites",
    "text": "To start lets plot the species vs sites\n\nCode# Transpose if needed to have sample site names on rows\nabund_table&lt;-mat_vegan\n# Convert to relative frequencies\nabund_table &lt;- abund_table/rowSums(abund_table)\nlibrary(reshape2)\ndf&lt;-melt(abund_table)\ncolnames(df)&lt;-c(\"Sampled_site\",\"Species\",\"Value\")\nlibrary(plyr)\nlibrary(scales)\n \n# We are going to apply transformation to our data to make it\n# easier on eyes \n \n#df&lt;-ddply(df,.(Samples),transform,rescale=scale(Value))\ndf&lt;-ddply(df,.(Sampled_site),transform,rescale=sqrt(Value))\n \n# Plot heatmap\np &lt;- ggplot(df, aes(Species, Sampled_site)) + \n  geom_tile(aes(fill = rescale),colour = \"white\") + \n  scale_fill_gradient(low = \"white\",high = \"#1E5A8C\")+\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) + theme(legend.position = \"none\",axis.ticks = element_blank(),axis.text.x = element_text(angle = 90, hjust = 1,size=6),axis.text.y = element_text(size=4))\n\n# ggplotly(p) # see interactive\n# View the plot\np\n\n\n\n\n\n\n\n\nNotice how some cameras didn’t record any species. Here showed as the gay horizontal line. Perhaps we need to delete those cameras."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#rarefaction-using-vegan",
    "href": "posts/2024-06-25-species-diversity/index.html#rarefaction-using-vegan",
    "title": "Species diversity",
    "section": "Rarefaction using vegan\n",
    "text": "Rarefaction using vegan\n\n\nNotice that sites are cameras and the accumulation is species per camera not time\n\nRarefaction is a technique to assess expected species richness. Rarefaction allows the calculation of species richness for a given number of individual samples, based on the construction of rarefaction curves.\nThe issue that occurs when sampling various species in a community is that the larger the number of individuals sampled, the more species that will be found. Rarefaction curves are created by randomly re-sampling the pool of N samples multiple times and then plotting the average number of species found in each sample (1,2, … N). “Thus rarefaction generates the expected number of species in a small collection of n individuals (or n samples) drawn at random from the large pool of N samples.”. Rarefaction curves generally grow rapidly at first, as the most common species are found, but the curves plateau as only the rarest species remain to be sampled.\n\nCode\nrarecurve(mat_vegan, col = \"blue\") \n\n\n\n\n\n\nCoderarecurve(t(cesar_sp), col = \"blue\") \n\n\n\n\n\n\nCode\nsp1 &lt;- specaccum(mat_vegan)\nsp2 &lt;- specaccum(mat_vegan, \"random\")\n# sp2\n# summary(sp2)\nplot(sp1, ci.type=\"poly\", col=\"blue\", lwd=2, ci.lty=0, ci.col=\"lightblue\")\n\n\n\n\n\n\nCode# boxplot(sp2, col=\"yellow\", add=TRUE, pch=\"+\")\n\n\nmods &lt;- fitspecaccum(sp1, \"gleason\")\nplot(mods, col=\"hotpink\")\nboxplot(sp2, col = \"yellow\", border = \"blue\", lty=1, cex=0.3, add= TRUE)\n\n\n\n\n\n\nCode\n\n## Accumulation model\npool &lt;- poolaccum(mat_vegan)\n# summary(pool, display = \"chao\")\nplot(pool)\n\n\n\n\n\n\n\nRanked abundance distribution\nAn alternative approach to species abundance distribution is to plot logarithmic abundances in decreasing order, or against ranks of species.\n\nCodek &lt;- sample(nrow(mat_vegan), 1)\nrad &lt;- radfit(mat_vegan[22,]) # species 22\n# plot(rad)\nradlattice(rad)\n\n\n\n\n\n\n\nHill Diversities using vegan\n\nCode# data(BCI)\ni &lt;- sample(nrow(mat_vegan), 20)\nmod &lt;- renyi(mat_vegan) #selecting sites with more than one record\nplot(mod)\n\n\n\n\n\n\nCodemod &lt;- renyiaccum(mat_vegan[55:89,])\nplot(mod, as.table=TRUE, col = c(1, 2, 2))\n\n\n\n\n\n\nCodepersp(mod)\n\n\n\n\n\n\n\nTotal number of species\n\nCodeDT::datatable(round(specpool(mat_vegan),3))\n\n\n\n\n\nNumber of unseen species per camera\nLook at S.chao1\n\nCodeDT::datatable(\nt(round(as.data.frame(estimateR(mat_vegan[,])),3))\n)\n\n\n\n\nCode\n# save as dataframe\nS_per_site &lt;- as.data.frame(t(round(as.data.frame(estimateR(mat_vegan[,])),3)))\n# add sites\nS_per_site$Station &lt;- rownames(S_per_site)\n\n\nIt is weird to have .5 species in some sites."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#map-it-converting-cameratrap-operation-to-sf",
    "href": "posts/2024-06-25-species-diversity/index.html#map-it-converting-cameratrap-operation-to-sf",
    "title": "Species diversity",
    "section": "Map it converting Cameratrap-operation to sf",
    "text": "Map it converting Cameratrap-operation to sf\nIn this step we convert the Cameratrap-operation table to sf, we add elevation from Amazon web Services (AWS), habitat type and species per site (S.chao1) to finally visualize the map showing the number of species as the size of the dot.\n\nCode\n# datos_distinct &lt;- datos |&gt; distinct(Longitude, Latitude, CT, Proyecto)\n\nprojlatlon &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\nCToperation_sf &lt;-  st_as_sf(x = CToperation,\n                         coords = c(\"Longitude\", \n                                    \"Latitude\"),\n                         crs = projlatlon)\n\n# write.csv(habs, \"C:/CodigoR/CameraTrapCesar/data/habitats.csv\")\nhabs &lt;- read.csv(\"C:/CodigoR/CameraTrapCesar/data/habitats.csv\")\n\nCToperation_elev_sf &lt;- get_elev_point(CToperation_sf, src = \"aws\") # get elevation from AWS\n\nCToperation_elev_sf &lt;- CToperation_elev_sf |&gt; left_join(habs, by='Station') |&gt; left_join(S_per_site, by='Station') |&gt; select(\"Station\", \"elevation\", \"minStart.x\",\"maxEnd.x\", \"Year.x\", \"hab_code\" , \"S.obs\", \"S.chao1\")\n\n# add habitat \n# CToperation_elev_sf$habs &lt;- habs$hab_code\n# see the map\nmapview(CToperation_elev_sf, zcol=\"hab_code\", cex = \"S.chao1\", alpha = 0)"
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#perhaps-it-is-easyer-to-plot-the-species-number-as-a-countour-map",
    "href": "posts/2024-06-25-species-diversity/index.html#perhaps-it-is-easyer-to-plot-the-species-number-as-a-countour-map",
    "title": "Species diversity",
    "section": "Perhaps it is easyer to plot the species number as a countour map",
    "text": "Perhaps it is easyer to plot the species number as a countour map\nOne advantage of using the eks density estimate, is that it is clearer what the output means. The 20% contour means “20% of the measurements lie inside this contour”. The documentation for eks takes issue with how stat_density_2d from ggplot2 does its calculation, I don’t know who is right because the estimated value is species and both graphs are similar.\n\nCode# select chao\nspecies &lt;- dplyr::select(CToperation_elev_sf, \"S.chao1\")\n# hakeoides_coord &lt;- data.frame(sf::st_coordinates(hakeoides))\nSta_den &lt;- eks::st_kde(species) # calculate density\n\n# VERY conveniently, eks can generate an sf file of contour lines\ncontours &lt;- eks::st_get_contour(Sta_den, cont=c( 10,20,30,40,50,60,70,80, 90)) %&gt;% \n  mutate(value=as.numeric(levels(contlabel)))\n\n\n# pal_fun &lt;- leaflet::colorQuantile(\"YlOrRd\", NULL, n = 5)\n\np_popup &lt;- paste(\"Species\", as.numeric(levels(contours$estimate)), \"number\")\n\n\ntmap::tmap_mode(\"view\") # set mode to interactive plots\n\ntmap::tm_shape(species) + \n    tmap::tm_sf(col=\"black\", size=0.2) +\n  #   contours from eks\n  tmap::tm_shape(contours) +\n    tmap::tm_polygons(\"estimate\",\n                      palette=\"Reds\",\n                      alpha=0.5 )\n\n\n\n\n\nCode\n\n## geom_sf plot\n# ## suitable smoothing matrix gives optimally smoothed contours\n# gs1 &lt;- ggplot(Sta_den) + geom_sf(data=CToperation_elev_sf, fill=NA) + ggthemes::theme_map() +\n#     colorspace::scale_fill_discrete_sequential(palette=\"Heat2\") \n# gs1 + geom_sf(data=st_get_contour(Sta_den), aes(fill=label_percent(contlabel))) +\n#     coord_sf(xlim=xlim, ylim=ylim) \n\n\n\nIn general terms the species estimate per site seems to be larger near the coal mine and decrease with the distance to the mine. Also notice kernel density estimates are larger than s.chao1."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#nonmetric-multidimensional-scaling-nmds",
    "href": "posts/2024-06-25-species-diversity/index.html#nonmetric-multidimensional-scaling-nmds",
    "title": "Species diversity",
    "section": "Nonmetric Multidimensional Scaling (NMDS)",
    "text": "Nonmetric Multidimensional Scaling (NMDS)\nOften in ecological research, we are interested not only in comparing univariate descriptors of communities, like diversity, but also in how the constituent species — or the species composition — changes from one community to the next. One common tool to do this is non-metric multidimensional scaling, or NMDS. The goal of NMDS is to collapse information from multiple dimensions (e.g, from multiple communities, sites were the cameratrap was installed, etc.) into just a few, so that they can be visualized and interpreted. Unlike other ordination techniques that rely on (primarily Euclidean) distances, such as Principal Coordinates Analysis, NMDS uses rank orders, and thus is an extremely flexible technique that can accommodate a variety of different kinds of data.\nIf the treatment is continuous, such as an environmental gradient, then it might be useful to plot contour lines rather than convex hulls. We can get some, elevation data for our original community matrix and overlay them onto the NMDS plot using ordisurf.\n\nCode\nexample_NMDS=metaMDS(as.data.frame(mat_vegan), \n                     distance=\"euclidean\",\n                     zerodist = \"ignore\",\n                     trymax=300,\n                     k=5) # T\n#&gt; Wisconsin double standardization\n#&gt; Run 0 stress 0.1177774 \n#&gt; Run 1 stress 0.1209687 \n#&gt; Run 2 stress 0.118209 \n#&gt; ... Procrustes: rmse 0.0624724  max resid 0.2850191 \n#&gt; Run 3 stress 0.1198435 \n#&gt; Run 4 stress 0.1180729 \n#&gt; ... Procrustes: rmse 0.06611055  max resid 0.2597874 \n#&gt; Run 5 stress 0.1192495 \n#&gt; Run 6 stress 0.1191001 \n#&gt; Run 7 stress 0.118703 \n#&gt; Run 8 stress 0.1189469 \n#&gt; Run 9 stress 0.1195762 \n#&gt; Run 10 stress 0.1176784 \n#&gt; ... New best solution\n#&gt; ... Procrustes: rmse 0.05429121  max resid 0.2019991 \n#&gt; Run 11 stress 0.1184288 \n#&gt; Run 12 stress 0.1202311 \n#&gt; Run 13 stress 0.118725 \n#&gt; Run 14 stress 0.1180123 \n#&gt; ... Procrustes: rmse 0.07239269  max resid 0.2365554 \n#&gt; Run 15 stress 0.1205773 \n#&gt; Run 16 stress 0.1191899 \n#&gt; Run 17 stress 0.1188361 \n#&gt; Run 18 stress 0.1184365 \n#&gt; Run 19 stress 0.1186787 \n#&gt; Run 20 stress 0.1197125 \n#&gt; Run 21 stress 0.1200854 \n#&gt; Run 22 stress 0.1193755 \n#&gt; Run 23 stress 0.1190254 \n#&gt; Run 24 stress 0.1189164 \n#&gt; Run 25 stress 0.1201878 \n#&gt; Run 26 stress 0.1188782 \n#&gt; Run 27 stress 0.1176361 \n#&gt; ... New best solution\n#&gt; ... Procrustes: rmse 0.04887124  max resid 0.1965572 \n#&gt; Run 28 stress 0.1193284 \n#&gt; Run 29 stress 0.1188695 \n#&gt; Run 30 stress 0.1187362 \n#&gt; Run 31 stress 0.1199703 \n#&gt; Run 32 stress 0.1200902 \n#&gt; Run 33 stress 0.1204805 \n#&gt; Run 34 stress 0.1183634 \n#&gt; Run 35 stress 0.1200571 \n#&gt; Run 36 stress 0.120608 \n#&gt; Run 37 stress 0.1174555 \n#&gt; ... New best solution\n#&gt; ... Procrustes: rmse 0.06466373  max resid 0.2138267 \n#&gt; Run 38 stress 0.1199128 \n#&gt; Run 39 stress 0.1182874 \n#&gt; Run 40 stress 0.1179684 \n#&gt; Run 41 stress 0.1181646 \n#&gt; Run 42 stress 0.119316 \n#&gt; Run 43 stress 0.1179985 \n#&gt; Run 44 stress 0.1185667 \n#&gt; Run 45 stress 0.119884 \n#&gt; Run 46 stress 0.117537 \n#&gt; ... Procrustes: rmse 0.07312771  max resid 0.3005623 \n#&gt; Run 47 stress 0.1193484 \n#&gt; Run 48 stress 0.1196886 \n#&gt; Run 49 stress 0.1186485 \n#&gt; Run 50 stress 0.119077 \n#&gt; Run 51 stress 0.1196321 \n#&gt; Run 52 stress 0.1196958 \n#&gt; Run 53 stress 0.1195044 \n#&gt; Run 54 stress 0.12194 \n#&gt; Run 55 stress 0.1187133 \n#&gt; Run 56 stress 0.1197584 \n#&gt; Run 57 stress 0.121428 \n#&gt; Run 58 stress 0.1181714 \n#&gt; Run 59 stress 0.1198407 \n#&gt; Run 60 stress 0.1181267 \n#&gt; Run 61 stress 0.1221596 \n#&gt; Run 62 stress 0.1192875 \n#&gt; Run 63 stress 0.1177634 \n#&gt; ... Procrustes: rmse 0.0623561  max resid 0.3224084 \n#&gt; Run 64 stress 0.1189194 \n#&gt; Run 65 stress 0.1185056 \n#&gt; Run 66 stress 0.1193711 \n#&gt; Run 67 stress 0.1183661 \n#&gt; Run 68 stress 0.1176511 \n#&gt; ... Procrustes: rmse 0.06957101  max resid 0.2794323 \n#&gt; Run 69 stress 0.1185096 \n#&gt; Run 70 stress 0.1197008 \n#&gt; Run 71 stress 0.1193223 \n#&gt; Run 72 stress 0.1191655 \n#&gt; Run 73 stress 0.1198682 \n#&gt; Run 74 stress 0.121012 \n#&gt; Run 75 stress 0.1200953 \n#&gt; Run 76 stress 0.1190634 \n#&gt; Run 77 stress 0.1197367 \n#&gt; Run 78 stress 0.1179696 \n#&gt; Run 79 stress 0.1191531 \n#&gt; Run 80 stress 0.119275 \n#&gt; Run 81 stress 0.1217399 \n#&gt; Run 82 stress 0.118579 \n#&gt; Run 83 stress 0.1193536 \n#&gt; Run 84 stress 0.1187558 \n#&gt; Run 85 stress 0.1201277 \n#&gt; Run 86 stress 0.1189315 \n#&gt; Run 87 stress 0.1198242 \n#&gt; Run 88 stress 0.1211253 \n#&gt; Run 89 stress 0.118587 \n#&gt; Run 90 stress 0.1206972 \n#&gt; Run 91 stress 0.1199921 \n#&gt; Run 92 stress 0.1205592 \n#&gt; Run 93 stress 0.1207654 \n#&gt; Run 94 stress 0.1186479 \n#&gt; Run 95 stress 0.1197808 \n#&gt; Run 96 stress 0.1207139 \n#&gt; Run 97 stress 0.1183789 \n#&gt; Run 98 stress 0.1188899 \n#&gt; Run 99 stress 0.1183217 \n#&gt; Run 100 stress 0.1189853 \n#&gt; Run 101 stress 0.1193416 \n#&gt; Run 102 stress 0.1182956 \n#&gt; Run 103 stress 0.1186056 \n#&gt; Run 104 stress 0.118168 \n#&gt; Run 105 stress 0.119432 \n#&gt; Run 106 stress 0.1205248 \n#&gt; Run 107 stress 0.1205914 \n#&gt; Run 108 stress 0.1190873 \n#&gt; Run 109 stress 0.1177303 \n#&gt; ... Procrustes: rmse 0.06420113  max resid 0.1644107 \n#&gt; Run 110 stress 0.1195587 \n#&gt; Run 111 stress 0.1198823 \n#&gt; Run 112 stress 0.1177629 \n#&gt; ... Procrustes: rmse 0.06946455  max resid 0.1762657 \n#&gt; Run 113 stress 0.119796 \n#&gt; Run 114 stress 0.120048 \n#&gt; Run 115 stress 0.118166 \n#&gt; Run 116 stress 0.118985 \n#&gt; Run 117 stress 0.1180202 \n#&gt; Run 118 stress 0.1183499 \n#&gt; Run 119 stress 0.119476 \n#&gt; Run 120 stress 0.1191711 \n#&gt; Run 121 stress 0.1193472 \n#&gt; Run 122 stress 0.1184224 \n#&gt; Run 123 stress 0.1182083 \n#&gt; Run 124 stress 0.1193056 \n#&gt; Run 125 stress 0.1189461 \n#&gt; Run 126 stress 0.1212718 \n#&gt; Run 127 stress 0.1199491 \n#&gt; Run 128 stress 0.1197179 \n#&gt; Run 129 stress 0.118448 \n#&gt; Run 130 stress 0.1210185 \n#&gt; Run 131 stress 0.1183708 \n#&gt; Run 132 stress 0.1180533 \n#&gt; Run 133 stress 0.1192362 \n#&gt; Run 134 stress 0.119377 \n#&gt; Run 135 stress 0.1185904 \n#&gt; Run 136 stress 0.1186234 \n#&gt; Run 137 stress 0.1187005 \n#&gt; Run 138 stress 0.1176376 \n#&gt; ... Procrustes: rmse 0.07484715  max resid 0.2725871 \n#&gt; Run 139 stress 0.1212374 \n#&gt; Run 140 stress 0.1187494 \n#&gt; Run 141 stress 0.1184809 \n#&gt; Run 142 stress 0.1172038 \n#&gt; ... New best solution\n#&gt; ... Procrustes: rmse 0.04079099  max resid 0.2089459 \n#&gt; Run 143 stress 0.1206096 \n#&gt; Run 144 stress 0.1194221 \n#&gt; Run 145 stress 0.1194105 \n#&gt; Run 146 stress 0.1194266 \n#&gt; Run 147 stress 0.120922 \n#&gt; Run 148 stress 0.1195354 \n#&gt; Run 149 stress 0.1196149 \n#&gt; Run 150 stress 0.1196258 \n#&gt; Run 151 stress 0.117419 \n#&gt; ... Procrustes: rmse 0.04584884  max resid 0.1794874 \n#&gt; Run 152 stress 0.1197675 \n#&gt; Run 153 stress 0.1195624 \n#&gt; Run 154 stress 0.1194191 \n#&gt; Run 155 stress 0.1205802 \n#&gt; Run 156 stress 0.118718 \n#&gt; Run 157 stress 0.1194109 \n#&gt; Run 158 stress 0.119818 \n#&gt; Run 159 stress 0.1185147 \n#&gt; Run 160 stress 0.1193766 \n#&gt; Run 161 stress 0.1179982 \n#&gt; Run 162 stress 0.1187516 \n#&gt; Run 163 stress 0.1193825 \n#&gt; Run 164 stress 0.1195552 \n#&gt; Run 165 stress 0.1197203 \n#&gt; Run 166 stress 0.1188373 \n#&gt; Run 167 stress 0.1177639 \n#&gt; Run 168 stress 0.117891 \n#&gt; Run 169 stress 0.1186089 \n#&gt; Run 170 stress 0.1208048 \n#&gt; Run 171 stress 0.1189637 \n#&gt; Run 172 stress 0.120335 \n#&gt; Run 173 stress 0.1201351 \n#&gt; Run 174 stress 0.1186743 \n#&gt; Run 175 stress 0.1199042 \n#&gt; Run 176 stress 0.1197071 \n#&gt; Run 177 stress 0.1194127 \n#&gt; Run 178 stress 0.1212232 \n#&gt; Run 179 stress 0.1192484 \n#&gt; Run 180 stress 0.118293 \n#&gt; Run 181 stress 0.1189459 \n#&gt; Run 182 stress 0.1197905 \n#&gt; Run 183 stress 0.1197827 \n#&gt; Run 184 stress 0.1189932 \n#&gt; Run 185 stress 0.118768 \n#&gt; Run 186 stress 0.1180644 \n#&gt; Run 187 stress 0.1198423 \n#&gt; Run 188 stress 0.118388 \n#&gt; Run 189 stress 0.1187335 \n#&gt; Run 190 stress 0.1215618 \n#&gt; Run 191 stress 0.1192225 \n#&gt; Run 192 stress 0.1174692 \n#&gt; ... Procrustes: rmse 0.06381036  max resid 0.2596444 \n#&gt; Run 193 stress 0.119184 \n#&gt; Run 194 stress 0.118651 \n#&gt; Run 195 stress 0.1188074 \n#&gt; Run 196 stress 0.1200572 \n#&gt; Run 197 stress 0.1191934 \n#&gt; Run 198 stress 0.1188984 \n#&gt; Run 199 stress 0.1192702 \n#&gt; Run 200 stress 0.1196948 \n#&gt; Run 201 stress 0.1203192 \n#&gt; Run 202 stress 0.1178779 \n#&gt; Run 203 stress 0.1174173 \n#&gt; ... Procrustes: rmse 0.06006906  max resid 0.3298726 \n#&gt; Run 204 stress 0.119164 \n#&gt; Run 205 stress 0.1198509 \n#&gt; Run 206 stress 0.1179513 \n#&gt; Run 207 stress 0.1186613 \n#&gt; Run 208 stress 0.1199664 \n#&gt; Run 209 stress 0.1183295 \n#&gt; Run 210 stress 0.1203662 \n#&gt; Run 211 stress 0.1180113 \n#&gt; Run 212 stress 0.1179495 \n#&gt; Run 213 stress 0.1196066 \n#&gt; Run 214 stress 0.1186845 \n#&gt; Run 215 stress 0.1180992 \n#&gt; Run 216 stress 0.1213641 \n#&gt; Run 217 stress 0.1180534 \n#&gt; Run 218 stress 0.1187341 \n#&gt; Run 219 stress 0.1199809 \n#&gt; Run 220 stress 0.1191373 \n#&gt; Run 221 stress 0.1187468 \n#&gt; Run 222 stress 0.1196124 \n#&gt; Run 223 stress 0.1192098 \n#&gt; Run 224 stress 0.1195232 \n#&gt; Run 225 stress 0.1179556 \n#&gt; Run 226 stress 0.118645 \n#&gt; Run 227 stress 0.1182333 \n#&gt; Run 228 stress 0.1197025 \n#&gt; Run 229 stress 0.1195408 \n#&gt; Run 230 stress 0.1206107 \n#&gt; Run 231 stress 0.1176344 \n#&gt; ... Procrustes: rmse 0.0644239  max resid 0.1621718 \n#&gt; Run 232 stress 0.1181272 \n#&gt; Run 233 stress 0.1205944 \n#&gt; Run 234 stress 0.11745 \n#&gt; ... Procrustes: rmse 0.05556895  max resid 0.3025734 \n#&gt; Run 235 stress 0.1190435 \n#&gt; Run 236 stress 0.1184543 \n#&gt; Run 237 stress 0.1202336 \n#&gt; Run 238 stress 0.1200461 \n#&gt; Run 239 stress 0.1206267 \n#&gt; Run 240 stress 0.1173029 \n#&gt; ... Procrustes: rmse 0.06638154  max resid 0.2565834 \n#&gt; Run 241 stress 0.1193184 \n#&gt; Run 242 stress 0.1199814 \n#&gt; Run 243 stress 0.1190829 \n#&gt; Run 244 stress 0.1187156 \n#&gt; Run 245 stress 0.1193862 \n#&gt; Run 246 stress 0.1210792 \n#&gt; Run 247 stress 0.1198936 \n#&gt; Run 248 stress 0.1191846 \n#&gt; Run 249 stress 0.1182276 \n#&gt; Run 250 stress 0.1182157 \n#&gt; Run 251 stress 0.1197108 \n#&gt; Run 252 stress 0.1209824 \n#&gt; Run 253 stress 0.1188383 \n#&gt; Run 254 stress 0.1182051 \n#&gt; Run 255 stress 0.1180277 \n#&gt; Run 256 stress 0.1187046 \n#&gt; Run 257 stress 0.1184256 \n#&gt; Run 258 stress 0.1202519 \n#&gt; Run 259 stress 0.1187021 \n#&gt; Run 260 stress 0.117941 \n#&gt; Run 261 stress 0.1195573 \n#&gt; Run 262 stress 0.1193972 \n#&gt; Run 263 stress 0.1199747 \n#&gt; Run 264 stress 0.119298 \n#&gt; Run 265 stress 0.1201172 \n#&gt; Run 266 stress 0.119013 \n#&gt; Run 267 stress 0.1197673 \n#&gt; Run 268 stress 0.119988 \n#&gt; Run 269 stress 0.1193972 \n#&gt; Run 270 stress 0.1175737 \n#&gt; ... Procrustes: rmse 0.05301357  max resid 0.3142549 \n#&gt; Run 271 stress 0.1189313 \n#&gt; Run 272 stress 0.1196079 \n#&gt; Run 273 stress 0.1183055 \n#&gt; Run 274 stress 0.1198814 \n#&gt; Run 275 stress 0.1190489 \n#&gt; Run 276 stress 0.1192575 \n#&gt; Run 277 stress 0.1179221 \n#&gt; Run 278 stress 0.1184128 \n#&gt; Run 279 stress 0.1191288 \n#&gt; Run 280 stress 0.1201759 \n#&gt; Run 281 stress 0.1192714 \n#&gt; Run 282 stress 0.1186438 \n#&gt; Run 283 stress 0.1195361 \n#&gt; Run 284 stress 0.1176095 \n#&gt; ... Procrustes: rmse 0.07576862  max resid 0.2951109 \n#&gt; Run 285 stress 0.1192549 \n#&gt; Run 286 stress 0.1185006 \n#&gt; Run 287 stress 0.1170757 \n#&gt; ... New best solution\n#&gt; ... Procrustes: rmse 0.06134378  max resid 0.3177206 \n#&gt; Run 288 stress 0.1193497 \n#&gt; Run 289 stress 0.1203118 \n#&gt; Run 290 stress 0.1187363 \n#&gt; Run 291 stress 0.1201647 \n#&gt; Run 292 stress 0.1184864 \n#&gt; Run 293 stress 0.1194933 \n#&gt; Run 294 stress 0.1204636 \n#&gt; Run 295 stress 0.1177112 \n#&gt; Run 296 stress 0.1202181 \n#&gt; Run 297 stress 0.1205242 \n#&gt; Run 298 stress 0.1184913 \n#&gt; Run 299 stress 0.1192696 \n#&gt; Run 300 stress 0.1202944 \n#&gt; *** Best solution was not repeated -- monoMDS stopping criteria:\n#&gt;    300: no. of iterations &gt;= maxit\n\n# plot the graph\nvegan::ordisurf((example_NMDS),CToperation_elev_sf$elevation,main=\"\",col=\"forestgreen\", trymax=100) # bubble = 2\n#&gt; \n#&gt; Family: gaussian \n#&gt; Link function: identity \n#&gt; \n#&gt; Formula:\n#&gt; y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n#&gt; \n#&gt; Estimated degrees of freedom:\n#&gt; 5.67  total = 6.67 \n#&gt; \n#&gt; REML score: 724.6911\nvegan::orditorp(example_NMDS,display=\"species\",col=\"blue\",air=0.1,\n   cex=0.5)\n\n\n\n\n\n\n\nWe can make a similar plot using gg_ordisurf from the package ggordiplots but also incorporating habitat type.\n\nCode# ggordiplots::gg_ordisurf()\n# To fit a surface with ggordiplots:\n\n \nordiplot &lt;- gg_ordisurf(ord = example_NMDS, \n                        env.var = CToperation_elev_sf$elevation,\n                        var.label = \"Elevation\",\n                        pt.size = 2,\n                        groups = CToperation_elev_sf$hab_code,\n                        binwidth = 50)\n\n\n\n\n\n\nCode\n# ggplotly(ordiplot$plot) # see interactive\n\n# # alternative using biodiversityR\n# \n# A1.surface &lt;- ordisurf( y=example_NMDS)\n# A1.grid &lt;- ordisurfgrid.long(A1.surface)\n# # Preparing the plot\n# \n# plotgg4 &lt;- ggplot() + \n#     geom_contour_filled(data=A1.grid, \n#                         aes(x=x, y=y, z=z)) +\n#     geom_vline(xintercept = c(0), color = \"grey70\", linetype = 2) +\n#     geom_hline(yintercept = c(0), color = \"grey70\", linetype = 2) +  \n#     xlab(axis.long2[1, \"label\"]) +\n#     ylab(axis.long2[2, \"label\"]) +  \n#     scale_x_continuous(sec.axis = dup_axis(labels=NULL, name=NULL)) +\n#     scale_y_continuous(sec.axis = dup_axis(labels=NULL, name=NULL)) +\n#     geom_point(data=sites.long2, \n#                aes(x=axis1, y=axis2, shape=Management), \n#                colour=\"red\", size=4) +\n#     BioR.theme +\n#     scale_fill_viridis_d() +\n#     labs(fill=\"A1\") +\n#     coord_fixed(ratio=1)\n# # and seeing the plot.\n# \n# plotgg4\n\n\nThe contours connect species in the ordination space that are predicted to have the same elevation. Notice this is not a geographic map, it is a multivariate space."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#rarefaction-using-inext",
    "href": "posts/2024-06-25-species-diversity/index.html#rarefaction-using-inext",
    "title": "Species diversity",
    "section": "Rarefaction using iNEXT\n",
    "text": "Rarefaction using iNEXT\n\n\nCode\n\n\nout &lt;- iNEXT(incidence_cesar, # The data frame\n             q=0,# The type of diversity estimator \n             datatype=\"incidence_freq\",   # The type of analysis\n             knots=40,                    # The number of data points \n             se=TRUE,                     # confidence intervals\n             conf=0.95,                   # The level of confidence intervals\n             nboot=100)                    # The number of bootstraps \n\nggiNEXT(out, type=1)\n\n\n\n\n\n\nCodeggiNEXT(out, type=2)\n\n\n\n\n\n\nCodeggiNEXT(out, type=3)\n\n\n\n\n\n\nCode\np1 &lt;- ggiNEXT(out, type=1)+ theme_classic() +   #  type 1 = the diversity estimator\n        labs(x = \"Survey sites\", y = \"Richness\")\n  \np2 &lt;- ggiNEXT(out, type=2)+ theme_classic() +    #  type 2 = the survey coverage\n        labs(x = \"Survey sites\")\n    \ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\nCode##############\nout2 &lt;- iNEXT(incidence_cesar, q=c(0,1,2) ,datatype=\"incidence_freq\" )\n\nggiNEXT(out2, type=1, facet.var=\"Order.q\", color.var=\"Assemblage\") + theme_classic() \n\n\n\n\n\n\n\nThe iNEXT package is well suited for comparisons of diversity indices through the use of hill numbers - of which the q value = 1 represents the traditional diversity indices: The species richness is q = 0. The Shannon index is (q=1), and Simpson is (q=2). Note Increasing values of q reduces the influence of rare species on our estimate of community diversity."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#package-citation",
    "href": "posts/2024-06-25-species-diversity/index.html#package-citation",
    "title": "Species diversity",
    "section": "Package Citation",
    "text": "Package Citation\n\nCodepkgs &lt;- cite_packages(output = \"paragraph\", out.dir = \".\") #knitr::kable(pkgs)\npkgs\n\nWe used R version 4.4.2 (R Core Team 2024) and the following R packages: camtrapR v. 2.3.0 (Niedballa et al. 2016), devtools v. 2.4.5 (Wickham et al. 2022), DT v. 0.33 (Xie, Cheng, and Tan 2024), eks v. 1.0.5 (Duong 2024), elevatr v. 0.99.0 (Hollister et al. 2023), ggforce v. 0.4.2 (Pedersen 2024a), ggordiplots v. 0.4.3 (Quensen, Simpson, and Oksanen 2024), ggvegan v. 0.1.999 (Simpson and Oksanen 2023), gridExtra v. 2.3 (Auguie 2017), iNEXT v. 3.0.1 (Chao et al. 2014; Hsieh, Ma, and Chao 2024), kableExtra v. 1.4.0 (Zhu 2024), knitr v. 1.49 (Xie 2014, 2015, 2024), mapview v. 2.11.2 (Appelhans et al. 2023), MeanRarity v. 0.0.1.5 (Roswell and Dushoff 2023), patchwork v. 1.3.0 (Pedersen 2024b), plotly v. 4.10.4 (Sievert 2020), plyr v. 1.8.9 (Wickham 2011), reshape2 v. 1.4.4 (Wickham 2007), rmarkdown v. 2.29 (Xie, Allaire, and Grolemund 2018; Xie, Dervieux, and Riederer 2020; Allaire et al. 2024), scales v. 1.3.0 (Wickham, Pedersen, and Seidel 2023), sf v. 1.0.19 (Pebesma 2018; Pebesma and Bivand 2023), SpadeR v. 0.1.1 (Chao et al. 2016), tidyverse v. 2.0.0 (Wickham et al. 2019), tmap v. 4.0 (Tennekes 2018), vegan v. 2.6.8 (Oksanen et al. 2024)."
  },
  {
    "objectID": "posts/2024-06-25-species-diversity/index.html#sesion-info",
    "href": "posts/2024-06-25-species-diversity/index.html#sesion-info",
    "title": "Species diversity",
    "section": "Sesion info",
    "text": "Sesion info\n\nSession info\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.4.2 (2024-10-31 ucrt)\n#&gt;  os       Windows 10 x64 (build 19045)\n#&gt;  system   x86_64, mingw32\n#&gt;  ui       RTerm\n#&gt;  language (EN)\n#&gt;  collate  Spanish_Colombia.utf8\n#&gt;  ctype    Spanish_Colombia.utf8\n#&gt;  tz       America/Bogota\n#&gt;  date     2025-07-11\n#&gt;  pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n#&gt;  ! package           * version    date (UTC) lib source\n#&gt;    abind               1.4-8      2024-09-12 [1] CRAN (R 4.4.1)\n#&gt;    base64enc           0.1-3      2015-07-28 [1] CRAN (R 4.4.0)\n#&gt;    brew                1.0-10     2023-12-16 [1] CRAN (R 4.4.2)\n#&gt;    bslib               0.8.0      2024-07-29 [1] CRAN (R 4.4.2)\n#&gt;    cachem              1.1.0      2024-05-16 [1] CRAN (R 4.4.2)\n#&gt;    camtrapR          * 2.3.0      2024-02-26 [1] CRAN (R 4.4.2)\n#&gt;    cellranger          1.1.0      2016-07-27 [1] CRAN (R 4.4.2)\n#&gt;    class               7.3-22     2023-05-03 [2] CRAN (R 4.4.2)\n#&gt;    classInt            0.4-10     2023-09-05 [1] CRAN (R 4.4.2)\n#&gt;    cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.2)\n#&gt;    cluster             2.1.6      2023-12-01 [2] CRAN (R 4.4.2)\n#&gt;    codetools           0.2-20     2024-03-31 [2] CRAN (R 4.4.2)\n#&gt;    colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.2)\n#&gt;    cols4all            0.8        2024-10-16 [1] CRAN (R 4.4.2)\n#&gt;    crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.2)\n#&gt;    crosstalk           1.2.1      2023-11-23 [1] CRAN (R 4.4.2)\n#&gt;    curl                6.0.0      2024-11-05 [1] CRAN (R 4.4.2)\n#&gt;    data.table          1.16.4     2024-12-06 [1] CRAN (R 4.4.2)\n#&gt;    DBI                 1.2.3      2024-06-02 [1] CRAN (R 4.4.2)\n#&gt;    devtools            2.4.5      2022-10-11 [1] CRAN (R 4.4.2)\n#&gt;    dichromat           2.0-0.1    2022-05-02 [1] CRAN (R 4.4.0)\n#&gt;    digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.2)\n#&gt;    dplyr             * 1.1.4      2023-11-17 [1] CRAN (R 4.4.2)\n#&gt;    DT                * 0.33       2024-04-04 [1] CRAN (R 4.4.2)\n#&gt;    e1071               1.7-16     2024-09-16 [1] CRAN (R 4.4.2)\n#&gt;    eks               * 1.0.5      2024-05-01 [1] CRAN (R 4.4.2)\n#&gt;    elevatr           * 0.99.0     2023-09-12 [1] CRAN (R 4.4.2)\n#&gt;    ellipsis            0.3.2      2021-04-29 [1] CRAN (R 4.4.2)\n#&gt;    evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.2)\n#&gt;    farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.2)\n#&gt;    fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.2)\n#&gt;    forcats           * 1.0.0      2023-01-29 [1] CRAN (R 4.4.2)\n#&gt;    fs                  1.6.5      2024-10-30 [1] CRAN (R 4.4.2)\n#&gt;    generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.2)\n#&gt;    ggforce           * 0.4.2      2024-02-19 [1] CRAN (R 4.4.2)\n#&gt;    ggordiplots       * 0.4.3      2024-01-14 [1] CRAN (R 4.4.2)\n#&gt;    ggplot2           * 3.5.2      2025-04-09 [1] CRAN (R 4.4.3)\n#&gt;    ggrepel             0.9.6      2024-09-07 [1] CRAN (R 4.4.2)\n#&gt;    ggvegan           * 0.1.999    2024-12-15 [1] Github (gavinsimpson/ggvegan@058c08c)\n#&gt;    glue              * 1.8.0      2024-09-30 [1] CRAN (R 4.4.2)\n#&gt;    grateful          * 0.2.10     2024-09-04 [1] CRAN (R 4.4.2)\n#&gt;    gridExtra         * 2.3        2017-09-09 [1] CRAN (R 4.4.2)\n#&gt;    gtable              0.3.6      2024-10-25 [1] CRAN (R 4.4.2)\n#&gt;    hms                 1.1.3      2023-03-21 [1] CRAN (R 4.4.2)\n#&gt;    htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.2)\n#&gt;    htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.2)\n#&gt;    httpuv              1.6.15     2024-03-26 [1] CRAN (R 4.4.2)\n#&gt;    httr                1.4.7      2023-08-15 [1] CRAN (R 4.4.2)\n#&gt;    iNEXT             * 3.0.1      2024-03-24 [1] CRAN (R 4.4.2)\n#&gt;    isoband             0.2.7      2022-12-20 [1] CRAN (R 4.4.2)\n#&gt;    jquerylib           0.1.4      2021-04-26 [1] CRAN (R 4.4.2)\n#&gt;    jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.2)\n#&gt;    kableExtra        * 1.4.0      2024-01-24 [1] CRAN (R 4.4.2)\n#&gt;    KernSmooth          2.23-24    2024-05-17 [2] CRAN (R 4.4.2)\n#&gt;    knitr             * 1.49       2024-11-08 [1] CRAN (R 4.4.2)\n#&gt;    ks                  1.14.3     2024-09-20 [1] CRAN (R 4.4.2)\n#&gt;    labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.0)\n#&gt;    later               1.3.2      2023-12-06 [1] CRAN (R 4.4.2)\n#&gt;    lattice           * 0.22-6     2024-03-20 [2] CRAN (R 4.4.2)\n#&gt;    lazyeval            0.2.2      2019-03-15 [1] CRAN (R 4.4.2)\n#&gt;    leafem              0.2.3      2023-09-17 [1] CRAN (R 4.4.2)\n#&gt;    leaflegend          1.2.1      2024-05-09 [1] CRAN (R 4.4.2)\n#&gt;    leaflet             2.2.2      2024-03-26 [1] CRAN (R 4.4.2)\n#&gt;    leaflet.providers   2.0.0      2023-10-17 [1] CRAN (R 4.4.2)\n#&gt;    leafpop             0.1.0      2021-05-22 [1] CRAN (R 4.4.2)\n#&gt;    leafsync            0.1.0      2019-03-05 [1] CRAN (R 4.4.2)\n#&gt;    lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.2)\n#&gt;    lubridate         * 1.9.4      2024-12-08 [1] CRAN (R 4.4.2)\n#&gt;    lwgeom              0.2-14     2024-02-21 [1] CRAN (R 4.4.2)\n#&gt;    magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.2)\n#&gt;    maplegend           0.2.0      2024-11-12 [1] CRAN (R 4.4.2)\n#&gt;    mapsf               0.12.0     2024-10-22 [1] CRAN (R 4.4.2)\n#&gt;    mapview           * 2.11.2     2023-10-13 [1] CRAN (R 4.4.2)\n#&gt;    MASS                7.3-61     2024-06-13 [2] CRAN (R 4.4.2)\n#&gt;    Matrix              1.7-1      2024-10-18 [2] CRAN (R 4.4.2)\n#&gt;    mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.2)\n#&gt;    MeanRarity        * 0.0.1.0005 2024-12-15 [1] Github (mikeroswell/MeanRarity@a8b518d)\n#&gt;    memoise             2.0.1      2021-11-26 [1] CRAN (R 4.4.2)\n#&gt;    mgcv                1.9-1      2023-12-21 [2] CRAN (R 4.4.2)\n#&gt;    microbenchmark      1.5.0      2024-09-04 [1] CRAN (R 4.4.2)\n#&gt;    mime                0.12       2021-09-28 [1] CRAN (R 4.4.0)\n#&gt;    miniUI              0.1.1.1    2018-05-18 [1] CRAN (R 4.4.2)\n#&gt;    munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.2)\n#&gt;    mvtnorm             1.3-2      2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    nlme                3.1-166    2024-08-14 [2] CRAN (R 4.4.2)\n#&gt;    patchwork         * 1.3.0      2024-09-16 [1] CRAN (R 4.4.2)\n#&gt;    permute           * 0.9-7      2022-01-27 [1] CRAN (R 4.4.2)\n#&gt;    pillar              1.10.1     2025-01-07 [1] CRAN (R 4.4.2)\n#&gt;    pkgbuild            1.4.5      2024-10-28 [1] CRAN (R 4.4.2)\n#&gt;    pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.2)\n#&gt;    pkgload             1.4.0      2024-06-28 [1] CRAN (R 4.4.2)\n#&gt;    plotly            * 4.10.4     2024-01-13 [1] CRAN (R 4.4.2)\n#&gt;    plyr              * 1.8.9      2023-10-02 [1] CRAN (R 4.4.2)\n#&gt;    png                 0.1-8      2022-11-29 [1] CRAN (R 4.4.0)\n#&gt;    polyclip            1.10-7     2024-07-23 [1] CRAN (R 4.4.1)\n#&gt;    pracma              2.4.4      2023-11-10 [1] CRAN (R 4.4.2)\n#&gt;    prettyunits         1.2.0      2023-09-24 [1] CRAN (R 4.4.2)\n#&gt;    profvis             0.4.0      2024-09-20 [1] CRAN (R 4.4.2)\n#&gt;    progress            1.2.3      2023-12-06 [1] CRAN (R 4.4.2)\n#&gt;    progressr           0.15.0     2024-10-29 [1] CRAN (R 4.4.2)\n#&gt;    promises            1.3.0      2024-04-05 [1] CRAN (R 4.4.2)\n#&gt;    proxy               0.4-27     2022-06-09 [1] CRAN (R 4.4.2)\n#&gt;    purrr             * 1.0.2      2023-08-10 [1] CRAN (R 4.4.2)\n#&gt;    R6                  2.6.1      2025-02-15 [1] CRAN (R 4.4.2)\n#&gt;    raster              3.6-30     2024-10-02 [1] CRAN (R 4.4.2)\n#&gt;    rbibutils           2.3        2024-10-04 [1] CRAN (R 4.4.2)\n#&gt;    RColorBrewer        1.1-3      2022-04-03 [1] CRAN (R 4.4.0)\n#&gt;    Rcpp                1.0.13-1   2024-11-02 [1] CRAN (R 4.4.2)\n#&gt;    RcppNumerical       0.6-0      2023-09-06 [1] CRAN (R 4.4.2)\n#&gt;  D RcppParallel        5.1.9      2024-08-19 [1] CRAN (R 4.4.2)\n#&gt;    Rdpack              2.6.2      2024-11-15 [1] CRAN (R 4.4.2)\n#&gt;    readr             * 2.1.5      2024-01-10 [1] CRAN (R 4.4.2)\n#&gt;    readxl            * 1.4.3      2023-07-06 [1] CRAN (R 4.4.2)\n#&gt;    remotes             2.5.0      2024-03-17 [1] CRAN (R 4.4.2)\n#&gt;    renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.2)\n#&gt;    reshape2          * 1.4.4      2020-04-09 [1] CRAN (R 4.4.2)\n#&gt;    rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.2)\n#&gt;    rmarkdown           2.29       2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    rstudioapi          0.17.1     2024-10-22 [1] CRAN (R 4.4.2)\n#&gt;    s2                  1.1.7      2024-07-17 [1] CRAN (R 4.4.2)\n#&gt;    sass                0.4.9      2024-03-15 [1] CRAN (R 4.4.2)\n#&gt;    satellite           1.0.5      2024-02-10 [1] CRAN (R 4.4.2)\n#&gt;    scales            * 1.3.0      2023-11-28 [1] CRAN (R 4.4.2)\n#&gt;    secr                5.1.0      2024-11-04 [1] CRAN (R 4.4.2)\n#&gt;    sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.2)\n#&gt;    sf                * 1.0-19     2024-11-05 [1] CRAN (R 4.4.2)\n#&gt;    shiny               1.9.1      2024-08-01 [1] CRAN (R 4.4.2)\n#&gt;    slippymath          0.3.1      2019-06-28 [1] CRAN (R 4.4.2)\n#&gt;    sp                  2.1-4      2024-04-30 [1] CRAN (R 4.4.2)\n#&gt;    spacesXYZ           1.3-0      2024-01-23 [1] CRAN (R 4.4.2)\n#&gt;    SpadeR            * 0.1.1      2016-09-06 [1] CRAN (R 4.4.0)\n#&gt;    stars               0.6-8      2025-02-01 [1] CRAN (R 4.4.2)\n#&gt;    stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.0)\n#&gt;    stringr           * 1.5.1      2023-11-14 [1] CRAN (R 4.4.2)\n#&gt;    svglite             2.1.3      2023-12-08 [1] CRAN (R 4.4.2)\n#&gt;    systemfonts         1.1.0      2024-05-15 [1] CRAN (R 4.4.2)\n#&gt;    terra               1.8-21     2025-02-10 [1] CRAN (R 4.4.2)\n#&gt;    tibble            * 3.2.1      2023-03-20 [1] CRAN (R 4.4.2)\n#&gt;    tidyr             * 1.3.1      2024-01-24 [1] CRAN (R 4.4.2)\n#&gt;    tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.2)\n#&gt;    tidyverse         * 2.0.0      2023-02-22 [1] CRAN (R 4.4.2)\n#&gt;    timechange          0.3.0      2024-01-18 [1] CRAN (R 4.4.2)\n#&gt;    tmap              * 4.0        2025-01-27 [1] CRAN (R 4.4.3)\n#&gt;    tmaptools           3.2        2025-01-13 [1] CRAN (R 4.4.3)\n#&gt;    tweenr              2.0.3      2024-02-26 [1] CRAN (R 4.4.2)\n#&gt;    tzdb                0.4.0      2023-05-12 [1] CRAN (R 4.4.2)\n#&gt;    units               0.8-5      2023-11-28 [1] CRAN (R 4.4.2)\n#&gt;    urlchecker          1.0.1      2021-11-30 [1] CRAN (R 4.4.2)\n#&gt;    usethis             3.0.0      2024-07-29 [1] CRAN (R 4.4.1)\n#&gt;    uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n#&gt;    vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.2)\n#&gt;    vegan             * 2.6-8      2024-08-28 [1] CRAN (R 4.4.2)\n#&gt;    viridisLite         0.4.2      2023-05-02 [1] CRAN (R 4.4.2)\n#&gt;    withr               3.0.2      2024-10-28 [1] CRAN (R 4.4.2)\n#&gt;    wk                  0.9.4      2024-10-11 [1] CRAN (R 4.4.2)\n#&gt;    xfun                0.49       2024-10-31 [1] CRAN (R 4.4.2)\n#&gt;    XML                 3.99-0.17  2024-06-25 [1] CRAN (R 4.4.1)\n#&gt;    xml2                1.3.6      2023-12-04 [1] CRAN (R 4.4.2)\n#&gt;    xtable              1.8-4      2019-04-21 [1] CRAN (R 4.4.2)\n#&gt;    yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n#&gt; \n#&gt;  [1] C:/Users/usuario/AppData/Local/R/win-library/4.4\n#&gt;  [2] C:/Program Files/R/R-4.4.2/library\n#&gt; \n#&gt;  D ── DLL MD5 mismatch, broken installation.\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Diego Lizcano. As a wildlife ecologist, the R statistical computing language is an important part of my daily workflow. I mainly use R for data analysis, modelling, and data manipulation."
  },
  {
    "objectID": "about.html#current-research",
    "href": "about.html#current-research",
    "title": "About",
    "section": "Current research",
    "text": "Current research\nI am currently working as a data annalist for WCS Andes-Amazon-Orinoco Region\nSome research I am currently working on:\n\nJaguar detection and occupancy modeling\nWCS Strongholds analysis\n\nI am always looking for new collaborations. Do not hesitate to contact me!"
  },
  {
    "objectID": "about.html#scientific-interests-no-specific-order",
    "href": "about.html#scientific-interests-no-specific-order",
    "title": "About",
    "section": "Scientific interests (no specific order)",
    "text": "Scientific interests (no specific order)\n\nMammal ecology\nCamera trap data analysis\nAcoustics\nEcological modeling"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#shoud-i-use-multiseason-model",
    "href": "posts/2024-07-17-stackmodel/index.html#shoud-i-use-multiseason-model",
    "title": "“Stacked” Models",
    "section": "",
    "text": "Multi-season (or dynamic) models are commonly used to estimate colonization and/or extinction probabilities, and to test hypotheses on these parameters (using covariates on the parameters gamma and epsilon). This approach needs good amounts of data (many sites, and specially many seasons or years). If you don’t need to estimate dynamic parameters (Colonization or extinction, gamma and epsilon) but you’d like to test for temporal variation in occupancy (Psi) between two or three years taking in to account detection probability (p) you could apply a single-season model with random effects (being random effects the camera trap, sampling unit, or site), by stacking years (i.e., your sampling units would be combination camera-years)."
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#the-stacked-model",
    "href": "posts/2024-07-17-stackmodel/index.html#the-stacked-model",
    "title": "“Stacked” Models",
    "section": "The “stacked” model",
    "text": "The “stacked” model\nAn alternative approach to try a dynamic model, is to fit multiple years of data into a single-season model, using the “stacked” approach. Essentially, you treat unique site-year combinations as sites and can make occupancy comparisons between years.\nThere are several potential reasons for this:\n\n\nTake in to account that dynamic models and Dail-Madsen type models are particularly data hungry.\n\n\nYou are not interested in the transition probabilities (colonization or extinction rates).\n\n\nYou have very few years or seasons (less than five) in your sampling design, and the occupancy did not changed substantially in those few years.\n\n\nThis is specially useful if you only have 2 years of data, so there is no great gain in fitting a dynamic occupancy model with the four parameters parameters \\(\\Psi\\), \\(p\\), \\(\\gamma\\), and \\(\\epsilon\\), especially if you have a low number of detections and few years or seasond. So the best approach is combining (stacking) the two-treee years, and running a single season occupancy model, with just two parameters (\\(\\Psi\\) and \\(p\\) instead of four parameters), with year as an explanatory variable and the site as random effect as using lme4 notation as:\nmodel &lt;- occu (~ effort ~ elevation + year + (1 | site), data = newOccu)"
  },
  {
    "objectID": "posts/2024-07-17-stackmodel/index.html#shoud-i-use-a-multiseason-model",
    "href": "posts/2024-07-17-stackmodel/index.html#shoud-i-use-a-multiseason-model",
    "title": "“Stacked” Models",
    "section": "",
    "text": "Multi-season (or dynamic) models are commonly used to estimate colonization and/or extinction probabilities, and to test hypotheses on these parameters (using covariates on the parameters gamma and epsilon). This approach needs good amounts of data (many sites, and specially many seasons or years). If you don’t need to estimate dynamic parameters (Colonization or extinction, gamma and epsilon) but you’d like to test for temporal variation in occupancy (Psi) between two or three years taking in to account detection probability (p) you could apply a single-season model with random effects (being random effects the camera trap, sampling unit, or site), by stacking years (i.e., your sampling units would be combination camera-years)."
  }
]